"""
GPT-2 åˆ†æ‹†æ¨¡å‹å®¢æˆ·ç«¯ï¼ˆGradio + gRPCï¼‰

æ¶æ„ï¼š
  - Bottom (æœ¬åœ°): å‰ 2 å±‚ (layers 0-1) + embeddings
  - Trunk (è¿œç¨‹ gRPC): ä¸­é—´ 8 å±‚ (layers 2-9)
  - Top (æœ¬åœ°): å 2 å±‚ (layers 10-11) + final norm + LM head

ç”¨æ³•ï¼š
    export GPT2_TRUNK_SERVER="localhost:50051"
    PYTHONPATH=./SplitLearnCore/src:./SplitLearnComm/src python gpt2_client_gradio_grpc.py
"""

import os
import sys
import time
import socket
import logging
from pathlib import Path
from typing import Dict, List

os.environ.setdefault("GRADIO_ANALYTICS_ENABLED", "false")

import torch
import gradio as gr
import plotly.graph_objects as go
import pandas as pd

# æ·»åŠ è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "SplitLearnCore" / "src"))
sys.path.insert(0, str(Path(__file__).parent / "SplitLearnComm" / "src"))

from transformers import AutoTokenizer
from splitlearn_core.quickstart import load_split_model
from splitlearn_comm.client import GRPCComputeClient

# é…ç½®æ—¥å¿—
log_dir = Path("./logs")
log_dir.mkdir(exist_ok=True)

LOG_LEVEL_NAME = os.environ.get("GPT2_LOG_LEVEL", "INFO").upper()
LOG_LEVEL = getattr(logging, LOG_LEVEL_NAME, logging.INFO)
GRPC_VERBOSE = os.environ.get("GPT2_GRPC_LOG", "1") == "1"

logging.basicConfig(
    level=LOG_LEVEL,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_dir / "gpt2_client.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ============================================================================
# å…¨å±€å˜é‡ï¼ˆæ¨¡å‹åŠ è½½ï¼‰
# ============================================================================

model_id = "gpt2"
split_points = [2, 10]  # Bottom: 0-1, Trunk: 2-9, Top: 10-11
device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
SERVER_ADDRESS = os.environ.get("GPT2_TRUNK_SERVER", "localhost:50051")
CLIENT_PORT = int(os.environ.get("GPT2_CLIENT_PORT", "7860"))
SHARE_ENABLED = os.environ.get("GRADIO_SHARE", "1") == "1"
MODEL_CACHE = str(Path("./models").resolve())

logger.info("=" * 70)
logger.info("GPT-2 åˆ†æ‹†å®¢æˆ·ç«¯ï¼ˆGradio + gRPCï¼‰")
logger.info("=" * 70)
logger.info(f"æ—¥å¿—çº§åˆ«: {LOG_LEVEL_NAME}")
logger.info(f"æœ¬åœ°è®¾å¤‡: {device}")
logger.info(f"è¿œç¨‹æœåŠ¡: {SERVER_ADDRESS}")
logger.info(f"æ‹†åˆ†ç‚¹: {split_points}")
logger.info(f"Gradio ç«¯å£: {CLIENT_PORT}")
logger.info(f"å…¬ç½‘åˆ†äº«: {SHARE_ENABLED}")
logger.info(f"æ¨¡å‹ç¼“å­˜ç›®å½•: {MODEL_CACHE}")
logger.info(f"gRPC ä¼ è¾“æ—¥å¿—: {GRPC_VERBOSE}")

# åŠ è½½åˆ†æ‹†æ¨¡å‹ï¼ˆåªåŠ è½½ bottom å’Œ topï¼‰
logger.info("åŠ è½½ Bottom å’Œ Top æ¨¡å‹...")
bottom, _, top = load_split_model(
    model_type="gpt2",
    split_points=split_points,
    model_name_or_path=model_id,
    cache_dir="./models",
    device=device,
    parts=["bottom", "top"],  # trunk ä¸åŠ è½½
)

# åº”ç”¨ torch.compile() ä¼˜åŒ–
if hasattr(torch, 'compile') and device == "cuda":
    logger.info("åº”ç”¨ torch.compile() ä¼˜åŒ–...")
    try:
        bottom = torch.compile(bottom, mode="reduce-overhead")
        top = torch.compile(top, mode="reduce-overhead")
        logger.info("âœ“ torch.compile() ä¼˜åŒ–å·²åº”ç”¨")
    except Exception as e:
        logger.warning(f"torch.compile() ä¼˜åŒ–å¤±è´¥: {e}")

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=MODEL_CACHE)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

logger.info("âœ“ Bottom å’Œ Top æ¨¡å‹åŠ è½½å®Œæˆ")
logger.info(f"  Bottom å‚æ•°: {sum(p.numel() for p in bottom.parameters())/1e6:.2f}M")
logger.info(f"  Top å‚æ•°: {sum(p.numel() for p in top.parameters())/1e6:.2f}M")

# è¿æ¥ gRPC æœåŠ¡
logger.info(f"è¿æ¥è¿œç¨‹æœåŠ¡ {SERVER_ADDRESS}...")
client = GRPCComputeClient(server_address=SERVER_ADDRESS, timeout=60.0)
if not client.connect():
    raise RuntimeError(f"æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ {SERVER_ADDRESS}")
logger.info("âœ“ å·²è¿æ¥åˆ°è¿œç¨‹æœåŠ¡")

# å…¨å±€ç»Ÿè®¡å˜é‡
all_token_stats = []
generation_history = []


# ============================================================================
# æ ¸å¿ƒç”Ÿæˆå‡½æ•°
# ============================================================================

def generate_text_with_kv_cache(
    prompt: str,
    max_new_tokens: int = 50,
    temperature: float = 1.0,
    top_k: int = 50,
):
    """ä½¿ç”¨ KV Cache çš„é«˜æ•ˆæ–‡æœ¬ç”Ÿæˆï¼ˆç”Ÿæˆå™¨å‡½æ•°ï¼Œæµå¼è¾“å‡ºï¼‰"""
    global all_token_stats

    logger.info(
        f"[generate] æ”¶åˆ°ç”Ÿæˆè¯·æ±‚ "
        f"prompt_len={len(prompt)} max_new_tokens={max_new_tokens} "
        f"temp={temperature} top_k={top_k}"
    )

    # ç¼–ç è¾“å…¥
    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)

    # ç»Ÿè®¡å˜é‡
    token_times = []  # æ¯ä¸ª token çš„ç”Ÿæˆæ—¶é—´
    server_compute_times = []  # æœåŠ¡ç«¯è®¡ç®—æ—¶é—´
    network_times = []  # ç½‘ç»œå¾€è¿”æ—¶é—´
    bottom_times = []
    top_times = []

    generated_tokens = []
    past_key_values = None  # KV cache

    total_start = time.time()

    def _tensor_bytes(t: torch.Tensor) -> int:
        return t.numel() * t.element_size()

    with torch.no_grad():
        for step in range(max_new_tokens):
            step_start = time.time()

            # 1. Bottom: åµŒå…¥ + å‰ 2 å±‚
            bottom_start = time.time()
            if step == 0:
                # é¦–æ¬¡ï¼šå¤„ç†å®Œæ•´æç¤º
                current_input_ids = input_ids
            else:
                # åç»­ï¼šåªå¤„ç†æ–° token
                current_input_ids = torch.tensor([[next_token_id]], device=device)

            bottom_out = bottom(current_input_ids)
            bottom_time = time.time() - bottom_start
            bottom_times.append(bottom_time * 1000)

            # 2. Trunk (è¿œç¨‹ gRPC): ä¸­é—´ 8 å±‚ + KV Cache
            trunk_start = time.time()
            try:
                if GRPC_VERBOSE:
                    logger.info(
                        f"[gRPC->trunk] step={step} "
                        f"input_shape={list(bottom_out.shape)} "
                        f"bytes={_tensor_bytes(bottom_out)} "
                        f"dtype={bottom_out.dtype} "
                        f"use_cache={past_key_values is not None}"
                    )
                trunk_out, present_kv, timing = client.compute_with_cache(
                    bottom_out.to("cpu"),
                    past_key_values=past_key_values,  # ä¼ å…¥å†å² KV
                    use_cache=True,           # è¿”å›æ–° KV
                    model_id="gpt2"
                )
                if GRPC_VERBOSE:
                    logger.info(
                        f"[gRPC<-trunk] step={step} "
                        f"output_shape={list(trunk_out.shape)} "
                        f"bytes={_tensor_bytes(trunk_out)} "
                        f"server_ms={timing.get('server_compute_ms', 0)} "
                        f"network_ms={timing.get('network_overhead_ms', 0)}"
                    )
            except Exception as e:
                logger.error(f"gRPC è°ƒç”¨å¤±è´¥: {e}")
                yield {
                    "text": f"é”™è¯¯ï¼šæ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨\n{str(e)}",
                    "stats": "ç”Ÿæˆå¤±è´¥"
                }
                return

            trunk_time = time.time() - trunk_start

            # è®°å½•æœåŠ¡ç«¯è®¡ç®—æ—¶é—´
            server_compute_times.append(timing.get("server_compute_ms", 0))
            network_times.append(timing.get("network_overhead_ms", 0))

            # æ›´æ–° KV cache
            past_key_values = present_kv

            # 3. Top (æœ¬åœ°): å 2 å±‚ + LM head
            top_start = time.time()
            trunk_out = trunk_out.to(device)
            output = top(trunk_out)
            logits = output.logits[0, -1, :]  # æœ€åä¸€ä¸ªä½ç½®çš„ logits
            top_time = time.time() - top_start
            top_times.append(top_time * 1000)

            # 4. é‡‡æ ·ä¸‹ä¸€ä¸ª token
            sampling_start = time.time()
            if temperature > 0:
                logits = logits / temperature
                if top_k > 0:
                    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]
                    logits[indices_to_remove] = float('-inf')
                probs = torch.softmax(logits, dim=-1)
                next_token_id = torch.multinomial(probs, num_samples=1).item()
            else:
                next_token_id = logits.argmax(dim=-1).item()
            sampling_time = time.time() - sampling_start

            generated_tokens.append(next_token_id)

            # 5. è®°å½• token ç”Ÿæˆæ—¶é—´
            token_time = time.time() - step_start
            token_times.append(token_time * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’

            # è®°å½•è¯¦ç»†ç»Ÿè®¡
            token_text = tokenizer.decode([next_token_id])
            all_token_stats.append({
                "step": step,
                "token_id": next_token_id,
                "token_text": token_text,
                "total_ms": token_time * 1000,
                "bottom_ms": bottom_time * 1000,
                "trunk_ms": trunk_time * 1000,
                "top_ms": top_time * 1000,
                "sampling_ms": sampling_time * 1000,
                "server_compute_ms": timing.get("server_compute_ms", 0),
                "network_ms": timing.get("network_overhead_ms", 0),
            })

            # 6. æ£€æŸ¥ç»“æŸç¬¦
            if next_token_id == tokenizer.eos_token_id:
                break

            # 7. å®æ—¶è¾“å‡ºï¼ˆæµå¼ç”Ÿæˆï¼‰
            current_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            elapsed = time.time() - total_start

            # è®°å½•æ¯ä¸ª token çš„è¯¦ç»†ä¿¡æ¯åˆ°æ—¥å¿—
            token_text = tokenizer.decode([next_token_id])
            logger.info(f"Token #{len(generated_tokens)}: '{token_text}' (ID={next_token_id}) | "
                       f"Total={token_time*1000:.2f}ms, Server={timing.get('server_compute_ms', 0):.2f}ms, "
                       f"Network={timing.get('network_overhead_ms', 0):.2f}ms")

            stats_text = f"""ğŸ”„ ç”Ÿæˆä¸­...

Tokenæ•°: {len(generated_tokens)}/{max_new_tokens}
æœ€æ–°Token: '{token_text}'
é€Ÿåº¦: {len(generated_tokens)/elapsed:.2f} tokens/s
å¹³å‡å»¶è¿Ÿ: {sum(token_times)/len(token_times):.2f}ms/token
æœåŠ¡ç«¯è®¡ç®—: {sum(server_compute_times)/len(server_compute_times):.2f}ms
ç½‘ç»œå¼€é”€: {sum(network_times)/len(network_times):.2f}ms
"""

            yield prompt + current_text, stats_text

    # æœ€ç»ˆç»Ÿè®¡
    total_time = time.time() - total_start
    final_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)

    # è·å–æœ€è¿‘ç”Ÿæˆçš„tokenè¯¦æƒ…ï¼ˆç”¨äºæœ€ç»ˆå±•ç¤ºï¼‰
    recent_stats = all_token_stats[-len(generated_tokens):] if len(all_token_stats) >= len(generated_tokens) else all_token_stats
    token_list = "\n".join([
        f"  {i+1}. '{s['token_text']}' (ID={s['token_id']}, {s['total_ms']:.1f}ms)"
        for i, s in enumerate(recent_stats[:10])  # æ˜¾ç¤ºå‰10ä¸ªtoken
    ])
    if len(recent_stats) > 10:
        token_list += f"\n  ... (è¿˜æœ‰ {len(recent_stats)-10} ä¸ªtoken)"

    stats_text = f"""âœ… ç”Ÿæˆå®Œæˆ

æ€»Tokenæ•°: {len(generated_tokens)}
æ€»æ—¶é—´: {total_time:.2f}s
å¹³å‡é€Ÿåº¦: {len(generated_tokens)/total_time:.2f} tokens/s
å¹³å‡å»¶è¿Ÿ: {sum(token_times)/len(token_times):.2f}ms/token

ç»„ä»¶è€—æ—¶:
  - Bottom: {sum(bottom_times)/len(bottom_times):.2f}ms
  - Trunk (å«ç½‘ç»œ): {sum(token_times)/len(token_times):.2f}ms
  - Top: {sum(top_times)/len(top_times):.2f}ms

æœåŠ¡ç«¯ç»Ÿè®¡:
  - çº¯è®¡ç®—æ—¶é—´: {sum(server_compute_times)/len(server_compute_times):.2f}ms
  - ç½‘ç»œå¼€é”€: {sum(network_times)/len(network_times):.2f}ms

ç”Ÿæˆçš„Token (å‰10ä¸ª):
{token_list}

æç¤º: æŸ¥çœ‹ 'ğŸ” Tokenè¯¦æƒ…' æ ‡ç­¾é¡µæŸ¥çœ‹å®Œæ•´tokenåˆ—è¡¨
"""

    # è®°å½•æœ€ç»ˆç»Ÿè®¡åˆ°æ—¥å¿—
    logger.info(f"âœ… ç”Ÿæˆå®Œæˆ: {len(generated_tokens)} tokens in {total_time:.2f}s ({len(generated_tokens)/total_time:.2f} tokens/s)")
    logger.info(f"   å¹³å‡å»¶è¿Ÿ: Bottom={sum(bottom_times)/len(bottom_times):.2f}ms, "
               f"Server={sum(server_compute_times)/len(server_compute_times):.2f}ms, "
               f"Network={sum(network_times)/len(network_times):.2f}ms")

    yield prompt + final_text, stats_text


# ============================================================================
# ç›‘æ§æ•°æ®è·å–
# ============================================================================

def update_monitoring():
    """ä»æœåŠ¡ç«¯è·å–ç›‘æ§æ•°æ®"""
    try:
        # è·å–æœ€æ–°çš„æœåŠ¡ç«¯ç›‘æ§å¿«ç…§
        monitoring_data = client.get_server_monitoring_data()

        if not monitoring_data or len(monitoring_data) == 0:
            return "N/A", "N/A", "N/A", "æ— ç›‘æ§æ•°æ®"

        latest = monitoring_data[-1]

        cpu = f"{latest.get('cpu_percent', 0):.1f}%"
        gpu = f"{latest.get('gpu_utilization', 0):.1f}%" if latest.get('gpu_available') else "N/A"
        memory = f"{latest.get('memory_mb', 0):.0f} MB ({latest.get('memory_percent', 0):.1f}%)"

        # æ ¼å¼åŒ–å†å²è®°å½•
        log_lines = []
        for snapshot in monitoring_data[-10:]:  # æœ€è¿‘ 10 æ¡
            ts = snapshot.get('timestamp', time.time())
            log_lines.append(
                f"[{time.strftime('%H:%M:%S', time.localtime(ts))}] "
                f"CPU: {snapshot.get('cpu_percent', 0):.1f}% | "
                f"GPU: {snapshot.get('gpu_utilization', 0):.1f}% | "
                f"MEM: {snapshot.get('memory_mb', 0):.0f}MB"
            )

        return cpu, gpu, memory, "\n".join(log_lines)

    except Exception as e:
        return "é”™è¯¯", "é”™è¯¯", "é”™è¯¯", f"è·å–ç›‘æ§æ•°æ®å¤±è´¥: {str(e)}"


def update_latency_stats():
    """æ›´æ–°å»¶è¿Ÿç»Ÿè®¡"""
    if not all_token_stats:
        empty_df = pd.DataFrame(columns=["æŒ‡æ ‡", "å€¼"])
        empty_fig = go.Figure()
        empty_fig.update_layout(title="æš‚æ— æ•°æ®")
        return empty_df, empty_fig, empty_fig

    # åˆ›å»ºç»Ÿè®¡è¡¨
    df = pd.DataFrame({
        "æŒ‡æ ‡": [
            "æ€»è¯·æ±‚æ•°",
            "å¹³å‡æ€»å»¶è¿Ÿ(ms)",
            "å¹³å‡Bottom(ms)",
            "å¹³å‡Trunk+ç½‘ç»œ(ms)",
            "å¹³å‡Top(ms)",
            "å¹³å‡æœåŠ¡ç«¯è®¡ç®—(ms)",
            "å¹³å‡ç½‘ç»œå¼€é”€(ms)",
        ],
        "å€¼": [
            len(all_token_stats),
            f"{sum(s['total_ms'] for s in all_token_stats) / len(all_token_stats):.2f}",
            f"{sum(s['bottom_ms'] for s in all_token_stats) / len(all_token_stats):.2f}",
            f"{sum(s['trunk_ms'] for s in all_token_stats) / len(all_token_stats):.2f}",
            f"{sum(s['top_ms'] for s in all_token_stats) / len(all_token_stats):.2f}",
            f"{sum(s['server_compute_ms'] for s in all_token_stats) / len(all_token_stats):.2f}",
            f"{sum(s['network_ms'] for s in all_token_stats) / len(all_token_stats):.2f}",
        ]
    })

    # å»¶è¿Ÿåˆ†å¸ƒç›´æ–¹å›¾
    total_times = [s['total_ms'] for s in all_token_stats]
    fig_dist = go.Figure(data=[go.Histogram(x=total_times, nbinsx=20)])
    fig_dist.update_layout(
        title="Token å»¶è¿Ÿåˆ†å¸ƒ",
        xaxis_title="å»¶è¿Ÿ (ms)",
        yaxis_title="é¢‘æ¬¡"
    )

    # Token ç”Ÿæˆæ—¶é—´çº¿
    steps = [s['step'] for s in all_token_stats[-50:]]  # æœ€è¿‘50ä¸ª
    total_ms = [s['total_ms'] for s in all_token_stats[-50:]]
    server_ms = [s['server_compute_ms'] for s in all_token_stats[-50:]]
    network_ms = [s['network_ms'] for s in all_token_stats[-50:]]

    fig_timeline = go.Figure()
    fig_timeline.add_trace(go.Scatter(x=steps, y=total_ms, name="æ€»å»¶è¿Ÿ", mode='lines+markers'))
    fig_timeline.add_trace(go.Scatter(x=steps, y=server_ms, name="æœåŠ¡ç«¯è®¡ç®—", mode='lines'))
    fig_timeline.add_trace(go.Scatter(x=steps, y=network_ms, name="ç½‘ç»œå¼€é”€", mode='lines'))
    fig_timeline.update_layout(
        title="Token ç”Ÿæˆæ—¶é—´çº¿ï¼ˆæœ€è¿‘50ä¸ªï¼‰",
        xaxis_title="Token åºå·",
        yaxis_title="æ—¶é—´ (ms)"
    )

    return df, fig_dist, fig_timeline


def update_token_details():
    """æ›´æ–°Tokenè¯¦æƒ…è¡¨æ ¼ï¼ˆæœ€è¿‘50ä¸ªtokenï¼‰"""
    if not all_token_stats:
        return pd.DataFrame(columns=["åºå·", "Token", "ID", "æ€»å»¶è¿Ÿ(ms)", "Bottom(ms)", "Trunk(ms)", "Top(ms)", "æœåŠ¡ç«¯(ms)", "ç½‘ç»œ(ms)"])

    # è·å–æœ€è¿‘50ä¸ªtoken
    recent_tokens = all_token_stats[-50:]

    df = pd.DataFrame({
        "åºå·": [s['step'] for s in recent_tokens],
        "Token": [s['token_text'] for s in recent_tokens],
        "ID": [s['token_id'] for s in recent_tokens],
        "æ€»å»¶è¿Ÿ(ms)": [f"{s['total_ms']:.2f}" for s in recent_tokens],
        "Bottom(ms)": [f"{s['bottom_ms']:.2f}" for s in recent_tokens],
        "Trunk(ms)": [f"{s['trunk_ms']:.2f}" for s in recent_tokens],
        "Top(ms)": [f"{s['top_ms']:.2f}" for s in recent_tokens],
        "æœåŠ¡ç«¯(ms)": [f"{s['server_compute_ms']:.2f}" for s in recent_tokens],
        "ç½‘ç»œ(ms)": [f"{s['network_ms']:.2f}" for s in recent_tokens],
    })

    return df


# ============================================================================
# Gradio ç•Œé¢
# ============================================================================

with gr.Blocks(title="GPT-2 åˆ†æ‹†å®¢æˆ·ç«¯", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# GPT-2 åˆ†æ‹†å­¦ä¹ å®¢æˆ·ç«¯")
    gr.Markdown(f"""
    **æ¶æ„**:
    - ğŸ–¥ï¸ **æœ¬åœ° Bottom**: å‰ 2 å±‚ (layers 0-1) + embeddings
    - â˜ï¸ **è¿œç¨‹ Trunk**: ä¸­é—´ 8 å±‚ (layers 2-9) - `{SERVER_ADDRESS}`
    - ğŸ–¥ï¸ **æœ¬åœ° Top**: å 2 å±‚ (layers 10-11) + final norm + LM head
    - âš¡ **ä¼˜åŒ–**: KV Cache + torch.compile()
    """)

    with gr.Tab("ğŸ“ æ–‡æœ¬ç”Ÿæˆ"):
        with gr.Row():
            with gr.Column(scale=3):
                prompt_input = gr.Textbox(
                    label="è¾“å…¥æç¤º (Prompt)",
                    placeholder="ä¾‹å¦‚: The future of AI is",
                    lines=3,
                    value="Once upon a time"
                )
                gr.Examples(
                    examples=[
                        ["Once upon a time"],
                        ["The future of AI is"],
                        ["In the year 2050,"],
                        ["Hello, my name is"],
                    ],
                    inputs=prompt_input,
                )

            with gr.Column(scale=2):
                max_tokens = gr.Slider(1, 200, value=50, step=1, label="æœ€å¤§ç”Ÿæˆ tokens")
                temperature = gr.Slider(0.0, 2.0, value=1.0, step=0.1, label="Temperature")
                top_k = gr.Slider(0, 100, value=50, step=1, label="Top-K")

        generate_btn = gr.Button("ğŸš€ å¼€å§‹ç”Ÿæˆ", variant="primary", size="lg")

        with gr.Row():
            with gr.Column(scale=3):
                output_text = gr.Textbox(label="ç”Ÿæˆç»“æœ", lines=12, interactive=False)

            with gr.Column(scale=1):
                stats_display = gr.Textbox(label="ç”Ÿæˆç»Ÿè®¡", lines=12, interactive=False)

        # æŒ‰é’®äº‹ä»¶
        generate_btn.click(
            fn=generate_text_with_kv_cache,
            inputs=[prompt_input, max_tokens, temperature, top_k],
            outputs=[output_text, stats_display],
        )

    with gr.Tab("ğŸ“Š æœåŠ¡ç«¯ç›‘æ§"):
        gr.Markdown("å®æ—¶ç›‘æ§æœåŠ¡ç«¯èµ„æºä½¿ç”¨æƒ…å†µï¼ˆæ¯2ç§’åˆ·æ–°ï¼‰")

        with gr.Row():
            cpu_display = gr.Textbox(label="CPU ä½¿ç”¨ç‡", interactive=False)
            gpu_display = gr.Textbox(label="GPU ä½¿ç”¨ç‡", interactive=False)
            memory_display = gr.Textbox(label="å†…å­˜ä½¿ç”¨", interactive=False)

        monitoring_log = gr.Textbox(label="ç›‘æ§å†å²ï¼ˆæœ€è¿‘10æ¡ï¼‰", lines=12, interactive=False)

    with gr.Tab("â±ï¸ å»¶è¿Ÿåˆ†æ"):
        gr.Markdown("Token ç”Ÿæˆæ€§èƒ½åˆ†æï¼ˆæ¯5ç§’åˆ·æ–°ï¼‰")

        latency_table = gr.DataFrame(label="å»¶è¿Ÿç»Ÿè®¡")

        with gr.Row():
            latency_plot = gr.Plot(label="å»¶è¿Ÿåˆ†å¸ƒ")
            token_timeline = gr.Plot(label="Token ç”Ÿæˆæ—¶é—´çº¿")

    with gr.Tab("ğŸ” Tokenè¯¦æƒ…"):
        gr.Markdown("é€Tokenç”Ÿæˆè¯¦æƒ…ï¼ˆæœ€è¿‘50ä¸ªï¼Œæ¯3ç§’åˆ·æ–°ï¼‰")
        gr.Markdown("""
        **è¯´æ˜**ï¼šæ­¤è¡¨æ ¼æ˜¾ç¤ºæ¯ä¸ªç”Ÿæˆçš„tokenåŠå…¶è¯¦ç»†æ—¶é—´åˆ†è§£
        - **Token**: ç”Ÿæˆçš„æ–‡æœ¬token
        - **æ€»å»¶è¿Ÿ**: ä»è¾“å…¥åˆ°è¾“å‡ºçš„å®Œæ•´æ—¶é—´
        - **Bottom**: æœ¬åœ°å‰2å±‚è®¡ç®—æ—¶é—´
        - **Trunk**: è¿œç¨‹8å±‚è®¡ç®—+ç½‘ç»œä¼ è¾“æ—¶é—´
        - **Top**: æœ¬åœ°å2å±‚è®¡ç®—æ—¶é—´
        - **æœåŠ¡ç«¯**: è¿œç¨‹æœåŠ¡å™¨çº¯è®¡ç®—æ—¶é—´
        - **ç½‘ç»œ**: ç½‘ç»œå¾€è¿”å¼€é”€
        """)

        token_details_table = gr.DataFrame(label="Tokenç”Ÿæˆè¯¦æƒ…", wrap=True)

    # è‡ªåŠ¨åˆ·æ–°ç›‘æ§æ•°æ®
    demo.load(
        fn=update_monitoring,
        outputs=[cpu_display, gpu_display, memory_display, monitoring_log],
        every=2
    )

    # è‡ªåŠ¨åˆ·æ–°å»¶è¿Ÿç»Ÿè®¡
    demo.load(
        fn=update_latency_stats,
        outputs=[latency_table, latency_plot, token_timeline],
        every=5
    )

    # è‡ªåŠ¨åˆ·æ–°Tokenè¯¦æƒ…
    demo.load(
        fn=update_token_details,
        outputs=[token_details_table],
        every=3
    )


if __name__ == "__main__":
    logger.info("\nå¯åŠ¨ Gradio ç•Œé¢...")
    demo.queue()  # Gradio 6.0 æ¨èæ·»åŠ é˜Ÿåˆ—
    logger.info(
        f"å¯åŠ¨ Gradio: share={SHARE_ENABLED}, server_name=0.0.0.0, "
        f"port={CLIENT_PORT}"
    )
    launch_kwargs = dict(
        share=SHARE_ENABLED,
        server_name="127.0.0.1",  # ä½¿ç”¨ 127.0.0.1 è€Œä¸æ˜¯ 0.0.0.0ï¼Œé¿å… localhost è®¿é—®é—®é¢˜
        server_port=CLIENT_PORT,
        show_error=True,
        inbrowser=SHARE_ENABLED,
    )
    local_url = share_url = None
    try:
        _, local_url, share_url = demo.launch(**launch_kwargs)
    except (OSError, ValueError) as e:
        # å¤„ç†ç«¯å£è¢«å ç”¨æˆ– localhost ä¸å¯è®¿é—®çš„æƒ…å†µ
        if "localhost" in str(e).lower() or "share" in str(e).lower():
            # localhost ä¸å¯è®¿é—®ï¼Œå¼ºåˆ¶å¯ç”¨ share
            logger.warning(f"localhost ä¸å¯è®¿é—®ï¼Œè‡ªåŠ¨å¯ç”¨å…¬ç½‘åˆ†äº«: {e}")
            launch_kwargs["share"] = True
            launch_kwargs["server_name"] = "127.0.0.1"
            _, local_url, share_url = demo.launch(**launch_kwargs)
        else:
            # ç«¯å£è¢«å ç”¨ï¼Œè‡ªåŠ¨åˆ‡æ¢ç«¯å£
            logger.warning(f"ç«¯å£ {CLIENT_PORT} è¢«å ç”¨ï¼Œå°è¯•è‡ªåŠ¨åˆ‡æ¢: {e}")
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.bind(("", 0))
                free_port = s.getsockname()[1]
            launch_kwargs["server_port"] = free_port
            logger.info(f"æ”¹ç”¨ç©ºé—²ç«¯å£: {free_port}")
            _, local_url, share_url = demo.launch(**launch_kwargs)

    logger.info(f"Gradio æœ¬åœ°åœ°å€: {local_url}")
    logger.info(f"Gradio åˆ†äº«é“¾æ¥: {share_url if share_url else 'æœªå¼€å¯æˆ–åˆ›å»ºå¤±è´¥'}")
