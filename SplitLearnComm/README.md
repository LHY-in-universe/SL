# splitlearn-comm

A high-performance gRPC-based communication library for distributed deep learning.

## Features

- ğŸš€ **High Performance**: Optimized tensor serialization using binary format
- ğŸ”Œ **Model Agnostic**: Completely decoupled from specific models via `ComputeFunction` abstraction
- ğŸ”„ **Reliable**: Built-in retry mechanisms with exponential backoff
- ğŸ›¡ï¸ **Robust**: Comprehensive error handling and health checks
- ğŸ“Š **Observable**: Performance metrics and statistics tracking
- ğŸŒ **Flexible**: Supports single-machine, LAN, and WAN deployments

## Installation / ä¾èµ–

- å·²éªŒè¯ç¯å¢ƒï¼šPython 3.11.12ã€torch 2.9.1ã€grpcio 1.69.0ï¼ˆæ³¨æ„ä¸Šé™ `<1.70.0`ï¼‰ã€protobuf 4.25.xã€‚
- æºç  + PYTHONPATHï¼ˆå½“å‰ç›®å½•ç»“æ„ï¼‰ï¼š`export PYTHONPATH=/Users/lhy/Desktop/Git/SL/SplitLearnComm/src:$PYTHONPATH`
- æˆ–å¼€å‘æ¨¡å¼å®‰è£…ï¼š`pip install -e /Users/lhy/Desktop/Git/SL/SplitLearnComm`

## Quick Start

### Server Example

```python
import torch
from splitlearn_comm import GRPCComputeServer
from splitlearn_comm.core import ModelComputeFunction

# Create a simple model
model = torch.nn.Sequential(
    torch.nn.Linear(768, 768),
    torch.nn.ReLU(),
    torch.nn.Linear(768, 768)
)

# Wrap in ComputeFunction
compute_fn = ModelComputeFunction(model, device="cuda")

# Start server
server = GRPCComputeServer(
    compute_fn=compute_fn,
    host="0.0.0.0",
    port=50051
)

server.start()
server.wait_for_termination()
```

### Client Example

```python
import torch
from splitlearn_comm import GRPCComputeClient

# Connect to server
client = GRPCComputeClient("localhost:50051")
client.connect()

# Perform computation
input_tensor = torch.randn(1, 10, 768)
output_tensor = client.compute(input_tensor)

print(f"Output shape: {output_tensor.shape}")

# Get statistics
stats = client.get_statistics()
print(f"Total requests: {stats['total_requests']}")
print(f"Avg network time: {stats['avg_network_time_ms']:.2f}ms")

client.close()
```

## Core Concepts

### ComputeFunction

The `ComputeFunction` abstraction allows you to use any computation logic:

```python
from splitlearn_comm.core import ComputeFunction

class MyComputeFunction(ComputeFunction):
    def __init__(self, model):
        self.model = model

    def compute(self, input_tensor):
        # Your custom logic here
        with torch.no_grad():
            return self.model(input_tensor)

    def get_info(self):
        return {"model_name": "MyModel", "device": "cuda"}
```

### Retry Strategy

Built-in retry mechanisms for reliability:

```python
from splitlearn_comm import GRPCComputeClient, ExponentialBackoff

# Custom retry strategy
retry = ExponentialBackoff(
    max_retries=5,
    initial_delay=1.0,
    max_delay=30.0
)

client = GRPCComputeClient(
    "localhost:50051",
    retry_strategy=retry
)
```

## Advanced Usage

### Compressed Communication

For bandwidth-constrained scenarios:

```python
from splitlearn_comm.core import CompressedTensorCodec

codec = CompressedTensorCodec(compression_level=6)

server = GRPCComputeServer(
    compute_fn=compute_fn,
    codec=codec,
    port=50051
)
```

### Context Manager Support

```python
# Server
with GRPCComputeServer(compute_fn, port=50051) as server:
    # Server automatically starts and stops
    pass

# Client
with GRPCComputeClient("localhost:50051") as client:
    output = client.compute(input_tensor)
```

### Health Checks and Service Info

```python
# Health check
is_healthy = client.health_check()

# Get service information
info = client.get_service_info()
print(f"Service: {info['service_name']}")
print(f"Version: {info['version']}")
print(f"Device: {info['device']}")
print(f"Total requests: {info['total_requests']}")
```

## API Documentation

See [docs/api.md](docs/api.md) for complete API reference.

## Examples

- [examples/simple_server.py](examples/simple_server.py) - Basic server
- [examples/simple_client.py](examples/simple_client.py) - Basic client
- [examples/custom_service.py](examples/custom_service.py) - Custom compute function

## Performance

splitlearn-comm uses optimized binary serialization for tensors:

- **Bytes format**: 4x faster than protobuf's `repeated float`
- **Zero-copy**: Minimal overhead for numpy/torch conversion
- **Compression**: Optional zlib compression for WAN scenarios

## Requirements

- Python >= 3.8
- PyTorch >= 2.0.0
- gRPC >= 1.50.0
- NumPy >= 1.24.0

## Development

```bash
# Clone repository
git clone https://github.com/yourusername/splitlearn-comm.git
cd splitlearn-comm

# Install in development mode
pip install -e ".[dev]"

# Compile protobuf
bash scripts/compile_proto.sh

# Run tests
pytest tests/
```

## License

MIT License - see [LICENSE](LICENSE) for details.

## Minimal smoke test (imports only)

```bash
export PYTHONPATH=/Users/lhy/Desktop/Git/SL/SplitLearnComm/src:${PYTHONPATH:-}
/Library/Frameworks/Python.framework/Versions/3.11/bin/python3 - <<'PY'
from splitlearn_comm import GRPCComputeClient, GRPCComputeServer
from splitlearn_comm.core import ComputeFunction
print("imports ok")
PY
```

## ç®€æ˜“ API æ¦‚è§ˆ

- `GRPCComputeServer(compute_fn, host="0.0.0.0", port=50051, codec=None, retry_strategy=None)`: å¯åŠ¨ gRPC è®¡ç®—æœåŠ¡ï¼Œ`compute_fn` å®ç°æ¨ç†é€»è¾‘ã€‚
- `GRPCComputeClient(target, retry_strategy=None, codec=None)`: è¿æ¥è¿œç«¯æœåŠ¡ï¼Œ`client.compute(tensor)` è¿›è¡Œè¿œç¨‹æ¨ç†ã€‚
- `ComputeFunction`: æŠ½è±¡åŸºç±»ï¼Œéœ€å®ç° `compute(input_tensor)` å’Œ `get_info()`ï¼›å¯ç”¨ `ModelComputeFunction(model, device)` å°è£… torch æ¨¡å‹ã€‚
- å…¶ä»–å¸¸ç”¨å·¥å…·ï¼š`ExponentialBackoff`ï¼ˆé‡è¯•ç­–ç•¥ï¼‰ã€`CompressedTensorCodec`ï¼ˆå‹ç¼©ä¼ è¾“ï¼‰ã€‚

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## Citation

If you use splitlearn-comm in your research, please cite:

```bibtex
@software{splitlearn_comm2024,
  title = {splitlearn-comm: A gRPC Communication Library for Distributed Deep Learning},
  author = {SplitLearn Contributors},
  year = {2024},
  url = {https://github.com/yourusername/splitlearn-comm}
}
```
