"""
Client-side Gradio UI for interactive text generation with Split Learning
"""

import time
import threading
from typing import Any, Optional, List, Dict

import gradio as gr
import torch
import pandas as pd
import plotly.graph_objects as go

from .components import get_theme, DEFAULT_CSS, StatsPanel
from ..monitoring import MetricsManager, LogManager, LogLevel, MonitoringConfig


class ClientUI:
    """
    Gradio UI for client-side text generation

    Provides an interactive web interface for text generation using a split learning
    architecture where the model is distributed across client and server.

    Example:
        >>> from splitlearn_comm import GRPCComputeClient
        >>> from splitlearn_comm.ui import ClientUI
        >>>
        >>> client = GRPCComputeClient("localhost:50051")
        >>> client.connect()
        >>>
        >>> ui = ClientUI(
        ...     client=client,
        ...     bottom_model=bottom_model,
        ...     top_model=top_model,
        ...     tokenizer=tokenizer
        ... )
        >>> ui.launch(share=False, server_port=7860)
    """

    def __init__(
        self,
        client: Any,
        bottom_model: Any,
        top_model: Any,
        tokenizer: Any,
        theme: str = "default",
        model_id: Optional[str] = None,
        monitoring_config: Optional[MonitoringConfig] = None,
    ):
        """
        Args:
            client: GRPCComputeClient instance (must be connected)
            bottom_model: Bottom part of the split model (runs locally)
            top_model: Top part of the split model (runs locally)
            tokenizer: Tokenizer for encoding/decoding text
            theme: UI theme variant ("default", "dark", "light")
            model_id: Optional model ID to send to server
            monitoring_config: Monitoring configuration (default: MonitoringConfig)
        """
        self.client = client
        self.bottom_model = bottom_model
        self.top_model = top_model
        self.tokenizer = tokenizer
        self.model_id = model_id
        self.theme = get_theme(theme)

        # Generation control
        self.stop_generation = False

        # Monitoring managers
        config = monitoring_config or MonitoringConfig()
        self.metrics_manager = MetricsManager(max_history_size=config.max_history_size)
        self.log_manager = LogManager(max_logs=config.max_log_size)
        self.refresh_interval = config.ui_refresh_interval

        # Log initialization
        self.log_manager.add_log(
            LogLevel.INFO,
            "Client UI initialized",
            source="client"
        )

        # Build the interface
        self.demo = self._build_interface()

    def _stop_generation(self):
        """Stop the current generation"""
        self.stop_generation = True
        return "ğŸ›‘ æ­£åœ¨åœæ­¢ç”Ÿæˆ..."

    def _generate_text_streaming(
        self,
        prompt: str,
        max_length: int = 20,
        temperature: float = 1.0,
        top_k: int = 50,
        show_speed: bool = True
    ):
        """
        Generate text with streaming output (generator function)

        Args:
            prompt: Input text prompt
            max_length: Maximum number of tokens to generate
            temperature: Sampling temperature (higher = more random)
            top_k: Top-K sampling parameter (0 = greedy)
            show_speed: Whether to show generation speed statistics

        Yields:
            Tuple of (generated_text, statistics_text)
        """
        self.stop_generation = False

        # Validate input
        if not prompt or len(prompt.strip()) == 0:
            yield "âš ï¸ è¯·è¾“å…¥ä¸€ä¸ªæœ‰æ•ˆçš„ promptï¼", ""
            return

        try:
            # Log generation start
            self.log_manager.add_log(
                LogLevel.INFO,
                f"Started text generation (max_length={max_length})",
                source="client"
            )

            # Encode input
            input_ids = self.tokenizer.encode(prompt, return_tensors="pt")
            generated_text = prompt
            start_time = time.time()
            tokens_generated = 0

            yield generated_text, "ğŸ”„ ç”Ÿæˆä¸­..."

            # Generation loop
            for step in range(max_length):
                if self.stop_generation:
                    elapsed_time = time.time() - start_time
                    stats = StatsPanel.format_generation_stats(
                        tokens_generated, max_length, elapsed_time, status="stopped"
                    )
                    yield generated_text, stats
                    break

                # 1. Bottom model (local)
                with torch.no_grad():
                    hidden_bottom = self.bottom_model(input_ids)

                # 2. Trunk model (remote server)
                try:
                    # Record server communication latency
                    compute_start = time.time()
                    hidden_trunk = self.client.compute(hidden_bottom, model_id=self.model_id)
                    compute_latency = time.time() - compute_start

                    # Record to metrics manager
                    self.metrics_manager.record_latency(compute_latency)

                    # Log successful communication
                    self.log_manager.add_log(
                        LogLevel.DEBUG,
                        f"Server compute completed in {compute_latency*1000:.2f}ms (token {tokens_generated+1})",
                        source="client"
                    )

                except Exception as e:
                    elapsed_time = time.time() - start_time
                    error_msg = f"âŒ æœåŠ¡å™¨é€šä¿¡é”™è¯¯\n\n{str(e)}\n\nå·²ç”Ÿæˆ {tokens_generated} tokens"

                    # Log error
                    self.log_manager.add_log(
                        LogLevel.ERROR,
                        f"Server communication failed: {str(e)}",
                        source="client"
                    )

                    yield generated_text + f"\n\n[é”™è¯¯: {str(e)}]", error_msg
                    break

                # 3. Top model (local)
                with torch.no_grad():
                    output = self.top_model(hidden_trunk)
                    logits = output.logits[:, -1, :] / temperature

                # 4. Sampling
                if top_k > 0:
                    # Top-K sampling
                    top_k_logits, top_k_indices = torch.topk(logits, top_k)
                    probs = torch.softmax(top_k_logits, dim=-1)
                    next_token_idx = torch.multinomial(probs, num_samples=1)
                    next_token_id = top_k_indices.gather(-1, next_token_idx)
                else:
                    # Greedy sampling
                    next_token_id = logits.argmax(dim=-1).unsqueeze(-1)

                # Update input_ids
                input_ids = torch.cat([input_ids, next_token_id], dim=-1)
                tokens_generated += 1

                # Decode new token
                new_word = self.tokenizer.decode(next_token_id[0])
                generated_text += new_word

                # Calculate statistics
                elapsed_time = time.time() - start_time

                if show_speed:
                    stats = StatsPanel.format_generation_stats(
                        tokens_generated, max_length, elapsed_time, status="generating"
                    )
                else:
                    stats = f"Tokens: {tokens_generated}/{max_length}"

                yield generated_text, stats

                # Check for EOS token
                if self.tokenizer.eos_token_id and next_token_id.item() == self.tokenizer.eos_token_id:
                    stats = StatsPanel.format_generation_stats(
                        tokens_generated, max_length, elapsed_time, status="completed"
                    )
                    yield generated_text, stats
                    break

                time.sleep(0.02)  # Small delay for UI responsiveness

            else:
                # Normal completion (max_length reached)
                elapsed_time = time.time() - start_time
                stats = StatsPanel.format_generation_stats(
                    tokens_generated, max_length, elapsed_time, status="completed"
                )

                # Log completion
                self.log_manager.add_log(
                    LogLevel.INFO,
                    f"Generation completed: {tokens_generated} tokens in {elapsed_time:.2f}s",
                    source="client"
                )

                yield generated_text, stats

        except Exception as e:
            import traceback
            error_msg = f"âŒ ç”Ÿæˆè¿‡ç¨‹å‡ºé”™:\n\n{str(e)}\n\n{traceback.format_exc()}"

            # Log exception
            self.log_manager.add_log(
                LogLevel.ERROR,
                f"Generation error: {str(e)}",
                source="client"
            )

            yield generated_text if 'generated_text' in locals() else prompt, error_msg

    def _get_logs_text(self, level_filter: str = "ALL") -> str:
        """Get formatted logs as text"""
        try:
            logs = self.log_manager.get_logs(
                level_filter=level_filter if level_filter != "ALL" else None,
                limit=100
            )

            if not logs:
                return f"No {level_filter} logs available"

            # Format logs
            formatted_lines = []
            for log in logs:
                timestamp_str = log['timestamp'].strftime("%H:%M:%S.%f")[:-3]
                level = log['level']
                message = log['message']

                # Add level emoji
                level_icon = {
                    'DEBUG': 'ğŸ”',
                    'INFO': 'â„¹ï¸',
                    'WARNING': 'âš ï¸',
                    'ERROR': 'âŒ'
                }.get(level, 'ğŸ“')

                line = f"[{timestamp_str}] {level_icon} {level:8} {message}"
                formatted_lines.append(line)

            return "\n".join(formatted_lines)

        except Exception as e:
            return f"âŒ Error fetching logs: {str(e)}"

    def _get_latency_stats_dataframe(self) -> pd.DataFrame:
        """Get latency statistics as DataFrame"""
        try:
            stats = self.metrics_manager.get_latency_stats()

            if stats.get('count', 0) == 0:
                return pd.DataFrame({
                    "Metric": ["No data"],
                    "Value": [""]
                })

            data = {
                "Metric": [
                    "Count",
                    "Mean",
                    "Median (P50)",
                    "P95",
                    "P99",
                    "Min",
                    "Max",
                    "Std Dev"
                ],
                "Value": [
                    f"{stats['count']} requests",
                    f"{stats['mean']:.2f} ms",
                    f"{stats['median']:.2f} ms",
                    f"{stats['p95']:.2f} ms",
                    f"{stats['p99']:.2f} ms",
                    f"{stats['min']:.2f} ms",
                    f"{stats['max']:.2f} ms",
                    f"{stats['std']:.2f} ms"
                ]
            }

            return pd.DataFrame(data)

        except Exception as e:
            return pd.DataFrame({
                "Error": [str(e)]
            })

    def _get_latency_distribution_plot(self):
        """Get latency distribution histogram"""
        try:
            bin_edges, counts = self.metrics_manager.get_latency_distribution()

            if not bin_edges or not counts:
                fig = go.Figure()
                fig.add_annotation(
                    text="No latency data available",
                    xref="paper", yref="paper",
                    x=0.5, y=0.5, showarrow=False,
                    font=dict(size=16)
                )
                fig.update_layout(
                    title="Server Communication Latency Distribution",
                    xaxis_title="Latency (ms)",
                    yaxis_title="Count"
                )
                return fig

            # Calculate bin centers
            bin_centers = [(bin_edges[i] + bin_edges[i+1]) / 2 for i in range(len(counts))]

            fig = go.Figure(data=[
                go.Bar(
                    x=bin_centers,
                    y=counts,
                    name="Latency",
                    marker_color='rgb(55, 126, 184)'
                )
            ])

            fig.update_layout(
                title="Server Communication Latency Distribution",
                xaxis_title="Latency (ms)",
                yaxis_title="Count",
                showlegend=False,
                height=300
            )

            return fig

        except Exception as e:
            fig = go.Figure()
            fig.add_annotation(
                text=f"Error: {str(e)}",
                xref="paper", yref="paper",
                x=0.5, y=0.5, showarrow=False
            )
            return fig

    def _get_latency_trend_plot(self):
        """Get latency trend over time"""
        try:
            history = self.metrics_manager.get_latency_history(limit=50)

            if not history:
                fig = go.Figure()
                fig.add_annotation(
                    text="No latency history available",
                    xref="paper", yref="paper",
                    x=0.5, y=0.5, showarrow=False,
                    font=dict(size=16)
                )
                fig.update_layout(
                    title="Server Communication Latency Trend",
                    xaxis_title="Request",
                    yaxis_title="Latency (ms)"
                )
                return fig

            latencies = [h['latency_ms'] for h in history]

            fig = go.Figure(data=[
                go.Scatter(
                    x=list(range(len(latencies))),
                    y=latencies,
                    mode='lines+markers',
                    name="Latency",
                    line=dict(color='rgb(231, 99, 250)', width=2),
                    marker=dict(size=4)
                )
            ])

            fig.update_layout(
                title="Server Communication Latency Trend (Last 50 Requests)",
                xaxis_title="Request Index",
                yaxis_title="Latency (ms)",
                showlegend=False,
                height=300
            )

            return fig

        except Exception as e:
            fig = go.Figure()
            fig.add_annotation(
                text=f"Error: {str(e)}",
                xref="paper", yref="paper",
                x=0.5, y=0.5, showarrow=False
            )
            return fig

    def _build_interface(self) -> gr.Blocks:
        """Build the Gradio interface"""

        with gr.Blocks(
            title="Split Learning Client",
            theme=self.theme,
            css=DEFAULT_CSS
        ) as demo:

            gr.Markdown(
                """
                # ğŸš€ Split Learning åˆ†å¸ƒå¼æ¨ç†å®¢æˆ·ç«¯

                **æ¶æ„**: Bottom(æœ¬åœ°) â†’ Trunk(è¿œç¨‹æœåŠ¡å™¨) â†’ Top(æœ¬åœ°)

                è¿™ä¸ªç•Œé¢å…è®¸ä½ ä½¿ç”¨åˆ†å¸ƒå¼æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼Œå¹¶æä¾›å®æ—¶ç›‘æ§åŠŸèƒ½ã€‚
                """
            )

            # Auto-refresh notice
            gr.Markdown(
                f"ğŸ”„ **ç›‘æ§æ•°æ®è‡ªåŠ¨åˆ·æ–°**: æ¯ {self.refresh_interval} ç§’æ›´æ–°ä¸€æ¬¡"
            )

            # Tabs for generation and monitoring
            with gr.Tabs():
                # Tab 1: Text Generation
                with gr.Tab("ğŸ“ æ–‡æœ¬ç”Ÿæˆ"):
                    self._build_generation_tab()
                gr.Markdown("### ğŸ“ æ–‡æœ¬ç”Ÿæˆ")

                with gr.Row():
                    with gr.Column(scale=3):
                        input_box = gr.Textbox(
                            label="è¾“å…¥ Prompt",
                            placeholder="ä¾‹å¦‚: The future of AI is",
                            lines=3
                        )

                    with gr.Column(scale=2):
                        gr.Markdown("**ç”Ÿæˆå‚æ•°**")
                        max_length_slider = gr.Slider(
                            minimum=1,
                            maximum=100,
                            value=20,
                            step=1,
                            label="æœ€å¤§ç”Ÿæˆé•¿åº¦ (tokens)"
                        )
                        temperature_slider = gr.Slider(
                            minimum=0.1,
                            maximum=2.0,
                            value=1.0,
                            step=0.1,
                            label="Temperature (åˆ›é€ æ€§)"
                        )
                        top_k_slider = gr.Slider(
                            minimum=0,
                            maximum=100,
                            value=50,
                            step=1,
                            label="Top-K (0=è´ªå©ªé‡‡æ ·)"
                        )
                        show_speed_check = gr.Checkbox(
                            label="æ˜¾ç¤ºç”Ÿæˆé€Ÿåº¦",
                            value=True
                        )

                with gr.Row():
                    generate_btn = gr.Button("â–¶ï¸ å¼€å§‹ç”Ÿæˆ", variant="primary", size="lg")
                    stop_btn = gr.Button("â¹ï¸ åœæ­¢ç”Ÿæˆ", variant="stop", size="lg")

                with gr.Row():
                    with gr.Column(scale=3):
                        output_box = gr.Textbox(
                            label="ç”Ÿæˆç»“æœ",
                            lines=8,
                            show_copy_button=True,
                            elem_classes=["output-box"]
                        )
                    with gr.Column(scale=1):
                        stats_box = gr.Textbox(
                            label="ç”Ÿæˆç»Ÿè®¡",
                            lines=8,
                            elem_classes=["stats-box"]
                        )

            # Examples
            gr.Markdown("### ğŸ’¡ ç¤ºä¾‹ Prompts")
            gr.Examples(
                examples=[
                    ["The future of AI is"],
                    ["Once upon a time, in a land far away,"],
                    ["To be or not to be,"],
                    ["In the beginning, there was"],
                    ["The quick brown fox"],
                ],
                inputs=input_box,
                label="ç‚¹å‡»ä½¿ç”¨ç¤ºä¾‹"
            )

            # Usage instructions
            with gr.Accordion("ğŸ“– ä½¿ç”¨è¯´æ˜", open=False):
                gr.Markdown(
                    """
                    1. **è¾“å…¥æ–‡æœ¬**: åœ¨è¾“å…¥æ¡†ä¸­è¾“å…¥ä½ çš„ prompt
                    2. **è°ƒæ•´å‚æ•°** (å¯é€‰):
                       - **æœ€å¤§é•¿åº¦**: ç”Ÿæˆçš„ token æ•°é‡
                       - **Temperature**: æ§åˆ¶éšæœºæ€§ï¼ˆè¶Šé«˜è¶Šéšæœºï¼‰
                       - **Top-K**: é™åˆ¶é‡‡æ ·èŒƒå›´ï¼ˆ0=è´ªå©ªé‡‡æ ·ï¼‰
                    3. **å¼€å§‹ç”Ÿæˆ**: ç‚¹å‡»"å¼€å§‹ç”Ÿæˆ"æŒ‰é’®
                    4. **åœæ­¢ç”Ÿæˆ**: ç‚¹å‡»"åœæ­¢ç”Ÿæˆ"æŒ‰é’®å¯éšæ—¶ä¸­æ­¢

                    **æ³¨æ„**: ç¡®ä¿å·²è¿æ¥åˆ°æœåŠ¡å™¨å¹¶åŠ è½½äº†æ¨¡å‹ã€‚
                    """
                )

            # Event bindings
            generate_btn.click(
                fn=self._generate_text_streaming,
                inputs=[
                    input_box,
                    max_length_slider,
                    temperature_slider,
                    top_k_slider,
                    show_speed_check
                ],
                outputs=[output_box, stats_box],
                show_progress="full",
                api_name="generate"
            )

            stop_btn.click(
                fn=self._stop_generation,
                inputs=[],
                outputs=stats_box
            )

        return demo

    def launch(
        self,
        share: bool = False,
        server_name: str = "127.0.0.1",
        server_port: int = 7860,
        inbrowser: bool = True,
        blocking: bool = True,
        **kwargs
    ):
        """
        Launch the Gradio UI

        Args:
            share: Whether to create a public Gradio link
            server_name: Server hostname to bind to
            server_port: Server port to use
            inbrowser: Whether to automatically open in browser
            blocking: Whether to block the main thread (False = run in background)
            **kwargs: Additional arguments passed to demo.launch()

        Returns:
            If blocking=False, returns the running demo instance
        """
        self.demo.queue()  # Enable streaming support

        if blocking:
            self.demo.launch(
                share=share,
                server_name=server_name,
                server_port=server_port,
                inbrowser=inbrowser,
                show_error=True,
                **kwargs
            )
        else:
            # Run in background thread
            def _launch():
                self.demo.launch(
                    share=share,
                    server_name=server_name,
                    server_port=server_port,
                    inbrowser=inbrowser,
                    show_error=True,
                    **kwargs
                )

            thread = threading.Thread(target=_launch, daemon=True)
            thread.start()
            return self.demo
