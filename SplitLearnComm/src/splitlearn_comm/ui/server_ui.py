"""
Server-side Gradio UI for monitoring and analytics
"""

import time
import threading
from typing import Any, Optional
from datetime import datetime

import gradio as gr
import pandas as pd
import plotly.graph_objects as go

from .components import get_theme, DEFAULT_CSS, StatsPanel


class ServerMonitoringUI:
    """
    Gradio UI for server-side monitoring and analytics

    Provides a real-time dashboard for monitoring server metrics including
    request statistics, compute times, success rates, and request history.

    Example:
        >>> from splitlearn_comm import GRPCComputeServer
        >>> from splitlearn_comm.ui import ServerMonitoringUI
        >>>
        >>> server = GRPCComputeServer(compute_fn, port=50051)
        >>> server.start()
        >>>
        >>> ui = ServerMonitoringUI(servicer=server.servicer)
        >>> ui.launch(share=False, server_port=7861)
    """

    def __init__(
        self,
        servicer: Any,
        theme: str = "default",
        refresh_interval: int = 2,
    ):
        """
        Args:
            servicer: ComputeServicer instance (must be running)
            theme: UI theme variant ("default", "dark", "light")
            refresh_interval: Dashboard refresh interval in seconds
        """
        self.servicer = servicer
        self.theme = get_theme(theme)
        self.refresh_interval = refresh_interval

        # Build the interface
        self.demo = self._build_interface()

    def _get_stats_text(self):
        """Get current statistics as formatted text"""
        try:
            metrics = self.servicer.get_metrics()

            stats_text = StatsPanel.format_server_stats(
                total_requests=metrics["total_requests"],
                success_rate=metrics["success_rate"],
                avg_compute_time=metrics["avg_compute_time_ms"],
                uptime_seconds=metrics["uptime_seconds"],
                active_connections=0  # Could be enhanced
            )

            return stats_text
        except Exception as e:
            return f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}"

    def _get_request_history_dataframe(self):
        """Get request history as a pandas DataFrame"""
        try:
            metrics = self.servicer.get_metrics()
            history = metrics["request_history"]

            if not history:
                return pd.DataFrame({
                    "Request ID": [],
                    "Timestamp": [],
                    "Status": [],
                    "Compute Time (ms)": [],
                    "Error": []
                })

            # Convert to DataFrame
            df = pd.DataFrame([
                {
                    "Request ID": req["request_id"],
                    "Timestamp": req["timestamp"].strftime("%H:%M:%S"),
                    "Status": "âœ… Success" if req["success"] else "âŒ Failed",
                    "Compute Time (ms)": f"{req['compute_time_ms']:.2f}",
                    "Error": req["error"] or ""
                }
                for req in history
            ])

            # Sort by Request ID descending (most recent first)
            df = df.sort_values("Request ID", ascending=False)

            return df

        except Exception as e:
            return pd.DataFrame({
                "Error": [str(e)]
            })

    def _get_compute_time_data(self):
        """Get compute time data for plotting"""
        try:
            metrics = self.servicer.get_metrics()
            history = metrics["request_history"]

            if not history:
                return pd.DataFrame({
                    "Request ID": [],
                    "Compute Time (ms)": []
                })

            # Get last 50 requests for plotting
            recent = list(history)[-50:]

            df = pd.DataFrame([
                {
                    "Request ID": req["request_id"],
                    "Compute Time (ms)": req["compute_time_ms"]
                }
                for req in recent
            ])

            return df

        except Exception as e:
            return pd.DataFrame({
                "Error": [str(e)]
            })

    def _get_logs_text(self, level_filter: str = "ALL") -> str:
        """Get formatted logs as text"""
        try:
            logs = self.servicer.get_logs(
                level_filter=level_filter if level_filter != "ALL" else None,
                limit=100
            )

            if not logs:
                return f"No {level_filter} logs available"

            # Format logs
            formatted_lines = []
            for log in logs:
                timestamp_str = log['timestamp'].strftime("%H:%M:%S.%f")[:-3]
                level = log['level']
                message = log['message']

                # Add level emoji
                level_icon = {
                    'DEBUG': 'ğŸ”',
                    'INFO': 'â„¹ï¸',
                    'WARNING': 'âš ï¸',
                    'ERROR': 'âŒ'
                }.get(level, 'ğŸ“')

                line = f"[{timestamp_str}] {level_icon} {level:8} {message}"
                formatted_lines.append(line)

            return "\n".join(formatted_lines)

        except Exception as e:
            return f"âŒ Error fetching logs: {str(e)}"

    def _get_latency_stats_dataframe(self) -> pd.DataFrame:
        """Get latency statistics as DataFrame"""
        try:
            metrics = self.servicer.get_metrics()
            stats = metrics.get("latency_stats", {})

            if stats.get('count', 0) == 0:
                return pd.DataFrame({
                    "Metric": ["No data"],
                    "Value": [""]
                })

            data = {
                "Metric": [
                    "Count",
                    "Mean",
                    "Median (P50)",
                    "P95",
                    "P99",
                    "Min",
                    "Max",
                    "Std Dev"
                ],
                "Value": [
                    f"{stats['count']} requests",
                    f"{stats['mean']:.2f} ms",
                    f"{stats['median']:.2f} ms",
                    f"{stats['p95']:.2f} ms",
                    f"{stats['p99']:.2f} ms",
                    f"{stats['min']:.2f} ms",
                    f"{stats['max']:.2f} ms",
                    f"{stats['std']:.2f} ms"
                ]
            }

            return pd.DataFrame(data)

        except Exception as e:
            return pd.DataFrame({
                "Error": [str(e)]
            })

    def _get_latency_distribution_plot(self):
        """Get latency distribution histogram"""
        try:
            metrics = self.servicer.get_metrics()
            dist = metrics.get("latency_distribution", {})

            bin_edges = dist.get("bin_edges", [])
            counts = dist.get("counts", [])

            if not bin_edges or not counts:
                # Return empty plot
                fig = go.Figure()
                fig.add_annotation(
                    text="No latency data available",
                    xref="paper", yref="paper",
                    x=0.5, y=0.5, showarrow=False,
                    font=dict(size=16)
                )
                fig.update_layout(
                    title="Latency Distribution",
                    xaxis_title="Latency (ms)",
                    yaxis_title="Count"
                )
                return fig

            # Calculate bin centers
            bin_centers = [(bin_edges[i] + bin_edges[i+1]) / 2 for i in range(len(counts))]

            fig = go.Figure(data=[
                go.Bar(
                    x=bin_centers,
                    y=counts,
                    name="Latency",
                    marker_color='rgb(55, 126, 184)'
                )
            ])

            fig.update_layout(
                title="Latency Distribution",
                xaxis_title="Latency (ms)",
                yaxis_title="Count",
                showlegend=False,
                height=300
            )

            return fig

        except Exception as e:
            fig = go.Figure()
            fig.add_annotation(
                text=f"Error: {str(e)}",
                xref="paper", yref="paper",
                x=0.5, y=0.5, showarrow=False
            )
            return fig

    def _get_latency_trend_plot(self):
        """Get latency trend over time"""
        try:
            metrics = self.servicer.get_metrics()
            history = metrics.get("latency_history", [])

            if not history:
                fig = go.Figure()
                fig.add_annotation(
                    text="No latency history available",
                    xref="paper", yref="paper",
                    x=0.5, y=0.5, showarrow=False,
                    font=dict(size=16)
                )
                fig.update_layout(
                    title="Latency Trend",
                    xaxis_title="Request",
                    yaxis_title="Latency (ms)"
                )
                return fig

            timestamps = [h['timestamp'] for h in history]
            latencies = [h['latency_ms'] for h in history]

            fig = go.Figure(data=[
                go.Scatter(
                    x=list(range(len(latencies))),
                    y=latencies,
                    mode='lines+markers',
                    name="Latency",
                    line=dict(color='rgb(231, 99, 250)', width=2),
                    marker=dict(size=4)
                )
            ])

            fig.update_layout(
                title="Latency Trend (Last 50 Requests)",
                xaxis_title="Request Index",
                yaxis_title="Latency (ms)",
                showlegend=False,
                height=300
            )

            return fig

        except Exception as e:
            fig = go.Figure()
            fig.add_annotation(
                text=f"Error: {str(e)}",
                xref="paper", yref="paper",
                x=0.5, y=0.5, showarrow=False
            )
            return fig

    def _build_interface(self) -> gr.Blocks:
        """Build the Gradio monitoring interface"""

        with gr.Blocks(
            title="Split Learning Server Monitoring",
            theme=self.theme,
            css=DEFAULT_CSS
        ) as demo:

            gr.Markdown(
                """
                # ğŸ“Š Split Learning æœåŠ¡å™¨ç»¼åˆç›‘æ§é¢æ¿

                å®æ—¶ç›‘æ§æœåŠ¡å™¨æ€§èƒ½ã€æ—¥å¿—ã€å»¶è¿Ÿåˆ†æå’Œè¯·æ±‚ç»Ÿè®¡
                """
            )

            # Auto-refresh notice
            gr.Markdown(
                f"ğŸ”„ **è‡ªåŠ¨åˆ·æ–°**: æ¯ {self.refresh_interval} ç§’æ›´æ–°ä¸€æ¬¡æ•°æ®"
            )

            # Tabs for different monitoring views
            with gr.Tabs():
                # Tab 1: Overview (existing functionality)
                with gr.Tab("ğŸ“ˆ æ¦‚è§ˆ"):
                    # Statistics Panel
                    with gr.Group():
                        gr.Markdown("### æœåŠ¡å™¨ç»Ÿè®¡")
                        stats_box = gr.Textbox(
                            label="å®æ—¶ç»Ÿè®¡",
                            value=self._get_stats_text(),
                            lines=8,
                            elem_classes=["stats-box"],
                            interactive=False
                        )

                    # Compute Time Graph
                    with gr.Group():
                        gr.Markdown("### è®¡ç®—æ—¶é—´è¶‹åŠ¿")
                        compute_time_plot = gr.LinePlot(
                            value=self._get_compute_time_data(),
                            x="Request ID",
                            y="Compute Time (ms)",
                            title="Recent Compute Times",
                            width=800,
                            height=300
                        )

                    # Request History Table
                    with gr.Group():
                        gr.Markdown("### è¯·æ±‚å†å²")
                        history_table = gr.Dataframe(
                            value=self._get_request_history_dataframe(),
                            label="æœ€è¿‘çš„è¯·æ±‚",
                            max_rows=20,
                            interactive=False
                        )

                # Tab 2: Real-time Logs
                with gr.Tab("ğŸ“ å®æ—¶æ—¥å¿—"):
                    gr.Markdown("### æœåŠ¡å™¨æ—¥å¿—")

                    with gr.Row():
                        log_level_filter = gr.Dropdown(
                            choices=["ALL", "DEBUG", "INFO", "WARNING", "ERROR"],
                            value="INFO",
                            label="æ—¥å¿—çº§åˆ«",
                            scale=1
                        )

                    logs_display = gr.Textbox(
                        label="æ—¥å¿—è¾“å‡º",
                        value=self._get_logs_text("INFO"),
                        lines=25,
                        max_lines=25,
                        interactive=False,
                        elem_classes=["stats-box"]
                    )

                    with gr.Accordion("ğŸ“– æ—¥å¿—è¯´æ˜", open=False):
                        gr.Markdown(
                            """
                            ## æ—¥å¿—çº§åˆ«è¯´æ˜

                            - ğŸ” **DEBUG**: è°ƒè¯•ä¿¡æ¯
                            - â„¹ï¸ **INFO**: ä¸€èˆ¬ä¿¡æ¯ï¼ˆé»˜è®¤ï¼‰
                            - âš ï¸ **WARNING**: è­¦å‘Šä¿¡æ¯
                            - âŒ **ERROR**: é”™è¯¯ä¿¡æ¯

                            ## åŠŸèƒ½

                            - è‡ªåŠ¨åˆ·æ–°æœ€æ–°æ—¥å¿—
                            - æ”¯æŒæŒ‰çº§åˆ«è¿‡æ»¤
                            - æ˜¾ç¤ºæœ€è¿‘ 100 æ¡æ—¥å¿—
                            """
                        )

                # Tab 3: Detailed Latency Analysis
                with gr.Tab("â±ï¸ å»¶è¿Ÿåˆ†æ"):
                    gr.Markdown("### è¯¦ç»†å»¶è¿Ÿç»Ÿè®¡ä¸åˆ†æ")

                    # Latency Statistics Table
                    with gr.Group():
                        gr.Markdown("#### å»¶è¿Ÿç»Ÿè®¡")
                        latency_stats_table = gr.Dataframe(
                            value=self._get_latency_stats_dataframe(),
                            label="ç»Ÿè®¡æŒ‡æ ‡",
                            interactive=False
                        )

                    # Latency Distribution Histogram
                    with gr.Group():
                        gr.Markdown("#### å»¶è¿Ÿåˆ†å¸ƒç›´æ–¹å›¾")
                        latency_dist_plot = gr.Plot(
                            value=self._get_latency_distribution_plot(),
                            label="åˆ†å¸ƒå›¾"
                        )

                    # Latency Trend
                    with gr.Group():
                        gr.Markdown("#### å»¶è¿Ÿè¶‹åŠ¿")
                        latency_trend_plot = gr.Plot(
                            value=self._get_latency_trend_plot(),
                            label="è¶‹åŠ¿å›¾"
                        )

                    with gr.Accordion("ğŸ“– å»¶è¿Ÿåˆ†æè¯´æ˜", open=False):
                        gr.Markdown(
                            """
                            ## ç»Ÿè®¡æŒ‡æ ‡è¯´æ˜

                            - **Mean**: å¹³å‡å»¶è¿Ÿ
                            - **Median (P50)**: ä¸­ä½æ•°å»¶è¿Ÿï¼Œ50% çš„è¯·æ±‚å»¶è¿Ÿä½äºæ­¤å€¼
                            - **P95**: 95% çš„è¯·æ±‚å»¶è¿Ÿä½äºæ­¤å€¼
                            - **P99**: 99% çš„è¯·æ±‚å»¶è¿Ÿä½äºæ­¤å€¼
                            - **Min/Max**: æœ€å°/æœ€å¤§å»¶è¿Ÿ
                            - **Std Dev**: æ ‡å‡†å·®ï¼Œè¡¨ç¤ºå»¶è¿Ÿæ³¢åŠ¨ç¨‹åº¦

                            ## å›¾è¡¨è¯´æ˜

                            - **åˆ†å¸ƒç›´æ–¹å›¾**: æ˜¾ç¤ºå»¶è¿Ÿçš„åˆ†å¸ƒæƒ…å†µ
                            - **è¶‹åŠ¿å›¾**: æ˜¾ç¤ºæœ€è¿‘ 50 ä¸ªè¯·æ±‚çš„å»¶è¿Ÿå˜åŒ–
                            """
                        )

            # Refresh button (global)
            gr.Markdown("---")
            with gr.Row():
                refresh_btn = gr.Button("ğŸ”„ æ‰‹åŠ¨åˆ·æ–°æ‰€æœ‰æ•°æ®", variant="primary")

            # Usage instructions
            with gr.Accordion("ğŸ“– ä½¿ç”¨è¯´æ˜", open=False):
                gr.Markdown(
                    """
                    ## ç›‘æ§é¢æ¿è¯´æ˜

                    æœ¬ç›‘æ§é¢æ¿åŒ…å« 3 ä¸ªæ ‡ç­¾é¡µï¼š

                    1. **æ¦‚è§ˆ**: æœåŠ¡å™¨ç»Ÿè®¡ã€è®¡ç®—æ—¶é—´è¶‹åŠ¿ã€è¯·æ±‚å†å²
                    2. **å®æ—¶æ—¥å¿—**: æŸ¥çœ‹æœåŠ¡å™¨å®æ—¶æ—¥å¿—ï¼Œæ”¯æŒçº§åˆ«è¿‡æ»¤
                    3. **å»¶è¿Ÿåˆ†æ**: è¯¦ç»†çš„å»¶è¿Ÿç»Ÿè®¡ï¼ˆP95/P99ï¼‰ã€åˆ†å¸ƒå›¾å’Œè¶‹åŠ¿å›¾

                    ## è‡ªåŠ¨åˆ·æ–°

                    æ‰€æœ‰æ•°æ®æ¯ {0} ç§’è‡ªåŠ¨æ›´æ–°ï¼Œä¹Ÿå¯ç‚¹å‡»"æ‰‹åŠ¨åˆ·æ–°"æŒ‰é’®ç«‹å³æ›´æ–°ã€‚
                    """.format(self.refresh_interval)
                )

            # Event bindings - log level filter
            log_level_filter.change(
                fn=self._get_logs_text,
                inputs=[log_level_filter],
                outputs=[logs_display]
            )

            # Event bindings - manual refresh all
            refresh_btn.click(
                fn=lambda level: (
                    self._get_stats_text(),
                    self._get_compute_time_data(),
                    self._get_request_history_dataframe(),
                    self._get_logs_text(level),
                    self._get_latency_stats_dataframe(),
                    self._get_latency_distribution_plot(),
                    self._get_latency_trend_plot()
                ),
                inputs=[log_level_filter],
                outputs=[
                    stats_box,
                    compute_time_plot,
                    history_table,
                    logs_display,
                    latency_stats_table,
                    latency_dist_plot,
                    latency_trend_plot
                ]
            )

            # Auto-refresh using demo.load
            demo.load(
                fn=lambda level: (
                    self._get_stats_text(),
                    self._get_compute_time_data(),
                    self._get_request_history_dataframe(),
                    self._get_logs_text(level),
                    self._get_latency_stats_dataframe(),
                    self._get_latency_distribution_plot(),
                    self._get_latency_trend_plot()
                ),
                inputs=[log_level_filter],
                outputs=[
                    stats_box,
                    compute_time_plot,
                    history_table,
                    logs_display,
                    latency_stats_table,
                    latency_dist_plot,
                    latency_trend_plot
                ],
                every=self.refresh_interval
            )

        return demo

    def launch(
        self,
        share: bool = False,
        server_name: str = "127.0.0.1",
        server_port: int = 7861,
        inbrowser: bool = True,
        blocking: bool = True,
        **kwargs
    ):
        """
        Launch the monitoring UI

        Args:
            share: Whether to create a public Gradio link
            server_name: Server hostname to bind to
            server_port: Server port to use
            inbrowser: Whether to automatically open in browser
            blocking: Whether to block the main thread (False = run in background)
            **kwargs: Additional arguments passed to demo.launch()

        Returns:
            If blocking=False, returns the running demo instance
        """
        if blocking:
            self.demo.launch(
                share=share,
                server_name=server_name,
                server_port=server_port,
                inbrowser=inbrowser,
                show_error=True,
                **kwargs
            )
        else:
            # Run in background thread
            def _launch():
                self.demo.launch(
                    share=share,
                    server_name=server_name,
                    server_port=server_port,
                    inbrowser=inbrowser,
                    show_error=True,
                    **kwargs
                )

            thread = threading.Thread(target=_launch, daemon=True)
            thread.start()
            return self.demo
