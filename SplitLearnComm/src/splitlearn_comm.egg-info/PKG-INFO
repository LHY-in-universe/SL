Metadata-Version: 2.4
Name: splitlearn-comm
Version: 1.0.0
Summary: A gRPC-based communication library for distributed computing
Author: SplitLearn Contributors
License: MIT
Project-URL: Homepage, https://github.com/yourusername/SplitLearnComm
Project-URL: Repository, https://github.com/yourusername/SplitLearnComm
Project-URL: Issues, https://github.com/yourusername/SplitLearnComm/issues
Keywords: grpc,distributed-computing,deep-learning,communication,rpc
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: System :: Distributed Computing
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: torch>=2.0.0
Requires-Dist: grpcio<1.70.0,>=1.60.0
Requires-Dist: protobuf<5.0.0,>=4.25.0
Requires-Dist: numpy>=1.24.0
Provides-Extra: client
Requires-Dist: grpcio>=1.76.0; extra == "client"
Requires-Dist: protobuf>=4.25.0; extra == "client"
Provides-Extra: server
Requires-Dist: grpcio>=1.76.0; extra == "server"
Requires-Dist: protobuf>=4.25.0; extra == "server"
Provides-Extra: ui
Requires-Dist: gradio<5.0.0,>=3.50.0; extra == "ui"
Requires-Dist: pandas>=1.5.0; extra == "ui"
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=3.0.0; extra == "dev"
Requires-Dist: pytest-timeout>=2.1.0; extra == "dev"
Requires-Dist: pytest-xdist>=3.0.0; extra == "dev"
Requires-Dist: black>=22.0.0; extra == "dev"
Requires-Dist: isort>=5.12.0; extra == "dev"
Requires-Dist: flake8>=4.0.0; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"
Requires-Dist: grpcio-tools>=1.76.0; extra == "dev"
Requires-Dist: sphinx>=5.0.0; extra == "dev"
Requires-Dist: sphinx-rtd-theme>=1.2.0; extra == "dev"
Requires-Dist: myst-parser>=1.0.0; extra == "dev"
Requires-Dist: build>=0.10.0; extra == "dev"
Requires-Dist: twine>=4.0.0; extra == "dev"
Requires-Dist: ipython>=8.0.0; extra == "dev"
Requires-Dist: ipdb>=0.13.0; extra == "dev"
Requires-Dist: gradio<5.0.0,>=3.50.0; extra == "dev"
Requires-Dist: pandas>=1.5.0; extra == "dev"
Provides-Extra: all
Requires-Dist: splitlearn-comm[client,dev,server,ui]; extra == "all"

# splitlearn-comm

A high-performance gRPC-based communication library for distributed deep learning.

## Features

- ðŸš€ **High Performance**: Optimized tensor serialization using binary format
- ðŸ”Œ **Model Agnostic**: Completely decoupled from specific models via `ComputeFunction` abstraction
- ðŸ”„ **Reliable**: Built-in retry mechanisms with exponential backoff
- ðŸ›¡ï¸ **Robust**: Comprehensive error handling and health checks
- ðŸ“Š **Observable**: Performance metrics and statistics tracking
- ðŸŒ **Flexible**: Supports single-machine, LAN, and WAN deployments

## Installation

```bash
pip install splitlearn-comm
```

Or install from source:

```bash
git clone https://github.com/yourusername/splitlearn-comm.git
cd splitlearn-comm
pip install -e .
```

## Quick Start

### Server Example

```python
import torch
from splitlearn_comm import GRPCComputeServer
from splitlearn_comm.core import ModelComputeFunction

# Create a simple model
model = torch.nn.Sequential(
    torch.nn.Linear(768, 768),
    torch.nn.ReLU(),
    torch.nn.Linear(768, 768)
)

# Wrap in ComputeFunction
compute_fn = ModelComputeFunction(model, device="cuda")

# Start server
server = GRPCComputeServer(
    compute_fn=compute_fn,
    host="0.0.0.0",
    port=50051
)

server.start()
server.wait_for_termination()
```

### Client Example

```python
import torch
from splitlearn_comm import GRPCComputeClient

# Connect to server
client = GRPCComputeClient("localhost:50051")
client.connect()

# Perform computation
input_tensor = torch.randn(1, 10, 768)
output_tensor = client.compute(input_tensor)

print(f"Output shape: {output_tensor.shape}")

# Get statistics
stats = client.get_statistics()
print(f"Total requests: {stats['total_requests']}")
print(f"Avg network time: {stats['avg_network_time_ms']:.2f}ms")

client.close()
```

## Core Concepts

### ComputeFunction

The `ComputeFunction` abstraction allows you to use any computation logic:

```python
from splitlearn_comm.core import ComputeFunction

class MyComputeFunction(ComputeFunction):
    def __init__(self, model):
        self.model = model

    def compute(self, input_tensor):
        # Your custom logic here
        with torch.no_grad():
            return self.model(input_tensor)

    def get_info(self):
        return {"model_name": "MyModel", "device": "cuda"}
```

### Retry Strategy

Built-in retry mechanisms for reliability:

```python
from splitlearn_comm import GRPCComputeClient, ExponentialBackoff

# Custom retry strategy
retry = ExponentialBackoff(
    max_retries=5,
    initial_delay=1.0,
    max_delay=30.0
)

client = GRPCComputeClient(
    "localhost:50051",
    retry_strategy=retry
)
```

## Advanced Usage

### Compressed Communication

For bandwidth-constrained scenarios:

```python
from splitlearn_comm.core import CompressedTensorCodec

codec = CompressedTensorCodec(compression_level=6)

server = GRPCComputeServer(
    compute_fn=compute_fn,
    codec=codec,
    port=50051
)
```

### Context Manager Support

```python
# Server
with GRPCComputeServer(compute_fn, port=50051) as server:
    # Server automatically starts and stops
    pass

# Client
with GRPCComputeClient("localhost:50051") as client:
    output = client.compute(input_tensor)
```

### Health Checks and Service Info

```python
# Health check
is_healthy = client.health_check()

# Get service information
info = client.get_service_info()
print(f"Service: {info['service_name']}")
print(f"Version: {info['version']}")
print(f"Device: {info['device']}")
print(f"Total requests: {info['total_requests']}")
```

## API Documentation

See [docs/api.md](docs/api.md) for complete API reference.

## Examples

- [examples/simple_server.py](examples/simple_server.py) - Basic server
- [examples/simple_client.py](examples/simple_client.py) - Basic client
- [examples/custom_service.py](examples/custom_service.py) - Custom compute function

## Performance

splitlearn-comm uses optimized binary serialization for tensors:

- **Bytes format**: 4x faster than protobuf's `repeated float`
- **Zero-copy**: Minimal overhead for numpy/torch conversion
- **Compression**: Optional zlib compression for WAN scenarios

## Requirements

- Python >= 3.8
- PyTorch >= 2.0.0
- gRPC >= 1.50.0
- NumPy >= 1.24.0

## Development

```bash
# Clone repository
git clone https://github.com/yourusername/splitlearn-comm.git
cd splitlearn-comm

# Install in development mode
pip install -e ".[dev]"

# Compile protobuf
bash scripts/compile_proto.sh

# Run tests
pytest tests/
```

## License

MIT License - see [LICENSE](LICENSE) for details.

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## Citation

If you use splitlearn-comm in your research, please cite:

```bibtex
@software{splitlearn_comm2024,
  title = {splitlearn-comm: A gRPC Communication Library for Distributed Deep Learning},
  author = {SplitLearn Contributors},
  year = {2024},
  url = {https://github.com/yourusername/splitlearn-comm}
}
```
