# ä¼˜åŒ–æ–¹æ³•ä½¿ç”¨å¯¹æ¯”ï¼šå…¨æ¨¡å‹ vs åˆ†æ‹†å­¦ä¹ ï¼ˆSLï¼‰

## ğŸ“Š ä¼˜åŒ–æ–¹æ³•ä½¿ç”¨æƒ…å†µæ€»è§ˆ

| ä¼˜åŒ–æ–¹æ³• | å…¨æ¨¡å‹ | SLå®¢æˆ·ç«¯ | SLæœåŠ¡å™¨ | SLæ ¸å¿ƒåº“ |
|---------|--------|---------|---------|---------|
| **1. Hugging Face ä¼˜åŒ–** |
| `low_cpu_mem_usage=True` | âœ… | âœ… | âœ… | âœ… |
| `use_fast=True` | âœ… | âŒ | âŒ | âŒ |
| **2. PyTorch ä¼˜åŒ–** |
| `torch.inference_mode()` | âœ… | âŒ | â“ | â“ |
| `torch.no_grad()` | âŒ | âœ… | â“ | â“ |
| `torch.set_float32_matmul_precision()` | âœ… | âŒ | âŒ | âŒ |
| `torch.backends.cudnn.benchmark` | âœ… | âŒ | âŒ | âŒ |
| `torch.compile()` | âœ… (å¯é€‰) | âœ… (å¯é€‰) | â“ | âŒ |
| **3. ç”Ÿæˆè¿‡ç¨‹ä¼˜åŒ–** |
| **KV Cache** (`use_cache=True`) | âœ… | âœ… | âœ… | âœ… |
| `output_attentions=False` | âœ… | âŒ | â“ | â“ |
| `output_hidden_states=False` | âœ… | âŒ | â“ | â“ |
| **4. é‡‡æ ·ä¼˜åŒ–** |
| ä¼˜åŒ–çš„ Top-k é‡‡æ · | âœ… | âŒ | âŒ | âŒ |

**å›¾ä¾‹**:
- âœ… = å·²ä½¿ç”¨
- âŒ = æœªä½¿ç”¨
- â“ = ä¸ç¡®å®šï¼ˆéœ€è¦æ£€æŸ¥æœåŠ¡å™¨ä»£ç ï¼‰

---

## 1ï¸âƒ£ **å…¨æ¨¡å‹ï¼ˆgpt2_full_model_gradio.pyï¼‰**

### âœ… å·²ä½¿ç”¨çš„ä¼˜åŒ–ï¼ˆ9ç§ï¼‰

#### Hugging Face ä¼˜åŒ–ï¼ˆ2ç§ï¼‰
1. âœ… **`low_cpu_mem_usage=True`** (ç¬¬132è¡Œ)
   ```python
   model_kwargs = {
       "low_cpu_mem_usage": LOW_CPU_MEM_USAGE,
   }
   ```

2. âœ… **`use_fast=True`** (ç¬¬170è¡Œ)
   ```python
   tokenizer = AutoTokenizer.from_pretrained(
       model_id,
       cache_dir=MODEL_CACHE,
       use_fast=True,
   )
   ```

#### PyTorch ä¼˜åŒ–ï¼ˆ3ç§ï¼‰
3. âœ… **`torch.inference_mode()`** (ç¬¬340è¡Œ)
   ```python
   with torch.inference_mode():
       for step in range(max_new_tokens):
           outputs = model(...)
   ```

4. âœ… **`torch.set_float32_matmul_precision("medium")`** (ç¬¬189è¡Œ)
   ```python
   if hasattr(torch, 'set_float32_matmul_precision'):
       torch.set_float32_matmul_precision(MATMUL_PRECISION)
   ```

5. âœ… **`torch.backends.cudnn.benchmark = True`** (ç¬¬197è¡Œ)
   ```python
   if device == "cuda":
       torch.backends.cudnn.benchmark = True
   ```

#### ç”Ÿæˆè¿‡ç¨‹ä¼˜åŒ–ï¼ˆ3ç§ï¼‰
6. âœ… **KV Cache** (`use_cache=True`) (ç¬¬365è¡Œ)
   ```python
   model_kwargs = {
       "past_key_values": past_key_values,
       "use_cache": True,
   }
   ```

7. âœ… **`output_attentions=False`** (ç¬¬366è¡Œ)
   ```python
   "output_attentions": False,
   ```

8. âœ… **`output_hidden_states=False`** (ç¬¬367è¡Œ)
   ```python
   "output_hidden_states": False,
   ```

#### é‡‡æ ·ä¼˜åŒ–ï¼ˆ1ç§ï¼‰
9. âœ… **ä¼˜åŒ–çš„ Top-k é‡‡æ ·** (ç¬¬359-363è¡Œ)
   ```python
   top_k_logits, top_k_indices = torch.topk(logits, top_k_value)
   logits_filtered.scatter_(0, top_k_indices, top_k_logits)
   ```

---

## 2ï¸âƒ£ **åˆ†æ‹†å­¦ä¹  - å®¢æˆ·ç«¯ï¼ˆgpt2_client_gradio_grpc.pyï¼‰**

### âœ… å·²ä½¿ç”¨çš„ä¼˜åŒ–ï¼ˆ3ç§ï¼‰

#### Hugging Face ä¼˜åŒ–ï¼ˆ1ç§ï¼‰
1. âœ… **`low_cpu_mem_usage=True`** (é€šè¿‡ `load_split_model`)
   ```python
   bottom, _, top = load_split_model(
       model_type="gpt2",
       ...
   )
   # load_split_model å†…éƒ¨ä½¿ç”¨ low_cpu_mem_usage=True
   ```

#### PyTorch ä¼˜åŒ–ï¼ˆ1ç§ï¼‰
2. âš ï¸ **`torch.no_grad()`** (ç¬¬158è¡Œ) - **ä¸æ˜¯æœ€ä¼˜**
   ```python
   with torch.no_grad():  # åº”è¯¥ä½¿ç”¨ inference_mode()
       for step in range(max_new_tokens):
   ```
   **é—®é¢˜**: ä½¿ç”¨ `no_grad()` è€Œä¸æ˜¯ `inference_mode()`ï¼Œæ€§èƒ½ç•¥å·®

3. âœ… **`torch.compile()`** (ç¬¬94-95è¡Œï¼Œå¯é€‰)
   ```python
   if hasattr(torch, 'compile') and device == "cuda":
       bottom = torch.compile(bottom, mode="reduce-overhead")
       top = torch.compile(top, mode="reduce-overhead")
   ```

#### ç”Ÿæˆè¿‡ç¨‹ä¼˜åŒ–ï¼ˆ1ç§ï¼‰
4. âœ… **KV Cache** (`use_cache=True`) (ç¬¬189è¡Œ)
   ```python
   trunk_out, present_kv, timing = client.compute_with_cache(
       bottom_out.to("cpu"),
       past_key_values=past_key_values,
       use_cache=True,
   )
   ```

### âŒ æœªä½¿ç”¨çš„ä¼˜åŒ–ï¼ˆ6ç§ï¼‰

1. âŒ **`use_fast=True`** - åˆ†è¯å™¨æœªä½¿ç”¨å¿«é€Ÿç‰ˆæœ¬
2. âŒ **`torch.inference_mode()`** - ä½¿ç”¨äº† `no_grad()` æ›¿ä»£
3. âŒ **`torch.set_float32_matmul_precision()`** - æœªè®¾ç½®
4. âŒ **`torch.backends.cudnn.benchmark`** - æœªè®¾ç½®
5. âŒ **`output_attentions=False`** - æœªæ˜¾å¼è®¾ç½®ï¼ˆå¯èƒ½ä½¿ç”¨é»˜è®¤å€¼ï¼‰
6. âŒ **ä¼˜åŒ–çš„ Top-k é‡‡æ ·** - ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼

---

## 3ï¸âƒ£ **åˆ†æ‹†å­¦ä¹  - æ ¸å¿ƒåº“ï¼ˆSplitLearnCoreï¼‰**

### âœ… å·²ä½¿ç”¨çš„ä¼˜åŒ–ï¼ˆ2ç§ï¼‰

#### Hugging Face ä¼˜åŒ–ï¼ˆ1ç§ï¼‰
1. âœ… **`low_cpu_mem_usage=True`** (quickstart.py ç¬¬108è¡Œ)
   ```python
   def load_full_model(
       ...
       low_cpu_mem_usage: bool = True,  # é»˜è®¤å¯ç”¨
   ):
       model = AutoModelForCausalLM.from_pretrained(
           ...
           "low_cpu_mem_usage": low_cpu_mem_usage,
       )
   ```

#### ç”Ÿæˆè¿‡ç¨‹ä¼˜åŒ–ï¼ˆ1ç§ï¼‰
2. âœ… **KV Cache** (`use_cache=True`) - åœ¨ core æ¨¡å—ä¸­æ”¯æŒ
   - `core/bottom.py`: æ”¯æŒ `use_cache` å‚æ•°
   - `core/trunk.py`: æ”¯æŒ `use_cache` å‚æ•°
   - `core/top.py`: æ”¯æŒ `use_cache` å‚æ•°

### âŒ æœªä½¿ç”¨çš„ä¼˜åŒ–ï¼ˆ7ç§ï¼‰

1. âŒ **`use_fast=True`** - åˆ†è¯å™¨æœªä½¿ç”¨å¿«é€Ÿç‰ˆæœ¬
2. âŒ **`torch.inference_mode()`** - æœªä½¿ç”¨
3. âŒ **`torch.set_float32_matmul_precision()`** - æœªè®¾ç½®
4. âŒ **`torch.backends.cudnn.benchmark`** - æœªè®¾ç½®
5. âŒ **`output_attentions=False`** - æœªæ˜¾å¼è®¾ç½®
6. âŒ **`output_hidden_states=False`** - æœªæ˜¾å¼è®¾ç½®
7. âŒ **ä¼˜åŒ–çš„ Top-k é‡‡æ ·** - æœªå®ç°

---

## ğŸ“Š **å¯¹æ¯”æ€»ç»“**

### å…¨æ¨¡å‹ vs SL å®¢æˆ·ç«¯

| ç±»åˆ« | å…¨æ¨¡å‹ | SLå®¢æˆ·ç«¯ | å·®å¼‚ |
|------|--------|---------|------|
| **Hugging Face ä¼˜åŒ–** | 2/2 | 1/2 | âŒ ç¼ºå°‘ `use_fast` |
| **PyTorch ä¼˜åŒ–** | 3/3 | 1/3 | âŒ ç¼ºå°‘ 2 ç§ï¼Œä¸”ä½¿ç”¨ `no_grad` è€Œé `inference_mode` |
| **ç”Ÿæˆè¿‡ç¨‹ä¼˜åŒ–** | 3/3 | 1/3 | âŒ ç¼ºå°‘ 2 ç§ |
| **é‡‡æ ·ä¼˜åŒ–** | 1/1 | 0/1 | âŒ ç¼ºå°‘ä¼˜åŒ–çš„ Top-k |
| **æ€»è®¡** | **9/9** | **3/9** | **ç¼ºå°‘ 6 ç§ä¼˜åŒ–** |

### æ€§èƒ½å½±å“

**å…¨æ¨¡å‹**: ä½¿ç”¨ 9 ç§ä¼˜åŒ–ï¼Œæ€§èƒ½æœ€ä¼˜
- é¦–æ¬¡ token: æå‡ 15-25%
- åç»­ token: æå‡ 500-1000%ï¼ˆKV Cacheï¼‰
- å†…å­˜: é™ä½ 25-40%

**SL å®¢æˆ·ç«¯**: ä½¿ç”¨ 3 ç§ä¼˜åŒ–ï¼Œæ€§èƒ½æ¬¡ä¼˜
- é¦–æ¬¡ token: æå‡ 5-10%ï¼ˆä¸»è¦æ¥è‡ª KV Cacheï¼‰
- åç»­ token: æå‡ 500-1000%ï¼ˆKV Cacheï¼‰
- å†…å­˜: é™ä½ 10-15%ï¼ˆä»… low_cpu_mem_usageï¼‰

---

## ğŸ’¡ **æ”¹è¿›å»ºè®®**

### å¯¹äº SL å®¢æˆ·ç«¯ï¼Œå¯ä»¥æ·»åŠ ï¼š

1. âœ… **`use_fast=True`** - å¿«é€Ÿåˆ†è¯å™¨
   ```python
   tokenizer = AutoTokenizer.from_pretrained(
       model_id,
       cache_dir=MODEL_CACHE,
       use_fast=True,  # æ·»åŠ 
   )
   ```

2. âœ… **`torch.inference_mode()`** - æ›¿æ¢ `no_grad()`
   ```python
   with torch.inference_mode():  # æ›¿æ¢ no_grad()
       for step in range(max_new_tokens):
   ```

3. âœ… **`torch.set_float32_matmul_precision("medium")`** - çŸ©é˜µä¹˜æ³•ä¼˜åŒ–
   ```python
   if hasattr(torch, 'set_float32_matmul_precision'):
       torch.set_float32_matmul_precision("medium")
   ```

4. âœ… **`torch.backends.cudnn.benchmark = True`** - CUDA ä¼˜åŒ–
   ```python
   if device == "cuda":
       torch.backends.cudnn.benchmark = True
   ```

5. âœ… **ä¼˜åŒ–çš„ Top-k é‡‡æ ·** - é‡‡æ ·ç®—æ³•ä¼˜åŒ–
   ```python
   top_k_logits, top_k_indices = torch.topk(logits, top_k_value)
   logits_filtered.scatter_(0, top_k_indices, top_k_logits)
   ```

---

## âœ… **ç»“è®º**

- **å…¨æ¨¡å‹**: ä½¿ç”¨äº† **9/9 ç§ä¼˜åŒ–**ï¼ˆ100%ï¼‰
- **SL å®¢æˆ·ç«¯**: ä½¿ç”¨äº† **3/9 ç§ä¼˜åŒ–**ï¼ˆ33%ï¼‰
- **SL æ ¸å¿ƒåº“**: ä½¿ç”¨äº† **2/9 ç§ä¼˜åŒ–**ï¼ˆ22%ï¼‰

**å»ºè®®**: SL å®¢æˆ·ç«¯å¯ä»¥æ·»åŠ å‰©ä½™çš„ 6 ç§ä¼˜åŒ–ï¼Œä»¥æ¥è¿‘å…¨æ¨¡å‹çš„æ€§èƒ½ã€‚
