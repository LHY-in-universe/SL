# GPT-2 优化方法详解

## 📚 目录

1. [Hugging Face 官方优化](#1-hugging-face-官方优化)
2. [PyTorch 标准库优化](#2-pytorch-标准库优化)
3. [生成过程优化](#3-生成过程优化)
4. [采样算法优化](#4-采样算法优化)

---

## 1. Hugging Face 官方优化

### 1.1 `low_cpu_mem_usage=True`

#### 📖 原理
- **问题**: 默认加载模型时，PyTorch 会先创建完整模型结构，然后加载权重，导致内存峰值翻倍
- **解决**: 使用 `safetensors` 格式或优化加载顺序，边加载边分配内存

#### 🔧 实现方式
```python
model = GPT2LMHeadModel.from_pretrained(
    model_id,
    low_cpu_mem_usage=True,  # 启用低内存加载
)
```

#### 📊 效果
- **内存峰值**: 降低 20-30%
- **加载时间**: 基本不变或略快
- **适用场景**: 所有设备（CPU/MPS/CUDA）

#### 💡 技术细节
- 使用 `torch.load()` 的 `weights_only` 模式
- 避免创建临时完整模型副本
- 特别适合大模型（>1B 参数）

---

### 1.2 `use_fast=True` (快速分词器)

#### 📖 原理
- **问题**: Python 实现的 `tokenizer` 速度较慢
- **解决**: 使用 Rust 实现的 `tokenizers` 库（Hugging Face 官方）

#### 🔧 实现方式
```python
tokenizer = AutoTokenizer.from_pretrained(
    model_id,
    use_fast=True,  # 使用快速分词器
)
```

#### 📊 效果
- **分词速度**: 提升 2-5 倍
- **内存使用**: 基本不变
- **兼容性**: 100%（API 完全相同）

#### 💡 技术细节
- 底层使用 Rust 的 `tokenizers` crate
- 支持所有主流模型（BERT, GPT, T5 等）
- 如果快速分词器不可用，自动回退到 Python 版本

---

## 2. PyTorch 标准库优化

### 2.1 `torch.inference_mode()`

#### 📖 原理
- **问题**: `torch.no_grad()` 只是禁用梯度计算，但 autograd 系统仍在运行
- **解决**: `inference_mode()` 完全禁用 autograd，减少开销

#### 🔧 实现方式
```python
with torch.inference_mode():  # 比 torch.no_grad() 更快
    outputs = model(input_ids, ...)
```

#### 📊 效果
- **速度提升**: 5-10%
- **内存节省**: 少量（减少 autograd 图构建）
- **适用场景**: 所有推理任务

#### 💡 技术细节
- **`torch.no_grad()`**: 禁用梯度计算，但保留 autograd 图
- **`torch.inference_mode()`**: 完全禁用 autograd，更彻底
- **注意**: 在 `inference_mode()` 中不能调用 `.backward()`

#### 🔄 对比
```python
# 旧方式（较慢）
with torch.no_grad():
    outputs = model(inputs)

# 新方式（更快）
with torch.inference_mode():
    outputs = model(inputs)
```

---

### 2.2 `torch.set_float32_matmul_precision("medium")`

#### 📖 原理
- **问题**: 默认的 float32 矩阵乘法使用最高精度，但推理不需要
- **解决**: 降低精度要求，允许使用 Tensor Core（CUDA）或优化算法

#### 🔧 实现方式
```python
if hasattr(torch, 'set_float32_matmul_precision'):
    torch.set_float32_matmul_precision("medium")  # high/medium/low
```

#### 📊 效果
- **速度提升**: 10-20%（CUDA 上更明显）
- **精度影响**: 几乎无影响（medium 模式）
- **适用场景**: PyTorch 2.0+，所有设备

#### 💡 技术细节
- **`high`**: 最高精度，最慢（默认）
- **`medium`**: 平衡精度和速度（推荐）
- **`low`**: 最快，但可能影响精度
- **CUDA**: 启用 Tensor Core 加速
- **MPS**: 使用优化的矩阵乘法算法

#### ⚠️ 注意事项
- 仅影响 float32 矩阵乘法
- float16/bfloat16 不受影响
- 训练时建议使用 `high`

---

### 2.3 `torch.backends.cudnn.benchmark = True`

#### 📖 原理
- **问题**: CUDNN 需要为每个输入形状选择最优算法，每次都测试很慢
- **解决**: 第一次测试后缓存结果，后续直接使用

#### 🔧 实现方式
```python
if device == "cuda":
    torch.backends.cudnn.benchmark = True  # 仅 CUDA
```

#### 📊 效果
- **速度提升**: 10-30%（重复形状的输入）
- **首次运行**: 可能稍慢（需要测试）
- **适用场景**: 仅 CUDA 设备

#### 💡 技术细节
- **工作原理**: 
  1. 第一次遇到新形状时，测试所有算法
  2. 选择最快的算法并缓存
  3. 后续相同形状直接使用缓存结果
- **适用条件**: 
  - 输入形状固定或变化较少
  - 卷积、RNN、注意力等操作
- **不适用**: 
  - 输入形状每次都不同（会频繁测试）
  - 需要确定性结果（`deterministic=True`）

#### ⚠️ 注意事项
- 仅 CUDA 设备有效
- MPS/CPU 设备自动忽略
- 可能增加显存使用（缓存算法信息）

---

## 3. 生成过程优化

### 3.1 KV Cache (Key-Value Cache)

#### 📖 原理
- **问题**: 生成每个新 token 时，需要重新计算所有历史 token 的注意力
- **解决**: 缓存历史 token 的 Key 和 Value，只计算新 token

#### 🔧 实现方式
```python
past_key_values = None  # 初始为空

for step in range(max_new_tokens):
    if step == 0:
        # 首次：处理完整 prompt，生成 KV cache
        outputs = model(input_ids, use_cache=True)
        past_key_values = outputs.past_key_values
    else:
        # 后续：只处理新 token，复用 KV cache
        outputs = model(
            new_token_ids,
            past_key_values=past_key_values,  # 复用缓存
            use_cache=True
        )
        past_key_values = outputs.past_key_values  # 更新缓存
```

#### 📊 效果
- **首次 token**: 正常速度（需要计算完整 prompt）
- **后续 token**: 速度提升 5-10 倍
- **内存增加**: 每个 token 增加 ~2MB（取决于模型大小）

#### 💡 技术细节
- **KV Cache 结构**: 
  - 每个 transformer 层都有独立的 K 和 V
  - GPT-2 (12层) 需要缓存 12 对 K/V
- **内存计算**: 
  - 每个 token: `2 * num_layers * hidden_size * dtype_size`
  - GPT-2: `2 * 12 * 768 * 4 bytes ≈ 73 KB/token`
- **优化效果**: 
  - 无 KV Cache: O(n²) 复杂度（n 为序列长度）
  - 有 KV Cache: O(n) 复杂度

#### 📈 性能对比
```
无 KV Cache:
  Token 1: 100ms
  Token 2: 200ms  (重新计算 Token 1)
  Token 3: 300ms  (重新计算 Token 1-2)
  ...

有 KV Cache:
  Token 1: 100ms  (计算完整 prompt)
  Token 2: 20ms   (只计算新 token)
  Token 3: 20ms   (只计算新 token)
  ...
```

---

### 3.2 `output_attentions=False`

#### 📖 原理
- **问题**: 默认返回所有层的注意力权重，占用大量内存和计算
- **解决**: 推理时不需要注意力权重，直接禁用

#### 🔧 实现方式
```python
outputs = model(
    input_ids,
    output_attentions=False,  # 不输出注意力权重
)
```

#### 📊 效果
- **内存节省**: 10-15%
- **速度提升**: 5-10%
- **适用场景**: 所有推理任务（除非需要可视化注意力）

#### 💡 技术细节
- **注意力权重大小**: 
  - 形状: `[batch, num_heads, seq_len, seq_len]`
  - GPT-2: `[1, 12, 1024, 1024] = 50MB` (每个层)
  - 12 层总计: ~600MB
- **计算开销**: 
  - 需要额外的 softmax 和归一化
  - 存储和传输开销

---

### 3.3 `output_hidden_states=False`

#### 📖 原理
- **问题**: 默认返回所有层的隐藏状态，占用大量内存
- **解决**: 推理时只需要最后一层输出，禁用中间层输出

#### 🔧 实现方式
```python
outputs = model(
    input_ids,
    output_hidden_states=False,  # 不输出隐藏状态
)
```

#### 📊 效果
- **内存节省**: 5-10%
- **速度提升**: 2-5%
- **适用场景**: 所有推理任务（除非需要中间层特征）

#### 💡 技术细节
- **隐藏状态大小**: 
  - 形状: `[batch, seq_len, hidden_size]`
  - GPT-2: `[1, 1024, 768] = 3MB` (每个层)
  - 12 层总计: ~36MB
- **计算开销**: 
  - 需要额外的内存分配和复制
  - 序列越长，开销越大

---

## 4. 采样算法优化

### 4.1 优化的 Top-k 采样

#### 📖 原理
- **问题**: 传统 Top-k 需要排序整个词汇表（50k+ tokens），很慢
- **解决**: 使用 `torch.topk()` 只获取 top-k，然后用 `scatter_()` 高效设置

#### 🔧 实现方式

**旧方式（慢）**:
```python
# 需要遍历整个词汇表
indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]
logits[indices_to_remove] = float('-inf')
```

**新方式（快）**:
```python
# 只处理 top-k 个 token
top_k_value = min(top_k, logits.size(-1))
top_k_logits, top_k_indices = torch.topk(logits, top_k_value)

# 高效设置：只修改 top-k 个位置
logits_filtered = torch.full_like(logits, float('-inf'))
logits_filtered.scatter_(0, top_k_indices, top_k_logits)
logits = logits_filtered
```

#### 📊 效果
- **速度提升**: 20-30%
- **内存使用**: 基本不变
- **适用场景**: Top-k 采样（k < 词汇表大小）

#### 💡 技术细节
- **`torch.topk()`**: 
  - 时间复杂度: O(n log k)，n 为词汇表大小
  - 只返回 top-k，不需要完整排序
- **`scatter_()`**: 
  - 原地操作，高效设置指定位置的值
  - 比逐个赋值快得多
- **优化效果**: 
  - 词汇表大小: 50,257 (GPT-2)
  - Top-k: 50
  - 处理元素: 从 50,257 降到 50

#### 📈 性能对比
```
传统方式:
  1. 排序整个词汇表: O(n log n) ≈ 800,000 次比较
  2. 设置 mask: O(n) ≈ 50,000 次赋值
  总计: ~850,000 次操作

优化方式:
  1. topk: O(n log k) ≈ 50,000 次比较
  2. scatter: O(k) ≈ 50 次赋值
  总计: ~50,050 次操作

速度提升: 17 倍
```

---

## 📊 综合效果总结

### 性能提升汇总

| 优化方法 | 速度提升 | 内存节省 | 适用设备 |
|---------|---------|---------|---------|
| `low_cpu_mem_usage` | - | 20-30% | 所有 |
| `use_fast=True` | 200-500% | - | 所有 |
| `torch.inference_mode()` | 5-10% | 少量 | 所有 |
| `matmul_precision` | 10-20% | - | 所有 |
| `cudnn.benchmark` | 10-30% | - | 仅 CUDA |
| **KV Cache** | **500-1000%** | -2MB/token | 所有 |
| `output_attentions=False` | 5-10% | 10-15% | 所有 |
| `output_hidden_states=False` | 2-5% | 5-10% | 所有 |
| 优化的 Top-k | 20-30% | - | 所有 |

### 整体效果

- **首次 token 生成**: 提升 15-25%
- **后续 token 生成**: 提升 500-1000%（主要来自 KV Cache）
- **内存使用**: 降低 25-40%
- **整体速度**: 提升 50-200%（取决于序列长度）

---

## 🎯 最佳实践建议

### 1. 必须使用的优化
- ✅ `torch.inference_mode()` - 所有推理任务
- ✅ KV Cache - 所有生成任务
- ✅ `output_attentions=False` - 除非需要可视化
- ✅ `output_hidden_states=False` - 除非需要中间层特征

### 2. 推荐使用的优化
- ✅ `low_cpu_mem_usage=True` - 大模型必用
- ✅ `use_fast=True` - 所有模型
- ✅ `torch.set_float32_matmul_precision("medium")` - PyTorch 2.0+

### 3. 条件使用的优化
- ⚠️ `cudnn.benchmark=True` - 仅 CUDA，且输入形状固定
- ⚠️ `torch.compile()` - 仅 CUDA，需要多次调用
- ⚠️ `torch_dtype=float16` - 需要测试精度影响

### 4. 避免的优化
- ❌ `cudnn.benchmark` + 输入形状频繁变化 = 反而变慢
- ❌ `torch.compile()` + MPS = 不支持
- ❌ `output_attentions=True` + 长序列 = 内存爆炸

---

## 📚 参考资料

- [PyTorch Inference Mode](https://pytorch.org/docs/stable/generated/torch.inference_mode.html)
- [Hugging Face Model Loading](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
- [KV Cache Explained](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/)
- [CUDNN Benchmark](https://pytorch.org/docs/stable/backends.html#torch.backends.cudnn.benchmark)
