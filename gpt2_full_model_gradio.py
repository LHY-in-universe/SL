"""
GPT-2 å®Œæ•´æ¨¡å‹éƒ¨ç½²ï¼ˆå¯¹ç…§ç»„ï¼‰

ç”¨é€”ï¼š
  - ä½œä¸ºåˆ†æ‹†æ¨¡å‹çš„æ€§èƒ½å¯¹ç…§
  - æµ‹è¯•åˆ†æ‹†æ˜¯å¦å¼•å…¥é¢å¤–å¼€é”€
  - éªŒè¯ç”Ÿæˆè´¨é‡ä¸€è‡´æ€§

æ¶æ„ï¼š
  - å•æœºè¿è¡Œå®Œæ•´çš„ GPT-2 æ¨¡å‹
  - ä½¿ç”¨ Hugging Face å®˜æ–¹ä¼˜åŒ–
  - KV Cache æµå¼ç”Ÿæˆ
  - è¯¦ç»†çš„æ€§èƒ½ç»Ÿè®¡

ä¼˜åŒ–ï¼š
  - Hugging Face: low_cpu_mem_usage, torch_dtype, device_map, flash_attention
  - PyTorch: torch.inference_mode(), torch.compile(), çŸ©é˜µä¹˜æ³•ç²¾åº¦ä¼˜åŒ–
  - ç”Ÿæˆ: KV Cache, ä¼˜åŒ–çš„é‡‡æ ·è¿‡ç¨‹, æµå¼è¾“å‡º

ç”¨æ³•ï¼š
    # é»˜è®¤å¯åŠ¨
    PYTHONPATH=./SplitLearnCore/src:./SplitLearnComm/src python gpt2_full_model_gradio.py
    
    # ä½¿ç”¨ä¼˜åŒ–é€‰é¡¹
    GPT2_TORCH_DTYPE=float16 GPT2_FLASH_ATTN=1 python gpt2_full_model_gradio.py
    
ç¯å¢ƒå˜é‡ï¼š
    GPT2_FULL_PORT: Gradio ç«¯å£ï¼ˆé»˜è®¤ 7861ï¼‰
    GRADIO_SHARE: æ˜¯å¦å¯ç”¨å…¬ç½‘åˆ†äº«ï¼ˆé»˜è®¤ 1ï¼‰
    GPT2_TORCH_DTYPE: æ•°æ®ç±»å‹ float32/float16/bfloat16ï¼ˆé»˜è®¤ float32ï¼‰
    GPT2_LOW_MEM: ä½å†…å­˜åŠ è½½ï¼ˆé»˜è®¤ 1ï¼‰
    GPT2_USE_DEVICE_MAP: è‡ªåŠ¨è®¾å¤‡æ˜ å°„ï¼ˆé»˜è®¤ 0ï¼Œä»… CUDAï¼‰
    GPT2_FLASH_ATTN: Flash Attention 2ï¼ˆé»˜è®¤ 0ï¼‰
    GPT2_ENABLE_COMPILE: torch.compileï¼ˆé»˜è®¤ 1ï¼Œä»… CUDAï¼‰
    GPT2_MATMUL_PRECISION: çŸ©é˜µä¹˜æ³•ç²¾åº¦ high/medium/lowï¼ˆé»˜è®¤ mediumï¼‰
    GPT2_LOG_LEVEL: æ—¥å¿—çº§åˆ« DEBUG/INFO/WARNING/ERRORï¼ˆé»˜è®¤ INFOï¼‰
"""

import os
import sys
import time
import logging
from pathlib import Path

os.environ.setdefault("GRADIO_ANALYTICS_ENABLED", "false")

import torch
import gradio as gr
import plotly.graph_objects as go
import pandas as pd

# æ·»åŠ è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "SplitLearnCore" / "src"))
sys.path.insert(0, str(Path(__file__).parent / "SplitLearnComm" / "src"))

from transformers import GPT2LMHeadModel, AutoTokenizer

# é…ç½®æ—¥å¿—
log_dir = Path("./logs")
log_dir.mkdir(exist_ok=True)

LOG_LEVEL_NAME = os.environ.get("GPT2_LOG_LEVEL", "INFO").upper()
LOG_LEVEL = getattr(logging, LOG_LEVEL_NAME, logging.INFO)

logging.basicConfig(
    level=LOG_LEVEL,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_dir / "gpt2_full.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ============================================================================
# å…¨å±€å˜é‡ï¼ˆæ¨¡å‹åŠ è½½ï¼‰
# ============================================================================

model_id = "gpt2"
device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
FULL_PORT = int(os.environ.get("GPT2_FULL_PORT", "7861"))
SHARE_ENABLED = os.environ.get("GRADIO_SHARE", "1") == "1"
MODEL_CACHE = str(Path("./models").resolve())

# ä¼˜åŒ–é€‰é¡¹ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶ï¼‰
ENABLE_TORCH_COMPILE = os.environ.get("GPT2_ENABLE_COMPILE", "1") == "1"  # æ˜¯å¦å¯ç”¨ torch.compile
MATMUL_PRECISION = os.environ.get("GPT2_MATMUL_PRECISION", "medium")  # çŸ©é˜µä¹˜æ³•ç²¾åº¦: high/medium/low

logger.info("=" * 70)
logger.info("GPT-2 å®Œæ•´æ¨¡å‹ï¼ˆå¯¹ç…§ç»„ï¼‰")
logger.info("=" * 70)
logger.info(f"æ—¥å¿—çº§åˆ«: {LOG_LEVEL_NAME}")
logger.info(f"è®¾å¤‡: {device}")
logger.info(f"æ¨¡å‹: {model_id}")
logger.info(f"Gradio ç«¯å£: {FULL_PORT}")
logger.info(f"å…¬ç½‘åˆ†äº«: {SHARE_ENABLED}")
logger.info(f"æ¨¡å‹ç¼“å­˜ç›®å½•: {MODEL_CACHE}")

logger.info("åŠ è½½å®Œæ•´ GPT-2 æ¨¡å‹...")

# ============================================================================
# Hugging Face å®˜æ–¹ä¼˜åŒ–é…ç½®
# ============================================================================

# 1. ä½å†…å­˜åŠ è½½ï¼ˆå‡å°‘å³°å€¼å†…å­˜ä½¿ç”¨ï¼‰
LOW_CPU_MEM_USAGE = os.environ.get("GPT2_LOW_MEM", "1") == "1"

# 2. æ•°æ®ç±»å‹ä¼˜åŒ–ï¼ˆå¯é€‰ï¼šfloat16, bfloat16ï¼‰
TORCH_DTYPE = os.environ.get("GPT2_TORCH_DTYPE", "float32")
if TORCH_DTYPE == "float16":
    torch_dtype = torch.float16
elif TORCH_DTYPE == "bfloat16":
    torch_dtype = torch.bfloat16
else:
    torch_dtype = None  # ä½¿ç”¨é»˜è®¤ float32

# 3. è®¾å¤‡æ˜ å°„ï¼ˆè‡ªåŠ¨é€‰æ‹©è®¾å¤‡ï¼‰
USE_DEVICE_MAP = os.environ.get("GPT2_USE_DEVICE_MAP", "0") == "1" and device == "cuda"

# 4. ä¼˜åŒ–çš„æ³¨æ„åŠ›å®ç°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
USE_FLASH_ATTENTION = os.environ.get("GPT2_FLASH_ATTN", "0") == "1"

logger.info("Hugging Face ä¼˜åŒ–é…ç½®:")
logger.info(f"  - low_cpu_mem_usage: {LOW_CPU_MEM_USAGE}")
logger.info(f"  - torch_dtype: {TORCH_DTYPE}")
logger.info(f"  - device_map: {USE_DEVICE_MAP}")
logger.info(f"  - flash_attention: {USE_FLASH_ATTENTION}")

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼ˆä½¿ç”¨ Hugging Face å®˜æ–¹ä¼˜åŒ–ï¼‰
model_kwargs = {
    "cache_dir": MODEL_CACHE,
    "low_cpu_mem_usage": LOW_CPU_MEM_USAGE,
}

# æ·»åŠ æ•°æ®ç±»å‹ï¼ˆå¦‚æœæŒ‡å®šï¼‰
if torch_dtype is not None and device != "cpu":  # CPU é€šå¸¸ä¸æ”¯æŒ float16/bfloat16
    model_kwargs["torch_dtype"] = torch_dtype
    logger.info(f"  ä½¿ç”¨ {TORCH_DTYPE} ç²¾åº¦ï¼ˆå¯èƒ½æå‡æ€§èƒ½ï¼‰")

# æ·»åŠ è®¾å¤‡æ˜ å°„ï¼ˆä»… CUDAï¼‰
if USE_DEVICE_MAP:
    model_kwargs["device_map"] = "auto"
    logger.info("  ä½¿ç”¨è‡ªåŠ¨è®¾å¤‡æ˜ å°„")

# æ·»åŠ ä¼˜åŒ–çš„æ³¨æ„åŠ›å®ç°
if USE_FLASH_ATTENTION:
    try:
        # æ£€æŸ¥æ˜¯å¦å®‰è£…äº† flash-attn
        import flash_attn
        model_kwargs["attn_implementation"] = "flash_attention_2"
        logger.info("  ä½¿ç”¨ Flash Attention 2ï¼ˆå·²å®‰è£…ï¼‰")
    except ImportError:
        logger.warning("  Flash Attention 2 æœªå®‰è£…ï¼Œä½¿ç”¨é»˜è®¤å®ç°")
        logger.info("  æç¤º: å®‰è£… flash-attn å¯æå‡æ€§èƒ½: pip install flash-attn")
    except Exception as e:
        logger.warning(f"  Flash Attention 2 é…ç½®å¤±è´¥: {e}")

# ä½¿ç”¨ Hugging Face å®˜æ–¹æ–¹æ³•åŠ è½½æ¨¡å‹ï¼ˆè‡ªåŠ¨åº”ç”¨æ‰€æœ‰ä¼˜åŒ–ï¼‰
try:
    model = GPT2LMHeadModel.from_pretrained(model_id, **model_kwargs)
    logger.info("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼ˆä½¿ç”¨ Hugging Face ä¼˜åŒ–ï¼‰")
except Exception as e:
    logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}", exc_info=True)
    raise

# åŠ è½½åˆ†è¯å™¨ï¼ˆä½¿ç”¨ä¼˜åŒ–é…ç½®ï¼‰
tokenizer = AutoTokenizer.from_pretrained(
    model_id,
    cache_dir=MODEL_CACHE,
    use_fast=True,  # ä½¿ç”¨å¿«é€Ÿåˆ†è¯å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# å¦‚æœæœªä½¿ç”¨ device_mapï¼Œæ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡
if not USE_DEVICE_MAP:
    model.to(device)

model.eval()

# ============================================================================
# PyTorch æ ‡å‡†åº“åŠ é€Ÿä¼˜åŒ–
# ============================================================================

# 1. çŸ©é˜µä¹˜æ³•ç²¾åº¦ä¼˜åŒ–ï¼ˆPyTorch 2.0+ï¼‰
if hasattr(torch, 'set_float32_matmul_precision'):
    try:
        torch.set_float32_matmul_precision(MATMUL_PRECISION)
        logger.info(f"âœ“ çŸ©é˜µä¹˜æ³•ç²¾åº¦ä¼˜åŒ–: {MATMUL_PRECISION}")
    except Exception as e:
        logger.warning(f"çŸ©é˜µä¹˜æ³•ç²¾åº¦ä¼˜åŒ–å¤±è´¥: {e}")

# 2. CUDA ä¼˜åŒ–ï¼ˆå¦‚æœä½¿ç”¨ CUDAï¼‰
if device == "cuda":
    # CUDNN åŸºå‡†æµ‹è¯•ï¼ˆè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç®—æ³•ï¼‰
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False  # å…è®¸éç¡®å®šæ€§ç®—æ³•ä»¥è·å¾—æ›´å¥½æ€§èƒ½
    logger.info("âœ“ CUDA ä¼˜åŒ–å·²å¯ç”¨ (cudnn.benchmark)")

# 3. MPS ä¼˜åŒ–ï¼ˆå¦‚æœä½¿ç”¨ MPSï¼‰
if device == "mps":
    # MPS ç‰¹å®šä¼˜åŒ–
    if hasattr(torch.backends, 'mps'):
        logger.info("âœ“ MPS è®¾å¤‡ä¼˜åŒ–å·²å¯ç”¨")
    # æ³¨æ„ï¼šMPS ä¸æ”¯æŒ torch.compileï¼Œä½†å¯ä»¥ä½¿ç”¨å…¶ä»–ä¼˜åŒ–

# 4. torch.compile() ä¼˜åŒ–ï¼ˆä»… CUDAï¼ŒPyTorch 2.0+ï¼‰
if ENABLE_TORCH_COMPILE and hasattr(torch, 'compile') and device == "cuda":
    logger.info("åº”ç”¨ torch.compile() ä¼˜åŒ–...")
    try:
        # reduce-overhead: å‡å°‘ Python å¼€é”€ï¼Œé€‚åˆé‡å¤è°ƒç”¨
        # å…¶ä»–æ¨¡å¼: "default" (å¹³è¡¡), "max-autotune" (æœ€ä¼˜åŒ–ä½†ç¼–è¯‘æ…¢)
        model = torch.compile(model, mode="reduce-overhead")
        logger.info("âœ“ torch.compile() ä¼˜åŒ–å·²åº”ç”¨ (mode=reduce-overhead)")
    except Exception as e:
        logger.warning(f"torch.compile() ä¼˜åŒ–å¤±è´¥: {e}")
elif device == "mps":
    logger.info("â„¹ MPS è®¾å¤‡ä¸æ”¯æŒ torch.compile()ï¼Œä½¿ç”¨å…¶ä»–ä¼˜åŒ–")

logger.info(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
logger.info(f"âœ“ å‚æ•°é‡: {sum(p.numel() for p in model.parameters())/1e6:.2f}M")
if torch_dtype is not None:
    actual_dtype = next(model.parameters()).dtype
    logger.info(f"âœ“ å®é™…ç²¾åº¦: {actual_dtype}")
logger.info("")
logger.info("=" * 70)
logger.info("å·²å¯ç”¨çš„ä¼˜åŒ–:")
logger.info("")
logger.info("Hugging Face å®˜æ–¹ä¼˜åŒ–:")
logger.info(f"  - low_cpu_mem_usage: {LOW_CPU_MEM_USAGE}")
logger.info(f"  - torch_dtype: {TORCH_DTYPE}")
logger.info(f"  - device_map: {USE_DEVICE_MAP}")
logger.info(f"  - flash_attention: {USE_FLASH_ATTENTION}")
logger.info("")
logger.info("PyTorch æ ‡å‡†åº“ä¼˜åŒ–:")
logger.info(f"  - æ¨ç†æ¨¡å¼: torch.inference_mode() (æ¯” torch.no_grad() æ›´å¿«)")
logger.info(f"  - çŸ©é˜µä¹˜æ³•ç²¾åº¦: {MATMUL_PRECISION}")
if device == "cuda":
    logger.info(f"  - CUDA ä¼˜åŒ–: cudnn.benchmark=True")
    if ENABLE_TORCH_COMPILE and hasattr(torch, 'compile'):
        logger.info(f"  - torch.compile(): å·²å¯ç”¨ (mode=reduce-overhead)")
    else:
        logger.info(f"  - torch.compile(): æœªå¯ç”¨")
elif device == "mps":
    logger.info(f"  - MPS è®¾å¤‡: ä½¿ç”¨ MPS åç«¯ä¼˜åŒ–")
    logger.info(f"  - torch.compile(): MPS ä¸æ”¯æŒ")
else:
    logger.info(f"  - CPU è®¾å¤‡: ä½¿ç”¨ CPU åç«¯")
logger.info("")
logger.info("ç”Ÿæˆä¼˜åŒ–:")
logger.info(f"  - KV Cache: å·²å¯ç”¨ï¼ˆé¿å…é‡å¤è®¡ç®—å†å² tokenï¼‰")
logger.info(f"  - output_attentions: Falseï¼ˆèŠ‚çœå†…å­˜ï¼‰")
logger.info(f"  - output_hidden_states: Falseï¼ˆèŠ‚çœå†…å­˜ï¼‰")
logger.info("=" * 70)
logger.info("")

# å…¨å±€ç»Ÿè®¡å˜é‡
all_token_stats = []


# ============================================================================
# æ ¸å¿ƒç”Ÿæˆå‡½æ•°
# ============================================================================

def generate_with_kv_cache(
    prompt: str,
    max_new_tokens: int = 50,
    temperature: float = 1.0,
    top_k: int = 50,
):
    """
    ä½¿ç”¨ KV Cache çš„å®Œæ•´æ¨¡å‹ç”Ÿæˆï¼ˆç”Ÿæˆå™¨å‡½æ•°ï¼Œæµå¼è¾“å‡ºï¼‰
    
    ä¼˜åŒ–è¯´æ˜ï¼š
    - ä½¿ç”¨æ‰‹åŠ¨å¾ªç¯ + KV Cache å®ç°æµå¼ç”Ÿæˆ
    - æ¯ä¸ª token å®æ—¶è¾“å‡ºï¼Œæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ
    - ä½¿ç”¨ Hugging Face å®˜æ–¹ä¼˜åŒ–å‚æ•°
    
    æ€§èƒ½è¯´æ˜ï¼š
    - åœ¨ MPS è®¾å¤‡ä¸Šï¼Œå®Œæ•´æ¨¡å‹å¯èƒ½æ¯”åˆ†æ‹†æ¨¡å‹æ…¢ï¼Œå› ä¸ºï¼š
      1. æ‰€æœ‰è®¡ç®—éƒ½åœ¨æœ¬åœ°ï¼Œæ²¡æœ‰å¹¶è¡ŒåŒ–
      2. MPS å¯¹é•¿åºåˆ—å¤„ç†æ€§èƒ½ä¸ç¨³å®š
      3. KV cache éšåºåˆ—å¢é•¿ï¼Œå†…å­˜å‹åŠ›å¢åŠ 
    - åœ¨ CUDA è®¾å¤‡ä¸Šï¼Œtorch.compile() ä¼šæ˜¾è‘—æå‡æ€§èƒ½
    """
    global all_token_stats

    # ========================================================================
    # å…³é”®è°ƒè¯•ï¼šç«‹å³è¾“å‡ºæ—¥å¿—ï¼Œç¡®ä¿å‡½æ•°è¢«è°ƒç”¨æ—¶èƒ½ç«‹å³çœ‹åˆ°
    # ========================================================================
    logger.info("=" * 70)
    logger.info("[GENERATE] ========== å‡½æ•°è¢«è°ƒç”¨ï¼==========")
    logger.info(f"[GENERATE] æ”¶åˆ°ç”Ÿæˆè¯·æ±‚")
    logger.info(f"[GENERATE]   - prompt: '{prompt}' (é•¿åº¦: {len(prompt) if prompt else 0})")
    logger.info(f"[GENERATE]   - max_new_tokens: {max_new_tokens} (ç±»å‹: {type(max_new_tokens)})")
    logger.info(f"[GENERATE]   - temperature: {temperature} (ç±»å‹: {type(temperature)})")
    logger.info(f"[GENERATE]   - top_k: {top_k} (ç±»å‹: {type(top_k)})")
    logger.info("=" * 70)
    
    # ç¡®ä¿å‚æ•°ç±»å‹æ­£ç¡®ï¼ˆGradio å¯èƒ½ä¼ é€’å­—ç¬¦ä¸²ï¼‰
    try:
        max_new_tokens = int(max_new_tokens)
        temperature = float(temperature)
        top_k = int(top_k)
        prompt = str(prompt) if prompt else ""
    except (ValueError, TypeError) as e:
        logger.error(f"å‚æ•°ç±»å‹è½¬æ¢å¤±è´¥: {e}, prompt={type(prompt)}, max_new_tokens={type(max_new_tokens)}, temperature={type(temperature)}, top_k={type(top_k)}")
        yield f"é”™è¯¯ï¼šå‚æ•°ç±»å‹æ— æ•ˆ\n{str(e)}", "ç”Ÿæˆå¤±è´¥"
        return

    # ç¼–ç è¾“å…¥ï¼ˆä½¿ç”¨ tokenizer çš„ä¼˜åŒ–æ–¹æ³•ï¼‰
    try:
        # ä½¿ç”¨ tokenizer çš„ä¼˜åŒ–ç¼–ç æ–¹æ³•
        encoded = tokenizer(
            prompt,
            return_tensors="pt",
            padding=False,  # ä¸éœ€è¦ padding
            truncation=False,  # ä¸æˆªæ–­
            add_special_tokens=True,  # æ·»åŠ ç‰¹æ®Š token
        )
        input_ids = encoded["input_ids"].to(device)
        attention_mask = encoded.get("attention_mask", None)
        if attention_mask is not None:
            attention_mask = attention_mask.to(device)
    except Exception as e:
        logger.error(f"[generate] ç¼–ç è¾“å…¥å¤±è´¥: {e}", exc_info=True)
        yield f"é”™è¯¯ï¼šç¼–ç è¾“å…¥å¤±è´¥\n{str(e)}", "ç”Ÿæˆå¤±è´¥"
        return

    # ç»Ÿè®¡
    token_times = []
    total_start = time.time()

    generated_tokens = []
    past_key_values = None

    # ä½¿ç”¨ torch.inference_mode() æ›¿ä»£ torch.no_grad()
    # inference_mode æ¯” no_grad æ›´å¿«ï¼Œå› ä¸ºå®ƒå®Œå…¨ç¦ç”¨æ¢¯åº¦è®¡ç®—å’Œ autograd
    with torch.inference_mode():
        for step in range(max_new_tokens):
            step_start = time.time()

            # è¾“å…¥ï¼šé¦–æ¬¡å¤„ç†å®Œæ•´ promptï¼Œåç»­åªå¤„ç†æ–° tokenï¼ˆKV Cache ä¼˜åŒ–ï¼‰
            if step == 0:
                current_input_ids = input_ids
                current_attention_mask = attention_mask
            else:
                # ä¼˜åŒ–ï¼šå¤ç”¨ tensorï¼Œé¿å…é‡å¤åˆ›å»ºï¼Œä½¿ç”¨ long ç±»å‹
                current_input_ids = torch.tensor([[next_token_id]], device=device, dtype=torch.long)
                current_attention_mask = None  # å• token ä¸éœ€è¦ attention_mask

            # å‰å‘ä¼ æ’­ï¼ˆå¸¦ KV Cacheï¼‰- ä½¿ç”¨ Hugging Face å®˜æ–¹ä¼˜åŒ–
            # KV Cache å·¥ä½œåŸç†ï¼š
            # - step=0: å¤„ç†å®Œæ•´ promptï¼Œç”Ÿæˆ KV cache
            # - step>0: åªå¤„ç†æ–° tokenï¼Œå¤ç”¨ä¹‹å‰çš„ KV cacheï¼Œé¿å…é‡å¤è®¡ç®—
            # 
            # Hugging Face ä¼˜åŒ–å‚æ•°ï¼š
            # - use_cache=True: å¯ç”¨ KV cacheï¼ˆé»˜è®¤å·²å¯ç”¨ï¼Œæ˜¾å¼æŒ‡å®šç¡®ä¿ä¼˜åŒ–ï¼‰
            # - output_attentions=False: ä¸è¾“å‡ºæ³¨æ„åŠ›æƒé‡ï¼ˆèŠ‚çœå†…å­˜å’Œè®¡ç®—ï¼‰
            # - output_hidden_states=False: ä¸è¾“å‡ºéšè—çŠ¶æ€ï¼ˆèŠ‚çœå†…å­˜ï¼‰
            # - attention_mask: ä»…åœ¨é¦–æ¬¡éœ€è¦ï¼Œåç»­å• token ä¸éœ€è¦
            model_kwargs = {
                "past_key_values": past_key_values,
                "use_cache": True,  # å¯ç”¨ KV cache ä¼˜åŒ–
                "output_attentions": False,  # ä¸è¾“å‡ºæ³¨æ„åŠ›æƒé‡ï¼ŒèŠ‚çœå†…å­˜
                "output_hidden_states": False,  # ä¸è¾“å‡ºéšè—çŠ¶æ€ï¼ŒèŠ‚çœå†…å­˜
            }
            
            if current_attention_mask is not None:
                model_kwargs["attention_mask"] = current_attention_mask
            
            outputs = model(current_input_ids, **model_kwargs)

            logits = outputs.logits[0, -1, :]
            past_key_values = outputs.past_key_values

            # ä¼˜åŒ–é‡‡æ ·è¿‡ç¨‹
            if temperature > 0:
                # æ¸©åº¦é‡‡æ ·
                logits = logits / temperature
                if top_k > 0:
                    # Top-k é‡‡æ ·ï¼šåªä¿ç•™ top-k ä¸ªæœ€é«˜æ¦‚ç‡çš„ token
                    # ä¼˜åŒ–ï¼šä½¿ç”¨ torch.topk ä¸€æ¬¡æ€§è·å–ï¼Œé¿å…å¤šæ¬¡è®¡ç®—
                    top_k_logits, top_k_indices = torch.topk(logits, min(top_k, logits.size(-1)))
                    # åˆ›å»º maskï¼Œåªä¿ç•™ top-k
                    logits_filtered = torch.full_like(logits, float('-inf'))
                    logits_filtered.scatter_(0, top_k_indices, top_k_logits)
                    logits = logits_filtered
                probs = torch.softmax(logits, dim=-1)
                next_token_id = torch.multinomial(probs, num_samples=1).item()
            else:
                # è´ªå©ªé‡‡æ ·ï¼šç›´æ¥å–æœ€å¤§å€¼ï¼ˆæœ€å¿«ï¼‰
                next_token_id = logits.argmax(dim=-1).item()

            generated_tokens.append(next_token_id)

            # è®°å½•æ—¶é—´
            token_time = time.time() - step_start
            token_times.append(token_time * 1000)

            # è®°å½•è¯¦ç»†ç»Ÿè®¡
            all_token_stats.append({
                "step": step,
                "token_id": next_token_id,
                "time_ms": token_time * 1000,
            })

            # è®°å½•æ¯ä¸ª token çš„è¯¦ç»†ä¿¡æ¯åˆ°æ—¥å¿—
            token_text = tokenizer.decode([next_token_id], skip_special_tokens=True)
            logger.info(f"Token #{len(generated_tokens)}: '{token_text}' (ID={next_token_id}) | "
                       f"Time={token_time*1000:.2f}ms")

            if next_token_id == tokenizer.eos_token_id:
                break

            # å®æ—¶è¾“å‡ºï¼ˆæµå¼ç”Ÿæˆï¼‰
            current_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            elapsed = time.time() - total_start

            stats_text = f"""ğŸ”„ ç”Ÿæˆä¸­...

Tokenæ•°: {len(generated_tokens)}/{max_new_tokens}
é€Ÿåº¦: {len(generated_tokens)/elapsed:.2f} tokens/s
å¹³å‡å»¶è¿Ÿ: {sum(token_times)/len(token_times):.2f}ms/token
"""

            yield prompt + current_text, stats_text

    # æœ€ç»ˆç»Ÿè®¡
    total_time = time.time() - total_start
    final_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)

    # è®°å½•æœ€ç»ˆç»Ÿè®¡åˆ°æ—¥å¿—
    if generated_tokens:
        logger.info(f"âœ… ç”Ÿæˆå®Œæˆ: {len(generated_tokens)} tokens in {total_time:.2f}s "
                   f"({len(generated_tokens)/total_time:.2f} tokens/s)")
        if token_times:
            avg_latency = sum(token_times) / len(token_times)
            logger.info(f"   å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}ms/token, "
                       f"æœ€å°: {min(token_times):.2f}ms, æœ€å¤§: {max(token_times):.2f}ms")
        
        stats_text = f"""âœ… ç”Ÿæˆå®Œæˆ

æ€»Tokenæ•°: {len(generated_tokens)}
æ€»æ—¶é—´: {total_time:.2f}s
å¹³å‡é€Ÿåº¦: {len(generated_tokens)/total_time:.2f} tokens/s
å¹³å‡å»¶è¿Ÿ: {sum(token_times)/len(token_times):.2f}ms/token

æœ€å°å»¶è¿Ÿ: {min(token_times):.2f}ms
æœ€å¤§å»¶è¿Ÿ: {max(token_times):.2f}ms
"""
    else:
        logger.warning("âš  æœªç”Ÿæˆä»»ä½• token")
        stats_text = "âš  æœªç”Ÿæˆä»»ä½• tokenï¼ˆå¯èƒ½é‡åˆ° EOS æˆ–é”™è¯¯ï¼‰"
        final_text = ""

    yield prompt + final_text, stats_text


# ============================================================================
# ç»Ÿè®¡åˆ†æ
# ============================================================================

def update_stats():
    """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
    if not all_token_stats:
        empty_df = pd.DataFrame(columns=["æŒ‡æ ‡", "å€¼"])
        empty_fig = go.Figure()
        empty_fig.update_layout(title="æš‚æ— æ•°æ®")
        return empty_df, empty_fig, empty_fig

    # åˆ›å»ºç»Ÿè®¡è¡¨
    times = [s['time_ms'] for s in all_token_stats]

    df = pd.DataFrame({
        "æŒ‡æ ‡": [
            "æ€»Tokenæ•°",
            "å¹³å‡å»¶è¿Ÿ(ms)",
            "æœ€å°å»¶è¿Ÿ(ms)",
            "æœ€å¤§å»¶è¿Ÿ(ms)",
            "æ ‡å‡†å·®(ms)",
        ],
        "å€¼": [
            len(all_token_stats),
            f"{sum(times) / len(times):.2f}",
            f"{min(times):.2f}",
            f"{max(times):.2f}",
            f"{pd.Series(times).std():.2f}",
        ]
    })

    # å»¶è¿Ÿåˆ†å¸ƒç›´æ–¹å›¾
    fig_dist = go.Figure(data=[go.Histogram(x=times, nbinsx=20)])
    fig_dist.update_layout(
        title="Token å»¶è¿Ÿåˆ†å¸ƒ",
        xaxis_title="å»¶è¿Ÿ (ms)",
        yaxis_title="é¢‘æ¬¡"
    )

    # Token ç”Ÿæˆæ—¶é—´çº¿
    steps = [s['step'] for s in all_token_stats[-50:]]  # æœ€è¿‘50ä¸ª
    times_recent = [s['time_ms'] for s in all_token_stats[-50:]]

    fig_timeline = go.Figure()
    fig_timeline.add_trace(go.Scatter(x=steps, y=times_recent, name="å»¶è¿Ÿ", mode='lines+markers'))
    fig_timeline.update_layout(
        title="Token ç”Ÿæˆæ—¶é—´çº¿ï¼ˆæœ€è¿‘50ä¸ªï¼‰",
        xaxis_title="Token åºå·",
        yaxis_title="æ—¶é—´ (ms)"
    )

    return df, fig_dist, fig_timeline


# ============================================================================
# Gradio ç•Œé¢
# ============================================================================

with gr.Blocks(title="GPT-2 å®Œæ•´æ¨¡å‹", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# GPT-2 å®Œæ•´æ¨¡å‹ï¼ˆå¯¹ç…§ç»„ï¼‰")
    gr.Markdown(f"""
    **æ¶æ„**: å®Œæ•´çš„ GPT-2 (12 å±‚ transformer)
    **è®¾å¤‡**: {device}
    **ä¼˜åŒ–**: KV Cache + torch.compile()
    **ç”¨é€”**: æ€§èƒ½å¯¹ç…§åŸºçº¿
    """)

    with gr.Tab("ğŸ“ æ–‡æœ¬ç”Ÿæˆ"):
        with gr.Row():
            with gr.Column(scale=3):
                prompt_input = gr.Textbox(
                    label="è¾“å…¥æç¤º (Prompt)",
                    placeholder="ä¾‹å¦‚: The future of AI is",
                    lines=3,
                    value="Once upon a time"
                )
                gr.Examples(
                    examples=[
                        ["Once upon a time"],
                        ["The future of AI is"],
                        ["In the year 2050,"],
                        ["Hello, my name is"],
                    ],
                    inputs=prompt_input,
                )

            with gr.Column(scale=2):
                max_tokens = gr.Slider(1, 200, value=50, step=1, label="æœ€å¤§ç”Ÿæˆ tokens")
                temperature = gr.Slider(0.0, 2.0, value=1.0, step=0.1, label="Temperature")
                top_k = gr.Slider(0, 100, value=50, step=1, label="Top-K")

        with gr.Row():
            generate_btn = gr.Button("ğŸš€ å¼€å§‹ç”Ÿæˆ", variant="primary", size="lg")
            test_btn = gr.Button("ğŸ§ª æµ‹è¯•è¿æ¥", variant="secondary", size="lg")
        
        # æµ‹è¯•è¾“å‡ºç»„ä»¶ï¼ˆå¿…é¡»åœ¨æŒ‰é’®ä¹‹åå®šä¹‰ï¼Œåœ¨ç”Ÿæˆè¾“å‡ºä¹‹å‰ï¼‰
        test_output = gr.Textbox(label="æµ‹è¯•ç»“æœ", lines=2, interactive=False, visible=True)
        
        def test_connection():
            """æµ‹è¯•å‡½æ•°ï¼ŒéªŒè¯ Gradio æ˜¯å¦èƒ½æ­£å¸¸è°ƒç”¨åç«¯å‡½æ•°"""
            logger.info("=" * 70)
            logger.info("[TEST] ========== æµ‹è¯•æŒ‰é’®è¢«ç‚¹å‡»ï¼==========")
            logger.info("[TEST] å¦‚æœçœ‹åˆ°è¿™æ¡æ—¥å¿—ï¼Œè¯´æ˜ Gradio å¯ä»¥æ­£å¸¸è°ƒç”¨åç«¯å‡½æ•°")
            logger.info("=" * 70)
            return "âœ… æµ‹è¯•æˆåŠŸï¼åç«¯å‡½æ•°å¯ä»¥æ­£å¸¸è°ƒç”¨ã€‚å¦‚æœçœ‹åˆ°è¿™æ¡æ¶ˆæ¯ï¼Œè¯´æ˜ Gradio è¿æ¥æ­£å¸¸ã€‚"

        with gr.Row():
            with gr.Column(scale=3):
                output_text = gr.Textbox(label="ç”Ÿæˆç»“æœ", lines=12, interactive=False)

            with gr.Column(scale=1):
                stats_display = gr.Textbox(label="ç”Ÿæˆç»Ÿè®¡", lines=12, interactive=False)

        # æŒ‰é’®äº‹ä»¶ - ç›´æ¥ä½¿ç”¨å‡½æ•°ï¼Œä¸å®¢æˆ·ç«¯ä¿æŒä¸€è‡´
        # æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼Œç¡®è®¤æŒ‰é’®ç»‘å®š
        logger.info("=" * 70)
        logger.info("[UI] æ­£åœ¨ç»‘å®šæŒ‰é’®äº‹ä»¶...")
        logger.info(f"[UI] ç”ŸæˆæŒ‰é’®: fn={generate_with_kv_cache.__name__}")
        logger.info(f"[UI] è¾“å…¥ç»„ä»¶: prompt_input, max_tokens, temperature, top_k")
        logger.info(f"[UI] è¾“å‡ºç»„ä»¶: output_text, stats_display")
        
        generate_btn.click(
            fn=generate_with_kv_cache,
            inputs=[prompt_input, max_tokens, temperature, top_k],
            outputs=[output_text, stats_display],
        )
        
        logger.info("[UI] âœ“ æŒ‰é’®äº‹ä»¶ç»‘å®šå®Œæˆ")
        logger.info("=" * 70)
        
        # æµ‹è¯•æŒ‰é’®äº‹ä»¶
        test_btn.click(
            fn=test_connection,
            inputs=[],
            outputs=[test_output],
        )

    with gr.Tab("ğŸ“Š æ€§èƒ½ç»Ÿè®¡"):
        gr.Markdown("Token ç”Ÿæˆæ€§èƒ½åˆ†æï¼ˆæ¯5ç§’åˆ·æ–°ï¼‰")

        stats_table = gr.DataFrame(label="ç»Ÿè®¡æ‘˜è¦")

        with gr.Row():
            stats_dist_plot = gr.Plot(label="å»¶è¿Ÿåˆ†å¸ƒ")
            stats_timeline_plot = gr.Plot(label="æ—¶é—´çº¿")

        # è‡ªåŠ¨åˆ·æ–°ç»Ÿè®¡
        demo.load(
            fn=update_stats,
            outputs=[stats_table, stats_dist_plot, stats_timeline_plot],
            every=5
        )


if __name__ == "__main__":
    # ========================================================================
    # å…³é”®è¯Šæ–­ï¼šç¡®è®¤æŒ‰é’®ç»‘å®šæ˜¯å¦å·²æ‰§è¡Œ
    # ========================================================================
    logger.info("=" * 70)
    logger.info("[å¯åŠ¨æ£€æŸ¥] å¼€å§‹å¯åŠ¨ Gradio æœåŠ¡å™¨...")
    logger.info(f"[å¯åŠ¨æ£€æŸ¥] demo å¯¹è±¡: {demo}")
    logger.info(f"[å¯åŠ¨æ£€æŸ¥] æŒ‰é’®å¯¹è±¡: generate_btn={generate_btn if 'generate_btn' in globals() else 'æœªå®šä¹‰'}")
    logger.info("=" * 70)
    logger.info("\nå¯åŠ¨ Gradio ç•Œé¢...")
    
    # ========================================================================
    # é‡è¦ï¼šå¯ç”¨é˜Ÿåˆ—ä»¥æ”¯æŒå¹¶å‘è¯·æ±‚å’Œæ›´å¥½çš„é”™è¯¯å¤„ç†
    # ========================================================================
    logger.info("[UI] å¯ç”¨ Gradio é˜Ÿåˆ—...")
    demo.queue(
        max_size=10,  # æœ€å¤§é˜Ÿåˆ—é•¿åº¦
        default_concurrency_limit=1,  # å¹¶å‘é™åˆ¶ï¼ˆç”Ÿæˆä»»åŠ¡é€šå¸¸ä¸²è¡Œæ‰§è¡Œï¼‰
    )
    logger.info("[UI] âœ“ é˜Ÿåˆ—å·²å¯ç”¨")
    
    # é¢„å…ˆè¾“å‡ºåœ°å€ä¿¡æ¯ï¼ˆå› ä¸º launch() æ˜¯é˜»å¡çš„ï¼‰
    expected_local_url = f"http://127.0.0.1:{FULL_PORT}/"
    logger.info("=" * 70)
    logger.info(f"é¢„æœŸæœ¬åœ°åœ°å€: {expected_local_url}")
    if SHARE_ENABLED:
        logger.info("å…¬ç½‘åˆ†äº«: å°†è‡ªåŠ¨åˆ›å»ºåˆ†äº«é“¾æ¥")
    logger.info("=" * 70)
    logger.info("")
    logger.info(
        f"å¯åŠ¨ Gradio: share={SHARE_ENABLED}, server_name=127.0.0.1, "
        f"port={FULL_PORT}"
    )
    
    local_url = share_url = None
    try:
        logger.info("æ­£åœ¨å¯åŠ¨ Gradio æœåŠ¡å™¨...")
        logger.info("æç¤º: demo.launch() æ˜¯é˜»å¡è°ƒç”¨ï¼ŒæœåŠ¡å™¨å¯åŠ¨ååœ°å€ä¿¡æ¯ä¼šåœ¨æ§åˆ¶å°æ˜¾ç¤º")
        
        # ç›´æ¥å¯åŠ¨ï¼ŒGradio ä¼šåœ¨æ§åˆ¶å°æ‰“å°åœ°å€ä¿¡æ¯
        # æ³¨æ„ï¼šlaunch() æ˜¯é˜»å¡çš„ï¼Œä¸ä¼šè¿”å›ï¼Œé™¤éæœåŠ¡å™¨å…³é—­
        launch_result = demo.launch(
            share=SHARE_ENABLED,  # å¯é€šè¿‡ç¯å¢ƒå˜é‡ GRADIO_SHARE æ§åˆ¶
            server_name="127.0.0.1",  # ä½¿ç”¨ 127.0.0.1 é¿å… localhost è®¿é—®é—®é¢˜
            server_port=FULL_PORT,
            show_error=True,
            inbrowser=SHARE_ENABLED,
        )
        # æ³¨æ„ï¼šå¦‚æœ launch() è¿”å›äº†ï¼ˆé€šå¸¸ä¸ä¼šï¼‰ï¼Œå¤„ç†è¿”å›å€¼
        if launch_result:
            if isinstance(launch_result, tuple) and len(launch_result) >= 3:
                _, local_url, share_url = launch_result
            else:
                local_url = f"http://127.0.0.1:{FULL_PORT}/"
    except ValueError as e:
        # localhost ä¸å¯è®¿é—®æ—¶ï¼Œè‡ªåŠ¨å¯ç”¨ share
        if "localhost" in str(e).lower() or "share" in str(e).lower():
            logger.warning(f"localhost ä¸å¯è®¿é—®ï¼Œè‡ªåŠ¨å¯ç”¨å…¬ç½‘åˆ†äº«: {e}")
            try:
                launch_result = demo.launch(
                    share=True,
                    server_name="127.0.0.1",
                    server_port=FULL_PORT,
                    show_error=True,
                    inbrowser=True,
                )
                if isinstance(launch_result, tuple) and len(launch_result) >= 3:
                    _, local_url, share_url = launch_result
                else:
                    local_url = f"http://127.0.0.1:{FULL_PORT}/"
            except Exception as e2:
                logger.error(f"å¯åŠ¨å…¬ç½‘åˆ†äº«å¤±è´¥: {e2}")
                local_url = f"http://127.0.0.1:{FULL_PORT}/"
        else:
            raise
    except Exception as e:
        logger.error(f"å¯åŠ¨ Gradio å¤±è´¥: {e}", exc_info=True)
        local_url = f"http://127.0.0.1:{FULL_PORT}/"
    
    logger.info("=" * 70)
    logger.info(f"âœ“ Gradio æœ¬åœ°åœ°å€: {local_url}")
    logger.info(f"âœ“ Gradio åˆ†äº«é“¾æ¥: {share_url if share_url else 'æœªå¼€å¯æˆ–åˆ›å»ºå¤±è´¥'}")
    logger.info("=" * 70)
    logger.info("æç¤º: å¦‚æœç‚¹å‡»ç”ŸæˆæŒ‰é’®åæ²¡æœ‰æ—¥å¿—ï¼Œè¯·ç¡®è®¤:")
    logger.info("  1. è®¿é—®çš„æ˜¯æ­£ç¡®çš„åœ°å€ï¼ˆä¸Šé¢æ˜¾ç¤ºçš„æœ¬åœ°åœ°å€æˆ–åˆ†äº«é“¾æ¥ï¼‰")
    logger.info("  2. é¡µé¢å·²å®Œå…¨åŠ è½½ï¼ˆæ²¡æœ‰ 502 é”™è¯¯ï¼‰")
    logger.info("  3. æµè§ˆå™¨æ§åˆ¶å°æ²¡æœ‰ JavaScript é”™è¯¯")
    logger.info("=" * 70)
