"""
GPT-2 å®Œæ•´æ¨¡åž‹éƒ¨ç½²ï¼ˆå¯¹ç…§ç»„ï¼‰

ç”¨é€”ï¼š
  - ä½œä¸ºåˆ†æ‹†æ¨¡åž‹çš„æ€§èƒ½å¯¹ç…§
  - æµ‹è¯•åˆ†æ‹†æ˜¯å¦å¼•å…¥é¢å¤–å¼€é”€
  - éªŒè¯ç”Ÿæˆè´¨é‡ä¸€è‡´æ€§

æž¶æž„ï¼š
  - å•æœºè¿è¡Œå®Œæ•´çš„ GPT-2 æ¨¡åž‹
  - åŒæ ·çš„ KV Cache å’Œä¼˜åŒ–
  - åŒæ ·çš„æ€§èƒ½ç»Ÿè®¡

ç”¨æ³•ï¼š
    PYTHONPATH=./SplitLearnCore/src:./SplitLearnComm/src python gpt2_full_model_gradio.py
"""

import os
import sys
import time
import logging
from pathlib import Path

import torch
import gradio as gr
import plotly.graph_objects as go
import pandas as pd

# æ·»åŠ è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "SplitLearnCore" / "src"))
sys.path.insert(0, str(Path(__file__).parent / "SplitLearnComm" / "src"))

from transformers import GPT2LMHeadModel, AutoTokenizer

# é…ç½®æ—¥å¿—
log_dir = Path("./logs")
log_dir.mkdir(exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_dir / "gpt2_full.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ============================================================================
# å…¨å±€å˜é‡ï¼ˆæ¨¡åž‹åŠ è½½ï¼‰
# ============================================================================

model_id = "gpt2"
device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"

logger.info("=" * 70)
logger.info("GPT-2 å®Œæ•´æ¨¡åž‹ï¼ˆå¯¹ç…§ç»„ï¼‰")
logger.info("=" * 70)
logger.info(f"è®¾å¤‡: {device}")
logger.info(f"æ¨¡åž‹: {model_id}")

logger.info("åŠ è½½å®Œæ•´ GPT-2 æ¨¡åž‹...")

# åŠ è½½æ¨¡åž‹å’Œåˆ†è¯å™¨
model = GPT2LMHeadModel.from_pretrained(model_id, cache_dir="./models")
tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir="./models")

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model.to(device)
model.eval()

# åº”ç”¨ torch.compile() ä¼˜åŒ–
if hasattr(torch, 'compile') and device == "cuda":
    logger.info("åº”ç”¨ torch.compile() ä¼˜åŒ–...")
    try:
        model = torch.compile(model, mode="reduce-overhead")
        logger.info("âœ“ torch.compile() ä¼˜åŒ–å·²åº”ç”¨")
    except Exception as e:
        logger.warning(f"torch.compile() ä¼˜åŒ–å¤±è´¥: {e}")

logger.info(f"âœ“ æ¨¡åž‹åŠ è½½å®Œæˆ")
logger.info(f"âœ“ å‚æ•°é‡: {sum(p.numel() for p in model.parameters())/1e6:.2f}M")

# å…¨å±€ç»Ÿè®¡å˜é‡
all_token_stats = []


# ============================================================================
# æ ¸å¿ƒç”Ÿæˆå‡½æ•°
# ============================================================================

def generate_with_kv_cache(
    prompt: str,
    max_new_tokens: int = 50,
    temperature: float = 1.0,
    top_k: int = 50,
):
    """ä½¿ç”¨ KV Cache çš„å®Œæ•´æ¨¡åž‹ç”Ÿæˆï¼ˆç”Ÿæˆå™¨å‡½æ•°ï¼Œæµå¼è¾“å‡ºï¼‰"""
    global all_token_stats

    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)

    # ç»Ÿè®¡
    token_times = []
    total_start = time.time()

    generated_tokens = []
    past_key_values = None

    with torch.no_grad():
        for step in range(max_new_tokens):
            step_start = time.time()

            # è¾“å…¥
            if step == 0:
                current_input_ids = input_ids
            else:
                current_input_ids = torch.tensor([[next_token_id]], device=device)

            # å‰å‘ä¼ æ’­ï¼ˆå¸¦ KV Cacheï¼‰
            outputs = model(
                current_input_ids,
                past_key_values=past_key_values,
                use_cache=True,
            )

            logits = outputs.logits[0, -1, :]
            past_key_values = outputs.past_key_values

            # é‡‡æ ·
            if temperature > 0:
                logits = logits / temperature
                if top_k > 0:
                    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]
                    logits[indices_to_remove] = float('-inf')
                probs = torch.softmax(logits, dim=-1)
                next_token_id = torch.multinomial(probs, num_samples=1).item()
            else:
                next_token_id = logits.argmax(dim=-1).item()

            generated_tokens.append(next_token_id)

            # è®°å½•æ—¶é—´
            token_time = time.time() - step_start
            token_times.append(token_time * 1000)

            # è®°å½•è¯¦ç»†ç»Ÿè®¡
            all_token_stats.append({
                "step": step,
                "token_id": next_token_id,
                "time_ms": token_time * 1000,
            })

            if next_token_id == tokenizer.eos_token_id:
                break

            # å®žæ—¶è¾“å‡º
            current_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            elapsed = time.time() - total_start

            stats_text = f"""ðŸ”„ ç”Ÿæˆä¸­...

Tokenæ•°: {len(generated_tokens)}/{max_new_tokens}
é€Ÿåº¦: {len(generated_tokens)/elapsed:.2f} tokens/s
å¹³å‡å»¶è¿Ÿ: {sum(token_times)/len(token_times):.2f}ms/token
"""

            yield prompt + current_text, stats_text

    # æœ€ç»ˆç»Ÿè®¡
    total_time = time.time() - total_start
    final_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)

    stats_text = f"""âœ… ç”Ÿæˆå®Œæˆ

æ€»Tokenæ•°: {len(generated_tokens)}
æ€»æ—¶é—´: {total_time:.2f}s
å¹³å‡é€Ÿåº¦: {len(generated_tokens)/total_time:.2f} tokens/s
å¹³å‡å»¶è¿Ÿ: {sum(token_times)/len(token_times):.2f}ms/token

æœ€å°å»¶è¿Ÿ: {min(token_times):.2f}ms
æœ€å¤§å»¶è¿Ÿ: {max(token_times):.2f}ms
"""

    yield prompt + final_text, stats_text


# ============================================================================
# ç»Ÿè®¡åˆ†æž
# ============================================================================

def update_stats():
    """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
    if not all_token_stats:
        empty_df = pd.DataFrame(columns=["æŒ‡æ ‡", "å€¼"])
        empty_fig = go.Figure()
        empty_fig.update_layout(title="æš‚æ— æ•°æ®")
        return empty_df, empty_fig, empty_fig

    # åˆ›å»ºç»Ÿè®¡è¡¨
    times = [s['time_ms'] for s in all_token_stats]

    df = pd.DataFrame({
        "æŒ‡æ ‡": [
            "æ€»Tokenæ•°",
            "å¹³å‡å»¶è¿Ÿ(ms)",
            "æœ€å°å»¶è¿Ÿ(ms)",
            "æœ€å¤§å»¶è¿Ÿ(ms)",
            "æ ‡å‡†å·®(ms)",
        ],
        "å€¼": [
            len(all_token_stats),
            f"{sum(times) / len(times):.2f}",
            f"{min(times):.2f}",
            f"{max(times):.2f}",
            f"{pd.Series(times).std():.2f}",
        ]
    })

    # å»¶è¿Ÿåˆ†å¸ƒç›´æ–¹å›¾
    fig_dist = go.Figure(data=[go.Histogram(x=times, nbinsx=20)])
    fig_dist.update_layout(
        title="Token å»¶è¿Ÿåˆ†å¸ƒ",
        xaxis_title="å»¶è¿Ÿ (ms)",
        yaxis_title="é¢‘æ¬¡"
    )

    # Token ç”Ÿæˆæ—¶é—´çº¿
    steps = [s['step'] for s in all_token_stats[-50:]]  # æœ€è¿‘50ä¸ª
    times_recent = [s['time_ms'] for s in all_token_stats[-50:]]

    fig_timeline = go.Figure()
    fig_timeline.add_trace(go.Scatter(x=steps, y=times_recent, name="å»¶è¿Ÿ", mode='lines+markers'))
    fig_timeline.update_layout(
        title="Token ç”Ÿæˆæ—¶é—´çº¿ï¼ˆæœ€è¿‘50ä¸ªï¼‰",
        xaxis_title="Token åºå·",
        yaxis_title="æ—¶é—´ (ms)"
    )

    return df, fig_dist, fig_timeline


# ============================================================================
# Gradio ç•Œé¢
# ============================================================================

with gr.Blocks(title="GPT-2 å®Œæ•´æ¨¡åž‹", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# GPT-2 å®Œæ•´æ¨¡åž‹ï¼ˆå¯¹ç…§ç»„ï¼‰")
    gr.Markdown(f"""
    **æž¶æž„**: å®Œæ•´çš„ GPT-2 (12 å±‚ transformer)
    **è®¾å¤‡**: {device}
    **ä¼˜åŒ–**: KV Cache + torch.compile()
    **ç”¨é€”**: æ€§èƒ½å¯¹ç…§åŸºçº¿
    """)

    with gr.Tab("ðŸ“ æ–‡æœ¬ç”Ÿæˆ"):
        with gr.Row():
            with gr.Column(scale=3):
                prompt_input = gr.Textbox(
                    label="è¾“å…¥æç¤º (Prompt)",
                    placeholder="ä¾‹å¦‚: The future of AI is",
                    lines=3,
                    value="Once upon a time"
                )
                gr.Examples(
                    examples=[
                        ["Once upon a time"],
                        ["The future of AI is"],
                        ["In the year 2050,"],
                        ["Hello, my name is"],
                    ],
                    inputs=prompt_input,
                )

            with gr.Column(scale=2):
                max_tokens = gr.Slider(1, 200, value=50, step=1, label="æœ€å¤§ç”Ÿæˆ tokens")
                temperature = gr.Slider(0.0, 2.0, value=1.0, step=0.1, label="Temperature")
                top_k = gr.Slider(0, 100, value=50, step=1, label="Top-K")

        generate_btn = gr.Button("ðŸš€ å¼€å§‹ç”Ÿæˆ", variant="primary", size="lg")

        with gr.Row():
            with gr.Column(scale=3):
                output_text = gr.Textbox(label="ç”Ÿæˆç»“æžœ", lines=12, interactive=False)

            with gr.Column(scale=1):
                stats_display = gr.Textbox(label="ç”Ÿæˆç»Ÿè®¡", lines=12, interactive=False)

        # æŒ‰é’®äº‹ä»¶
        generate_btn.click(
            fn=generate_with_kv_cache,
            inputs=[prompt_input, max_tokens, temperature, top_k],
            outputs=[output_text, stats_display],
        )

    with gr.Tab("ðŸ“Š æ€§èƒ½ç»Ÿè®¡"):
        gr.Markdown("Token ç”Ÿæˆæ€§èƒ½åˆ†æžï¼ˆæ¯5ç§’åˆ·æ–°ï¼‰")

        stats_table = gr.DataFrame(label="ç»Ÿè®¡æ‘˜è¦")

        with gr.Row():
            stats_dist_plot = gr.Plot(label="å»¶è¿Ÿåˆ†å¸ƒ")
            stats_timeline_plot = gr.Plot(label="æ—¶é—´çº¿")

        # è‡ªåŠ¨åˆ·æ–°ç»Ÿè®¡
        demo.load(
            fn=update_stats,
            outputs=[stats_table, stats_dist_plot, stats_timeline_plot],
            every=5
        )


if __name__ == "__main__":
    logger.info("\nå¯åŠ¨ Gradio ç•Œé¢...")
    demo.queue()  # Gradio 6.0 æŽ¨èæ·»åŠ é˜Ÿåˆ—
    demo.launch(
        share=True,  # ä½¿ç”¨ Gradio å…¬ç½‘åˆ†äº«é“¾æŽ¥
        show_error=True,
        inbrowser=True,
    )
