"""
å¸¦ Gradio ç•Œé¢çš„ Split Learning å®¢æˆ·ç«¯ (æ”¹è¿›ç‰ˆ)
"""
import torch
import gradio as gr
import time
import os
import sys
from transformers import AutoTokenizer

# æ·»åŠ è·¯å¾„ä»¥ä¾¿å¯¼å…¥æœ¬åœ°æ¨¡å—
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.append(os.path.join(project_root, 'SplitLearning', 'src'))
sys.path.append(os.path.join(project_root, 'splitlearn-comm', 'src'))

from splitlearn_comm import GRPCComputeClient

# å…¨å±€å˜é‡
bottom_model = None
top_model = None
tokenizer = None
client = None
stop_generation = False  # åœæ­¢ç”Ÿæˆæ ‡å¿—

def load_models():
    """åŠ è½½æœ¬åœ°æ¨¡å‹å’Œè¿æ¥æœåŠ¡å™¨"""
    global bottom_model, top_model, tokenizer, client

    try:
        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰ç¼“å­˜çš„æ¨¡å‹æ–‡ä»¶
        bottom_path = os.path.join(current_dir, "gpt2_bottom_cached.pt")
        top_path = os.path.join(current_dir, "gpt2_top_cached.pt")

        if os.path.exists(bottom_path) and os.path.exists(top_path):
            print("ä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹æ–‡ä»¶...")
            bottom_model = torch.load(bottom_path, map_location='cpu', weights_only=False)
            top_model = torch.load(top_path, map_location='cpu', weights_only=False)
            tokenizer = AutoTokenizer.from_pretrained('gpt2')
            print("æ¨¡å‹åŠ è½½å®Œæˆ")
        else:
            # éœ€è¦ä¸‹è½½å¹¶æ‹†åˆ†
            from splitlearn import ModelFactory
            print("é¦–æ¬¡è¿è¡Œï¼šæ­£åœ¨ä¸‹è½½å¹¶æ‹†åˆ† GPT-2 æ¨¡å‹...")
            print("è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
            bottom, trunk, top = ModelFactory.create_split_models(
                model_type='gpt2',
                model_name_or_path='gpt2',
                split_point_1=2,
                split_point_2=10,
                device='cpu'
            )
            bottom_model = bottom
            top_model = top
            tokenizer = AutoTokenizer.from_pretrained('gpt2')

            # ä¿å­˜ä»¥ä¾¿ä¸‹æ¬¡ä½¿ç”¨
            torch.save(bottom_model, bottom_path)
            torch.save(top_model, top_path)
            print("æ¨¡å‹å·²ç¼“å­˜åˆ°æœ¬åœ°")

        # è¿æ¥æœåŠ¡å™¨
        print("æ­£åœ¨è¿æ¥æœåŠ¡å™¨ 192.168.0.144:50053...")
        client = GRPCComputeClient("192.168.0.144:50053", timeout=20.0)
        if not client.connect():
            return "âŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ï¼\nè¯·ç¡®ä¿æœåŠ¡å™¨æ­£åœ¨è¿è¡Œï¼š\n  python testcode/start_server.py\n\n(å°è¯•äº†è¿æ¥ 127.0.0.1:50053)\n\næˆ–è¿è¡Œå‡†å¤‡è„šæœ¬ï¼š\n  python testcode/prepare_models.py"

        return "âœ… åˆå§‹åŒ–æˆåŠŸï¼\n\nâœ“ Bottom æ¨¡å‹å·²åŠ è½½\nâœ“ Top æ¨¡å‹å·²åŠ è½½\nâœ“ æœåŠ¡å™¨å·²è¿æ¥ (localhost:50053)\n\nç°åœ¨å¯ä»¥å¼€å§‹ç”Ÿæˆæ–‡æœ¬äº†ï¼"
    except Exception as e:
        import traceback
        error_msg = f"âŒ åˆå§‹åŒ–å¤±è´¥:\n\n{str(e)}\n\nè¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}"
        return error_msg


def check_models_loaded():
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½"""
    if bottom_model is None or top_model is None or tokenizer is None:
        return False, "âš ï¸ è¯·å…ˆç‚¹å‡»'åˆå§‹åŒ–æ¨¡å‹å¹¶è¿æ¥æœåŠ¡å™¨'æŒ‰é’®ï¼"
    if client is None or not client.channel:
        return False, "âš ï¸ æœåŠ¡å™¨æœªè¿æ¥ï¼è¯·é‡æ–°åˆå§‹åŒ–ã€‚"
    return True, ""


def stop_generation_fn():
    """åœæ­¢ç”Ÿæˆ"""
    global stop_generation
    stop_generation = True
    return "ğŸ›‘ æ­£åœ¨åœæ­¢ç”Ÿæˆ..."


def generate_text(prompt, max_length=20, temperature=1.0, top_k=50, show_speed=True):
    """ç”Ÿæˆæ–‡æœ¬çš„æ ¸å¿ƒé€»è¾‘ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
    global stop_generation
    stop_generation = False

    # æ£€æŸ¥æ¨¡å‹å’ŒæœåŠ¡å™¨çŠ¶æ€
    is_ready, error_msg = check_models_loaded()
    if not is_ready:
        yield error_msg, ""
        return

    # éªŒè¯è¾“å…¥
    if not prompt or len(prompt.strip()) == 0:
        yield "âš ï¸ è¯·è¾“å…¥ä¸€ä¸ªæœ‰æ•ˆçš„ promptï¼", ""
        return

    try:
        input_ids = tokenizer.encode(prompt, return_tensors="pt")
        generated_text = prompt
        start_time = time.time()
        tokens_generated = 0

        yield generated_text, "ğŸ”„ ç”Ÿæˆä¸­..."

        for step in range(max_length):
            if stop_generation:
                elapsed_time = time.time() - start_time
                avg_speed = tokens_generated / elapsed_time if elapsed_time > 0 else 0
                stats = f"ğŸ›‘ ç”Ÿæˆå·²åœæ­¢\n\nç”Ÿæˆäº† {tokens_generated} ä¸ª tokens\nå¹³å‡é€Ÿåº¦: {avg_speed:.2f} tokens/s"
                yield generated_text, stats
                break

            # 1. Bottom (æœ¬åœ°)
            with torch.no_grad():
                hidden_bottom = bottom_model(input_ids)

            # 2. Trunk (è¿œç¨‹æœåŠ¡å™¨)
            try:
                hidden_trunk = client.compute(hidden_bottom, model_id="gpt2-trunk")
            except Exception as e:
                error_stats = f"âŒ æœåŠ¡å™¨é€šä¿¡é”™è¯¯\n\n{str(e)}\n\nå·²ç”Ÿæˆ {tokens_generated} tokens"
                yield generated_text + f"\n\n[é”™è¯¯: {str(e)}]", error_stats
                break

            # 3. Top (æœ¬åœ°)
            with torch.no_grad():
                output = top_model(hidden_trunk)
                logits = output.logits[:, -1, :] / temperature

            # 4. Sampling (æ”¹è¿›çš„é‡‡æ ·ç­–ç•¥)
            if top_k > 0:
                # Top-K é‡‡æ ·
                top_k_logits, top_k_indices = torch.topk(logits, top_k)
                probs = torch.softmax(top_k_logits, dim=-1)
                next_token_idx = torch.multinomial(probs, num_samples=1)
                next_token_id = top_k_indices.gather(-1, next_token_idx)
            else:
                # Greedy é‡‡æ ·
                next_token_id = logits.argmax(dim=-1).unsqueeze(-1)

            input_ids = torch.cat([input_ids, next_token_id], dim=-1)
            tokens_generated += 1

            # Decode and update UI
            new_word = tokenizer.decode(next_token_id[0])
            generated_text += new_word

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            elapsed_time = time.time() - start_time
            avg_speed = tokens_generated / elapsed_time if elapsed_time > 0 else 0

            if show_speed:
                stats = f"â±ï¸ ç”Ÿæˆç»Ÿè®¡\n\nTokens: {tokens_generated}/{max_length}\né€Ÿåº¦: {avg_speed:.2f} tokens/s\nè€—æ—¶: {elapsed_time:.2f}s"
            else:
                stats = f"Tokens: {tokens_generated}/{max_length}"

            yield generated_text, stats

            # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†ç»“æŸæ ‡è®°
            if tokenizer.eos_token_id and next_token_id.item() == tokenizer.eos_token_id:
                final_stats = f"âœ… ç”Ÿæˆå®Œæˆ (é‡åˆ° EOS)\n\nç”Ÿæˆäº† {tokens_generated} ä¸ª tokens\nå¹³å‡é€Ÿåº¦: {avg_speed:.2f} tokens/s\næ€»è€—æ—¶: {elapsed_time:.2f}s"
                yield generated_text, final_stats
                break

            time.sleep(0.02)  # å‡å°‘å»¶è¿Ÿä»¥æå‡ä½“éªŒ
        else:
            # æ­£å¸¸å®Œæˆ
            elapsed_time = time.time() - start_time
            avg_speed = tokens_generated / elapsed_time if elapsed_time > 0 else 0
            final_stats = f"âœ… ç”Ÿæˆå®Œæˆ\n\nç”Ÿæˆäº† {tokens_generated} ä¸ª tokens\nå¹³å‡é€Ÿåº¦: {avg_speed:.2f} tokens/s\næ€»è€—æ—¶: {elapsed_time:.2f}s"
            yield generated_text, final_stats

    except Exception as e:
        import traceback
        error_msg = f"âŒ ç”Ÿæˆè¿‡ç¨‹å‡ºé”™:\n\n{str(e)}\n\n{traceback.format_exc()}"
        yield generated_text, error_msg


# åˆ›å»º Gradio ç•Œé¢ (æ”¹è¿›ç‰ˆ)
with gr.Blocks(
    title="Split Learning Demo",
    theme=gr.themes.Soft(),
    css="""
    .status-box {font-family: monospace; font-size: 14px;}
    .stats-box {font-family: monospace; font-size: 12px;}
    """
) as demo:

    gr.Markdown(
        """
        # ğŸš€ Split Learning åˆ†å¸ƒå¼æ¨ç†æ¼”ç¤º

        **æ¶æ„**: Bottom(æœ¬åœ°) â†’ Trunk(è¿œç¨‹æœåŠ¡å™¨) â†’ Top(æœ¬åœ°)

        è¿™ä¸ªæ¼”ç¤ºå±•ç¤ºäº†å¦‚ä½•å°†å¤§å‹è¯­è¨€æ¨¡å‹æ‹†åˆ†åˆ°å¤šä¸ªè®¾å¤‡ä¸Šè¿›è¡Œæ¨ç†ã€‚
        """
    )

    # åˆå§‹åŒ–åŒºåŸŸ
    with gr.Group():
        gr.Markdown("### 1ï¸âƒ£ åˆå§‹åŒ–")
        status_box = gr.Textbox(
            label="ç³»ç»ŸçŠ¶æ€",
            value="âšª æœªåˆå§‹åŒ– - è¯·ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®å¼€å§‹",
            lines=6,
            elem_classes=["status-box"]
        )
        init_btn = gr.Button("ğŸ”Œ åˆå§‹åŒ–æ¨¡å‹å¹¶è¿æ¥æœåŠ¡å™¨", variant="primary", size="lg")

    gr.Markdown("---")

    # ç”ŸæˆåŒºåŸŸ
    with gr.Group():
        gr.Markdown("### 2ï¸âƒ£ æ–‡æœ¬ç”Ÿæˆ")

        with gr.Row():
            with gr.Column(scale=3):
                input_box = gr.Textbox(
                    label="è¾“å…¥ Prompt",
                    placeholder="ä¾‹å¦‚: The future of AI is",
                    lines=3
                )

            with gr.Column(scale=2):
                gr.Markdown("**ç”Ÿæˆå‚æ•°**")
                max_length_slider = gr.Slider(
                    minimum=1,
                    maximum=100,
                    value=20,
                    step=1,
                    label="æœ€å¤§ç”Ÿæˆé•¿åº¦ (tokens)"
                )
                temperature_slider = gr.Slider(
                    minimum=0.1,
                    maximum=2.0,
                    value=1.0,
                    step=0.1,
                    label="Temperature (åˆ›é€ æ€§)"
                )
                top_k_slider = gr.Slider(
                    minimum=0,
                    maximum=100,
                    value=50,
                    step=1,
                    label="Top-K (0=è´ªå©ªé‡‡æ ·)"
                )
                show_speed_check = gr.Checkbox(
                    label="æ˜¾ç¤ºç”Ÿæˆé€Ÿåº¦",
                    value=True
                )

        with gr.Row():
            generate_btn = gr.Button("â–¶ï¸ å¼€å§‹ç”Ÿæˆ", variant="primary", size="lg")
            stop_btn = gr.Button("â¹ï¸ åœæ­¢ç”Ÿæˆ", variant="stop", size="lg")

        with gr.Row():
            with gr.Column(scale=3):
                output_box = gr.Textbox(
                    label="ç”Ÿæˆç»“æœ",
                    lines=8,
                    show_copy_button=True
                )
            with gr.Column(scale=1):
                stats_box = gr.Textbox(
                    label="ç”Ÿæˆç»Ÿè®¡",
                    lines=8,
                    elem_classes=["stats-box"]
                )

    # ç¤ºä¾‹
    gr.Markdown("### ğŸ’¡ ç¤ºä¾‹ Prompts")
    gr.Examples(
        examples=[
            ["The future of AI is"],
            ["Once upon a time, in a land far away,"],
            ["To be or not to be,"],
            ["In the beginning, there was"],
            ["The quick brown fox"],
        ],
        inputs=input_box,
        label="ç‚¹å‡»ä½¿ç”¨ç¤ºä¾‹"
    )

    # ä½¿ç”¨è¯´æ˜
    with gr.Accordion("ğŸ“– ä½¿ç”¨è¯´æ˜", open=False):
        gr.Markdown(
            """
            1. **åˆå§‹åŒ–**: ç‚¹å‡»"åˆå§‹åŒ–æ¨¡å‹å¹¶è¿æ¥æœåŠ¡å™¨"æŒ‰é’®
               - é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½ GPT-2 æ¨¡å‹ï¼ˆçº¦ 500MBï¼‰
               - åç»­è¿è¡Œä½¿ç”¨ç¼“å­˜ï¼Œå¯åŠ¨æ›´å¿«

            2. **ç”Ÿæˆæ–‡æœ¬**:
               - è¾“å…¥ä¸€ä¸ª prompt
               - è°ƒæ•´ç”Ÿæˆå‚æ•°ï¼ˆå¯é€‰ï¼‰
               - ç‚¹å‡»"å¼€å§‹ç”Ÿæˆ"

            3. **åœæ­¢ç”Ÿæˆ**: ç‚¹å‡»"åœæ­¢ç”Ÿæˆ"æŒ‰é’®å¯éšæ—¶ä¸­æ­¢

            4. **å‚æ•°è¯´æ˜**:
               - **æœ€å¤§é•¿åº¦**: ç”Ÿæˆçš„ token æ•°é‡
               - **Temperature**: æ§åˆ¶éšæœºæ€§ï¼ˆè¶Šé«˜è¶Šéšæœºï¼‰
               - **Top-K**: é™åˆ¶é‡‡æ ·èŒƒå›´ï¼ˆ0=è´ªå©ªé‡‡æ ·ï¼‰

            5. **æœåŠ¡å™¨è¦æ±‚**:
               - ç¡®ä¿è¿è¡Œäº† `python testcode/start_server.py`
               - æœåŠ¡å™¨åœ°å€: localhost:50053
            """
        )

    # äº‹ä»¶ç»‘å®š
    init_btn.click(
        fn=load_models,
        inputs=[],
        outputs=status_box
    )

    generate_btn.click(
        fn=generate_text,
        inputs=[
            input_box,
            max_length_slider,
            temperature_slider,
            top_k_slider,
            show_speed_check
        ],
        outputs=[output_box, stats_box],
        show_progress="full",  # æ˜¾ç¤ºè¿›åº¦
        api_name="generate"    # å…è®¸ API è°ƒç”¨
    )

    stop_btn.click(
        fn=stop_generation_fn,
        inputs=[],
        outputs=stats_box
    )

if __name__ == "__main__":
    # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
    share = os.environ.get("GRADIO_SHARE", "false").lower() == "true"
    server_port = int(os.environ.get("GRADIO_PORT", "7861"))

    print("=" * 70)
    print("Split Learning Gradio å®¢æˆ·ç«¯")
    print("=" * 70)
    print(f"å¯åŠ¨åœ°å€: http://127.0.0.1:{server_port}")
    print(f"Share æ¨¡å¼: {'å¯ç”¨' if share else 'ç¦ç”¨'}")
    print("=" * 70)

    demo.queue()  # å¯ç”¨é˜Ÿåˆ—ä»¥æ”¯æŒæµå¼è¾“å‡º
    demo.launch(
        server_name="0.0.0.0",  # ç›‘å¬æ‰€æœ‰ç½‘ç»œæ¥å£
        server_port=7870,
        share=False,  # ç¦ç”¨å…¬å…±é“¾æ¥ï¼Œåªä½¿ç”¨æœ¬åœ°è®¿é—®
        show_error=True,
        inbrowser=True  # è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
    )
