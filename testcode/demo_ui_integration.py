"""
ä¸€é”®æ¼”ç¤ºï¼šé›†æˆçš„ Gradio UI åŠŸèƒ½

è¿™ä¸ªè„šæœ¬å±•ç¤ºå¦‚ä½•ä½¿ç”¨é›†æˆåˆ° splitlearn-comm çš„ UI åŠŸèƒ½ã€‚
å®ƒä¼šåœ¨å•ä¸ªè¿›ç¨‹ä¸­å¯åŠ¨æœåŠ¡å™¨å’Œå®¢æˆ·ç«¯UIï¼ˆåˆ†åˆ«åœ¨ä¸åŒçš„çº¿ç¨‹ä¸­ï¼‰ã€‚
"""
import sys
import os
import time
import threading

current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, os.path.join(project_root, 'SplitLearning', 'src'))
sys.path.insert(0, os.path.join(project_root, 'splitlearn-comm', 'src'))
sys.path.insert(0, os.path.join(project_root, 'splitlearn-manager', 'src'))

import torch
from transformers import AutoTokenizer
from splitlearn_comm import GRPCComputeClient
from splitlearn_manager import ManagedServer, ServerConfig, ModelConfig


def start_server():
    """åœ¨åå°çº¿ç¨‹å¯åŠ¨æœåŠ¡å™¨å’Œç›‘æ§UI"""
    print("\n[æœåŠ¡å™¨çº¿ç¨‹] æ­£åœ¨å¯åŠ¨...")

    # æ£€æŸ¥æ¨¡å‹
    trunk_path = os.path.join(current_dir, "gpt2_trunk_full.pt")
    if not os.path.exists(trunk_path):
        print(f"[æœåŠ¡å™¨çº¿ç¨‹] âŒ Trunk æ¨¡å‹ä¸å­˜åœ¨: {trunk_path}")
        return

    # é…ç½®æœåŠ¡å™¨
    server_config = ServerConfig(
        host="localhost",
        port=50053,
        metrics_port=8002,
        log_level="WARNING"  # å‡å°‘æ—¥å¿—è¾“å‡º
    )

    server = ManagedServer(config=server_config)

    # åŠ è½½æ¨¡å‹
    print("[æœåŠ¡å™¨çº¿ç¨‹] åŠ è½½ Trunk æ¨¡å‹...")
    model_config = ModelConfig(
        model_id="gpt2-trunk",
        model_path=trunk_path,
        model_type="pytorch",
        device="cpu",
        warmup=False,
        config={"input_shape": (1, 10, 768)}
    )

    server.load_model(model_config)
    print("[æœåŠ¡å™¨çº¿ç¨‹] âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")

    # å¯åŠ¨æœåŠ¡å™¨
    server.start()
    print("[æœåŠ¡å™¨çº¿ç¨‹] âœ“ gRPC æœåŠ¡å™¨å·²å¯åŠ¨ (localhost:50053)")

    # ç­‰å¾…æœåŠ¡å™¨å®Œå…¨å¯åŠ¨
    time.sleep(2)

    # å¯åŠ¨ç›‘æ§ UIï¼ˆåœ¨åå°ï¼‰
    print("[æœåŠ¡å™¨çº¿ç¨‹] å¯åŠ¨ç›‘æ§ UI...")
    grpc_server = server.grpc_server

    try:
        grpc_server.launch_monitoring_ui(
            theme="default",
            refresh_interval=2,
            share=False,
            server_port=7861,
            blocking=False  # åœ¨åå°è¿è¡Œ
        )
        print("[æœåŠ¡å™¨çº¿ç¨‹] âœ“ ç›‘æ§ UI å·²å¯åŠ¨ (http://127.0.0.1:7861)")
    except ImportError as e:
        print(f"[æœåŠ¡å™¨çº¿ç¨‹] âš ï¸  ç›‘æ§ UI æœªå¯åŠ¨: {e}")
        print("[æœåŠ¡å™¨çº¿ç¨‹] å®‰è£…ä¾èµ–: pip install gradio pandas")

    # ä¿æŒæœåŠ¡å™¨è¿è¡Œ
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("\n[æœåŠ¡å™¨çº¿ç¨‹] æ­£åœ¨å…³é—­...")
    finally:
        server.stop()
        print("[æœåŠ¡å™¨çº¿ç¨‹] âœ“ æœåŠ¡å™¨å·²åœæ­¢")


def start_client():
    """åœ¨ä¸»çº¿ç¨‹å¯åŠ¨å®¢æˆ·ç«¯UI"""
    print("\n[å®¢æˆ·ç«¯çº¿ç¨‹] æ­£åœ¨å¯åŠ¨...")

    # æ£€æŸ¥æ¨¡å‹
    bottom_path = os.path.join(current_dir, "gpt2_bottom_cached.pt")
    top_path = os.path.join(current_dir, "gpt2_top_cached.pt")

    if not os.path.exists(bottom_path) or not os.path.exists(top_path):
        print("[å®¢æˆ·ç«¯çº¿ç¨‹] âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼")
        print(f"   Bottom: {bottom_path}")
        print(f"   Top: {top_path}")
        print("\nè¯·è¿è¡Œ: python testcode/prepare_models.py")
        return

    # åŠ è½½æ¨¡å‹
    print("[å®¢æˆ·ç«¯çº¿ç¨‹] åŠ è½½ Bottom å’Œ Top æ¨¡å‹...")
    bottom_model = torch.load(bottom_path, map_location='cpu', weights_only=False)
    top_model = torch.load(top_path, map_location='cpu', weights_only=False)
    tokenizer = AutoTokenizer.from_pretrained('gpt2')
    print("[å®¢æˆ·ç«¯çº¿ç¨‹] âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")

    # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
    print("[å®¢æˆ·ç«¯çº¿ç¨‹] ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨...")
    time.sleep(5)

    # è¿æ¥æœåŠ¡å™¨
    print("[å®¢æˆ·ç«¯çº¿ç¨‹] è¿æ¥åˆ°æœåŠ¡å™¨...")
    client = GRPCComputeClient("localhost:50053", timeout=10.0)

    max_retries = 5
    for i in range(max_retries):
        if client.connect():
            print("[å®¢æˆ·ç«¯çº¿ç¨‹] âœ“ å·²è¿æ¥åˆ°æœåŠ¡å™¨")
            break
        else:
            if i < max_retries - 1:
                print(f"[å®¢æˆ·ç«¯çº¿ç¨‹] é‡è¯•è¿æ¥ ({i+1}/{max_retries})...")
                time.sleep(2)
            else:
                print("[å®¢æˆ·ç«¯çº¿ç¨‹] âŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨")
                return

    # å¯åŠ¨å®¢æˆ·ç«¯ UI
    print("[å®¢æˆ·ç«¯çº¿ç¨‹] å¯åŠ¨å®¢æˆ·ç«¯ UI...")

    try:
        # ğŸš€ è¿™é‡Œæ˜¯å…³é”®ï¼šä½¿ç”¨é›†æˆçš„ launch_ui() æ–¹æ³•
        client.launch_ui(
            bottom_model=bottom_model,
            top_model=top_model,
            tokenizer=tokenizer,
            theme="default",
            share=False,
            server_port=7860
        )
    except ImportError as e:
        print(f"[å®¢æˆ·ç«¯çº¿ç¨‹] âŒ å¯¼å…¥é”™è¯¯: {e}")
        print("\nè¯·å®‰è£… UI ä¾èµ–:")
        print("  pip install gradio pandas")
    except KeyboardInterrupt:
        print("\n[å®¢æˆ·ç«¯çº¿ç¨‹] UI å·²åœæ­¢")
    finally:
        client.close()
        print("[å®¢æˆ·ç«¯çº¿ç¨‹] âœ“ è¿æ¥å·²å…³é—­")


def main():
    """ä¸»å‡½æ•°ï¼šå¯åŠ¨æ¼”ç¤º"""
    print("=" * 70)
    print("é›†æˆ UI åŠŸèƒ½æ¼”ç¤º")
    print("=" * 70)
    print("\nè¿™ä¸ªæ¼”ç¤ºå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨é›†æˆåˆ° splitlearn-comm çš„ Gradio UI")
    print("\nå°†å¯åŠ¨:")
    print("  1. gRPC æœåŠ¡å™¨ (localhost:50053)")
    print("  2. æœåŠ¡å™¨ç›‘æ§ UI (http://127.0.0.1:7861)")
    print("  3. å®¢æˆ·ç«¯ç”Ÿæˆ UI (http://127.0.0.1:7860)")
    print("\nğŸ¯ ä»£ç å¯¹æ¯”:")
    print("  æ—§æ–¹æ³•: 366 è¡Œä»£ç ")
    print("  æ–°æ–¹æ³•: 15 è¡Œä»£ç ")
    print("  å‡å°‘: 96%")
    print("\n" + "=" * 70)

    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    required_files = [
        ("gpt2_bottom_cached.pt", "Bottom æ¨¡å‹"),
        ("gpt2_top_cached.pt", "Top æ¨¡å‹"),
        ("gpt2_trunk_full.pt", "Trunk æ¨¡å‹"),
    ]

    print("\næ£€æŸ¥æ¨¡å‹æ–‡ä»¶...")
    all_exist = True
    for filename, desc in required_files:
        path = os.path.join(current_dir, filename)
        if os.path.exists(path):
            size_mb = os.path.getsize(path) / (1024 * 1024)
            print(f"  âœ“ {desc}: {size_mb:.1f} MB")
        else:
            print(f"  âœ— {desc}: ä¸å­˜åœ¨")
            all_exist = False

    if not all_exist:
        print("\nâŒ ç¼ºå°‘æ¨¡å‹æ–‡ä»¶")
        print("è¯·è¿è¡Œ: python testcode/prepare_models.py")
        return 1

    # å¯åŠ¨æœåŠ¡å™¨çº¿ç¨‹
    print("\nå¯åŠ¨æœåŠ¡å™¨çº¿ç¨‹...")
    server_thread = threading.Thread(target=start_server, daemon=True)
    server_thread.start()

    # åœ¨ä¸»çº¿ç¨‹å¯åŠ¨å®¢æˆ·ç«¯ UI (blocking)
    try:
        start_client()
    except KeyboardInterrupt:
        print("\n\næ­£åœ¨å…³é—­...")

    print("\n" + "=" * 70)
    print("æ¼”ç¤ºç»“æŸ")
    print("=" * 70)
    print("\nâœ¨ ä½“éªŒå¦‚ä½•ï¼Ÿ")
    print("\nä½¿ç”¨é›†æˆçš„ UI åŠŸèƒ½ï¼Œä½ å¯ä»¥:")
    print("  â€¢ ç”¨ 1 è¡Œä»£ç å¯åŠ¨å®Œæ•´çš„ UI")
    print("  â€¢ äº«å—ä¸“ä¸šçš„ç•Œé¢è®¾è®¡")
    print("  â€¢ è·å¾—å®æ—¶çš„æ€§èƒ½ç›‘æ§")
    print("  â€¢ å‡å°‘ 96% çš„æ ·æ¿ä»£ç ")
    print("\næŸ¥çœ‹æ›´å¤š:")
    print("  â€¢ æ–‡æ¡£: splitlearn-comm/examples/UI_README.md")
    print("  â€¢ ç¤ºä¾‹: splitlearn-comm/examples/")
    print()

    return 0


if __name__ == "__main__":
    exit(main())
