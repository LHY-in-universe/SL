"""
ä½¿ç”¨å¢é‡åŠ è½½åˆ›å»º Qwen2.5-3B åˆ†å‰²æ¨¡å‹
åªä¸‹è½½éœ€è¦çš„åˆ†ç‰‡ï¼Œä¸ä¸‹è½½æ•´ä¸ªæ¨¡å‹ï¼
"""
import sys
import os
import torch

current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
splitlearn_path = os.path.join(project_root, 'SplitLearning', 'src')
sys.path.insert(0, splitlearn_path)

print("=" * 70)
print("Qwen2.5-3B å¢é‡åŠ è½½ - åªä¸‹è½½éœ€è¦çš„åˆ†ç‰‡")
print("=" * 70)
print("\nä¼˜åŠ¿:")
print("  âœ“ åªä¸‹è½½å‰3å±‚å’Œå2å±‚éœ€è¦çš„æƒé‡")
print("  âœ“ ä¸ä¸‹è½½æ•´ä¸ª3.1GBæ¨¡å‹")
print("  âœ“ èŠ‚çœå¸¦å®½å’Œæ—¶é—´")
print("  âœ“ é™ä½å†…å­˜å³°å€¼")
print()

try:
    from splitlearn import ModelFactory
    
    print("ã€1ã€‘ä½¿ç”¨å¢é‡åŠ è½½åˆ›å»ºåˆ†å‰²æ¨¡å‹...")
    print("   é…ç½®:")
    print("   - æ¨¡å‹: Qwen2.5-3B (28å±‚)")
    print("   - Bottom: å‰3å±‚")
    print("   - Trunk: ä¸­é—´23å±‚")
    print("   - Top: å2å±‚")
    print("   - æ¨¡å¼: low_memory=True (å¢é‡åŠ è½½)")
    print()
    
    # å…³é”®ï¼šä½¿ç”¨ low_memory=True å¯ç”¨å¢é‡åŠ è½½
    bottom, trunk, top = ModelFactory.create_split_models(
        model_type='qwen2',
        model_name_or_path='Qwen/Qwen2.5-3B',
        split_point_1=3,
        split_point_2=26,
        device='cpu',
        low_memory=True,    # ğŸ”‘ å…³é”®å‚æ•°ï¼šåªä¸‹è½½éœ€è¦çš„åˆ†ç‰‡
        verbose=True         # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    )
    
    print("\n   âœ“ æ¨¡å‹åˆ›å»ºå®Œæˆï¼\n")
    
    print("ã€2ã€‘ä¿å­˜ä¸º .pt æ–‡ä»¶...")
    
    # ä¿å­˜è·¯å¾„
    bottom_path = os.path.join(current_dir, 'qwen25_3b_bottom_cached.pt')
    trunk_path = os.path.join(current_dir, 'qwen25_3b_trunk_cached.pt')
    top_path = os.path.join(current_dir, 'qwen25_3b_top_cached.pt')
    
    # ä¿å­˜
    torch.save(bottom.state_dict(), bottom_path)
    size_bottom = os.path.getsize(bottom_path) / (1024*1024)
    print(f"   âœ“ Bottom: {size_bottom:.1f}MB")
    
    torch.save(trunk.state_dict(), trunk_path)
    size_trunk = os.path.getsize(trunk_path) / (1024*1024)
    print(f"   âœ“ Trunk: {size_trunk:.1f}MB")
    
    torch.save(top.state_dict(), top_path)
    size_top = os.path.getsize(top_path) / (1024*1024)
    print(f"   âœ“ Top: {size_top:.1f}MB")
    
    total_size = size_bottom + size_trunk + size_top
    print(f"\n   æ€»å¤§å°: {total_size:.1f}MB")
    
    print("\nã€3ã€‘å¿«é€Ÿæµ‹è¯•...")
    from transformers import AutoTokenizer
    
    tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-3B')
    input_ids = tokenizer.encode("ä½ å¥½", return_tensors="pt")
    
    with torch.no_grad():
        h1 = bottom(input_ids)
        h2 = trunk(h1)
        output = top(h2)
    
    print(f"   âœ“ æ¨ç†æˆåŠŸ: {output.logits.shape}")
    
    print("\n" + "=" * 70)
    print("âœ… å®Œæˆï¼")
    print("=" * 70)
    
    print("\nã€ç”Ÿæˆçš„æ–‡ä»¶ã€‘")
    print(f"  {os.path.basename(bottom_path)}")
    print(f"  {os.path.basename(trunk_path)}")
    print(f"  {os.path.basename(top_path)}")
    
except KeyboardInterrupt:
    print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
    sys.exit(0)
    
except Exception as e:
    print(f"\nâœ— é”™è¯¯: {e}")
    import traceback
    traceback.print_exc()
    
    print("\nğŸ’¡ æç¤º:")
    print("  å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œï¼Œæ¨¡å‹æ–‡ä»¶éœ€è¦ä¸‹è½½")
    print("  å¢é‡åŠ è½½ä¼šæ™ºèƒ½åœ°åªä¸‹è½½éœ€è¦çš„åˆ†ç‰‡")
    print("  ä½†ä»éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
    sys.exit(1)

