"""
ç«¯åˆ°ç«¯æµ‹è¯•ï¼šå¯åŠ¨æœåŠ¡å™¨å’Œå®¢æˆ·ç«¯ï¼ŒéªŒè¯åŸºæœ¬é€šä¿¡
"""
import sys
import os
import time
import multiprocessing
import signal

current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, os.path.join(project_root, 'SplitLearning', 'src'))
sys.path.insert(0, os.path.join(project_root, 'splitlearn-comm', 'src'))
sys.path.insert(0, os.path.join(project_root, 'splitlearn-manager', 'src'))

def run_server(stop_event):
    """åœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­è¿è¡ŒæœåŠ¡å™¨"""
    try:
        import torch
        from splitlearn_manager import ManagedServer, ServerConfig, ModelConfig

        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        model_path = os.path.join(current_dir, "gpt2_trunk_full.pt")
        if not os.path.exists(model_path):
            print("âŒ Trunk æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
            return

        print("ğŸš€ å¯åŠ¨æœåŠ¡å™¨...")

        # é…ç½®æœåŠ¡å™¨
        server_config = ServerConfig(
            host="localhost",
            port=50053,
            metrics_port=8002,
            log_level="WARNING"  # å‡å°‘æ—¥å¿—è¾“å‡º
        )

        server = ManagedServer(config=server_config)

        # åŠ è½½æ¨¡å‹
        model_config = ModelConfig(
            model_id="gpt2-trunk",
            model_path=model_path,
            model_type="pytorch",
            device="cpu",
            warmup=False,  # è·³è¿‡é¢„çƒ­åŠ å¿«å¯åŠ¨
            config={"input_shape": (1, 10, 768)}
        )

        server.load_model(model_config)
        print("âœ“ æœåŠ¡å™¨æ¨¡å‹åŠ è½½å®Œæˆ")

        # å¯åŠ¨æœåŠ¡
        server.start()
        print("âœ“ æœåŠ¡å™¨å·²å¯åŠ¨ (port 50053)")

        # ç­‰å¾…åœæ­¢ä¿¡å·
        while not stop_event.is_set():
            time.sleep(0.5)

        server.stop()
        print("âœ“ æœåŠ¡å™¨å·²åœæ­¢")

    except Exception as e:
        print(f"âŒ æœåŠ¡å™¨é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

def test_client_connection():
    """æµ‹è¯•å®¢æˆ·ç«¯è¿æ¥å’ŒåŸºæœ¬åŠŸèƒ½"""
    try:
        import torch
        from splitlearn_comm import GRPCComputeClient

        print("\nğŸ”Œ æµ‹è¯•å®¢æˆ·ç«¯è¿æ¥...")

        # åŠ è½½æ¨¡å‹
        bottom_path = os.path.join(current_dir, "gpt2_bottom_cached.pt")
        top_path = os.path.join(current_dir, "gpt2_top_cached.pt")

        if not os.path.exists(bottom_path) or not os.path.exists(top_path):
            print("âŒ å®¢æˆ·ç«¯æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
            return False

        print("  åŠ è½½ Bottom æ¨¡å‹...")
        bottom_model = torch.load(bottom_path, map_location='cpu', weights_only=False)

        print("  åŠ è½½ Top æ¨¡å‹...")
        top_model = torch.load(top_path, map_location='cpu', weights_only=False)

        print("  è¿æ¥æœåŠ¡å™¨...")
        client = GRPCComputeClient("localhost:50053", timeout=5.0)

        if not client.connect():
            print("âŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨")
            return False

        print("âœ“ å®¢æˆ·ç«¯å·²è¿æ¥")

        # æµ‹è¯•æ¨ç†
        print("\nğŸ§ª æµ‹è¯•æ¨ç†æµç¨‹...")
        input_ids = torch.randint(0, 50257, (1, 5))

        print("  1. Bottom å‰å‘ä¼ æ’­...")
        with torch.no_grad():
            hidden_bottom = bottom_model(input_ids)
        print(f"     è¾“å‡ºå½¢çŠ¶: {hidden_bottom.shape}")

        print("  2. å‘é€åˆ° Trunk (è¿œç¨‹)...")
        hidden_trunk = client.compute(hidden_bottom)
        print(f"     è¾“å‡ºå½¢çŠ¶: {hidden_trunk.shape}")

        print("  3. Top å‰å‘ä¼ æ’­...")
        with torch.no_grad():
            output = top_model(hidden_trunk)
            logits = output.logits
        print(f"     è¾“å‡ºå½¢çŠ¶: {logits.shape}")

        # éªŒè¯å½¢çŠ¶
        expected_shape = (1, 5, 50257)
        if logits.shape == expected_shape:
            print(f"\nâœ… æ¨ç†æˆåŠŸï¼è¾“å‡ºå½¢çŠ¶æ­£ç¡®: {logits.shape}")
        else:
            print(f"\nâŒ è¾“å‡ºå½¢çŠ¶é”™è¯¯: æœŸæœ› {expected_shape}, å®é™… {logits.shape}")
            return False

        client.close()
        print("âœ“ å®¢æˆ·ç«¯å·²æ–­å¼€")

        return True

    except Exception as e:
        print(f"âŒ å®¢æˆ·ç«¯æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•"""
    print("=" * 70)
    print("ç«¯åˆ°ç«¯æµ‹è¯•ï¼šæœåŠ¡å™¨ + å®¢æˆ·ç«¯é€šä¿¡")
    print("=" * 70)

    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    required_files = [
        ("gpt2_bottom_cached.pt", "Bottom æ¨¡å‹"),
        ("gpt2_top_cached.pt", "Top æ¨¡å‹"),
        ("gpt2_trunk_full.pt", "Trunk æ¨¡å‹"),
    ]

    print("\næ£€æŸ¥æ¨¡å‹æ–‡ä»¶...")
    all_exist = True
    for filename, desc in required_files:
        path = os.path.join(current_dir, filename)
        if os.path.exists(path):
            size_mb = os.path.getsize(path) / (1024 * 1024)
            print(f"  âœ“ {desc}: {size_mb:.1f} MB")
        else:
            print(f"  âœ— {desc}: ä¸å­˜åœ¨")
            all_exist = False

    if not all_exist:
        print("\nâŒ ç¼ºå°‘æ¨¡å‹æ–‡ä»¶")
        print("è¯·è¿è¡Œ: python testcode/prepare_models.py")
        return 1

    # å¯åŠ¨æœåŠ¡å™¨
    print("\n" + "=" * 70)
    print("æ­¥éª¤ 1: å¯åŠ¨æœåŠ¡å™¨")
    print("=" * 70)

    multiprocessing.set_start_method('spawn', force=True)
    stop_event = multiprocessing.Event()

    server_process = multiprocessing.Process(target=run_server, args=(stop_event,))
    server_process.start()

    # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
    print("\nç­‰å¾…æœåŠ¡å™¨å¯åŠ¨...")
    time.sleep(5)

    if not server_process.is_alive():
        print("âŒ æœåŠ¡å™¨å¯åŠ¨å¤±è´¥")
        return 1

    # æµ‹è¯•å®¢æˆ·ç«¯
    print("\n" + "=" * 70)
    print("æ­¥éª¤ 2: æµ‹è¯•å®¢æˆ·ç«¯é€šä¿¡")
    print("=" * 70)

    success = test_client_connection()

    # åœæ­¢æœåŠ¡å™¨
    print("\n" + "=" * 70)
    print("æ­¥éª¤ 3: æ¸…ç†")
    print("=" * 70)
    print("åœæ­¢æœåŠ¡å™¨...")
    stop_event.set()
    server_process.join(timeout=5)

    if server_process.is_alive():
        print("âš ï¸  æœåŠ¡å™¨æœªæ­£å¸¸åœæ­¢ï¼Œå¼ºåˆ¶ç»ˆæ­¢...")
        server_process.terminate()
        server_process.join()

    # æ€»ç»“
    print("\n" + "=" * 70)
    print("æµ‹è¯•ç»“æœ")
    print("=" * 70)

    if success:
        print("âœ… ç«¯åˆ°ç«¯æµ‹è¯•é€šè¿‡ï¼")
        print("\næ‰€æœ‰ç»„ä»¶å·¥ä½œæ­£å¸¸:")
        print("  âœ“ æœåŠ¡å™¨å¯ä»¥å¯åŠ¨å’Œåœæ­¢")
        print("  âœ“ å®¢æˆ·ç«¯å¯ä»¥è¿æ¥æœåŠ¡å™¨")
        print("  âœ“ Bottom â†’ Trunk â†’ Top æ¨ç†æµç¨‹æ­£å¸¸")
        print("\nGradio åº”ç”¨å¯ä»¥æ­£å¸¸ä½¿ç”¨äº†ï¼")
        print("å¯åŠ¨å‘½ä»¤: python testcode/client_with_gradio.py")
        return 0
    else:
        print("âŒ ç«¯åˆ°ç«¯æµ‹è¯•å¤±è´¥")
        return 1

if __name__ == "__main__":
    exit(main())
