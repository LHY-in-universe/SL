"""
æµ‹è¯•å¢é‡åŠ è½½åŠŸèƒ½
ä½¿ç”¨ GPT-2 (éåˆ†ç‰‡) å’Œ Qwen2-0.5B (å¯èƒ½åˆ†ç‰‡) è¿›è¡Œæµ‹è¯•
"""
import sys
import os

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, os.path.join(project_root, 'SplitLearning', 'src'))

import torch
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("TestIncrementalLoading")

def test_imports():
    """æµ‹è¯•å¯¼å…¥"""
    logger.info("=" * 70)
    logger.info("æµ‹è¯• 1: å¯¼å…¥æ¨¡å—")
    logger.info("=" * 70)

    try:
        from splitlearn import ModelFactory
        from splitlearn.utils import ShardLoader, MemoryTracker
        logger.info("âœ… æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ")
        return True
    except Exception as e:
        logger.error(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_shard_detection():
    """æµ‹è¯•åˆ†ç‰‡æ£€æµ‹"""
    logger.info("\n" + "=" * 70)
    logger.info("æµ‹è¯• 2: åˆ†ç‰‡æ£€æµ‹")
    logger.info("=" * 70)

    try:
        from splitlearn.utils import ShardLoader

        # æµ‹è¯•éåˆ†ç‰‡æ¨¡å‹
        logger.info("æ£€æµ‹ GPT-2 (éåˆ†ç‰‡æ¨¡å‹)...")
        is_sharded = ShardLoader.is_sharded_model('gpt2')
        logger.info(f"  GPT-2 æ˜¯å¦åˆ†ç‰‡: {is_sharded}")

        if is_sharded:
            logger.warning("  âš ï¸  GPT-2 ä¸åº”è¯¥æ˜¯åˆ†ç‰‡æ¨¡å‹")
        else:
            logger.info("  âœ… æ­£ç¡®è¯†åˆ«ä¸ºéåˆ†ç‰‡æ¨¡å‹")

        # æµ‹è¯•åˆ†ç‰‡æ¨¡å‹ï¼ˆå¦‚æœç½‘ç»œå¯ç”¨ï¼‰
        logger.info("\næ£€æµ‹ Qwen2-0.5B (å¯èƒ½æ˜¯åˆ†ç‰‡æ¨¡å‹)...")
        try:
            is_sharded = ShardLoader.is_sharded_model('Qwen/Qwen2-0.5B')
            logger.info(f"  Qwen2-0.5B æ˜¯å¦åˆ†ç‰‡: {is_sharded}")
            logger.info("  âœ… æ£€æµ‹å®Œæˆ")
        except Exception as e:
            logger.warning(f"  âš ï¸  æ— æ³•æ£€æµ‹ Qwen2-0.5B (å¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜): {e}")

        return True

    except Exception as e:
        logger.error(f"âŒ åˆ†ç‰‡æ£€æµ‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_memory_tracker():
    """æµ‹è¯•å†…å­˜è¿½è¸ªå™¨"""
    logger.info("\n" + "=" * 70)
    logger.info("æµ‹è¯• 3: å†…å­˜è¿½è¸ªå™¨")
    logger.info("=" * 70)

    try:
        from splitlearn.utils import MemoryTracker

        tracker = MemoryTracker()

        # è®°å½•å¿«ç…§
        tracker.snapshot("å¼€å§‹")

        # åˆ†é…ä¸€äº›å†…å­˜
        data = torch.randn(1000, 1000)

        tracker.snapshot("åˆ†é…å†…å­˜å")

        # æŸ¥çœ‹æŠ¥å‘Š
        logger.info("\nå†…å­˜å˜åŒ–æŠ¥å‘Š:")
        tracker.report()

        # æŸ¥çœ‹æ‘˜è¦
        logger.info("\nå†…å­˜ä½¿ç”¨æ‘˜è¦:")
        tracker.summary()

        # è·å–å½“å‰ä½¿ç”¨
        usage = tracker.get_current_usage()
        logger.info(f"\nå½“å‰å†…å­˜ä½¿ç”¨:")
        logger.info(f"  RAM: {usage['ram_gb']:.2f} GB ({usage['ram_percent']:.1f}%)")
        logger.info(f"  GPU: {usage['gpu_gb']:.2f} GB")

        logger.info("\nâœ… å†…å­˜è¿½è¸ªå™¨æµ‹è¯•é€šè¿‡")
        return True

    except Exception as e:
        logger.error(f"âŒ å†…å­˜è¿½è¸ªå™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_traditional_loading():
    """æµ‹è¯•ä¼ ç»ŸåŠ è½½ï¼ˆéåˆ†ç‰‡æ¨¡å‹ï¼‰"""
    logger.info("\n" + "=" * 70)
    logger.info("æµ‹è¯• 4: ä¼ ç»ŸåŠ è½½ (GPT-2, low_memory=False)")
    logger.info("=" * 70)

    try:
        from splitlearn import ModelFactory

        logger.info("æ­£åœ¨åŠ è½½ GPT-2...")

        bottom, trunk, top = ModelFactory.create_split_models(
            model_type='gpt2',
            model_name_or_path='gpt2',
            split_point_1=2,
            split_point_2=10,
            device='cpu',
            low_memory=False,  # ä¼ ç»Ÿæ¨¡å¼
            verbose=False,
        )

        logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        logger.info(f"  Bottom: {type(bottom).__name__}")
        logger.info(f"  Trunk:  {type(trunk).__name__}")
        logger.info(f"  Top:    {type(top).__name__}")

        # æµ‹è¯•æ¨ç†
        logger.info("\næµ‹è¯•æ¨ç†...")
        vocab_size = bottom.wte.weight.shape[0]
        input_ids = torch.randint(0, vocab_size, (1, 5))

        hidden = bottom(input_ids)
        logger.info(f"  Bottom è¾“å‡º: {hidden.shape}")

        hidden = trunk(hidden)
        logger.info(f"  Trunk è¾“å‡º:  {hidden.shape}")

        output = top(hidden)
        # Top æ¨¡å‹è¿”å› CausalLMOutputWithPast å¯¹è±¡
        if hasattr(output, 'logits'):
            logger.info(f"  Top è¾“å‡º:    {output.logits.shape}")
        else:
            logger.info(f"  Top è¾“å‡º:    {output.shape}")

        logger.info("\nâœ… ä¼ ç»ŸåŠ è½½æµ‹è¯•é€šè¿‡")
        return True

    except Exception as e:
        logger.error(f"âŒ ä¼ ç»ŸåŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_incremental_loading_non_sharded():
    """æµ‹è¯•å¢é‡åŠ è½½ï¼ˆéåˆ†ç‰‡æ¨¡å‹åº”è¯¥å›é€€åˆ°ä¼ ç»ŸåŠ è½½ï¼‰"""
    logger.info("\n" + "=" * 70)
    logger.info("æµ‹è¯• 5: å¢é‡åŠ è½½éåˆ†ç‰‡æ¨¡å‹ (GPT-2, low_memory=True)")
    logger.info("=" * 70)

    try:
        from splitlearn import ModelFactory

        logger.info("æ­£åœ¨åŠ è½½ GPT-2 (low_memory=True)...")
        logger.info("æœŸæœ›ï¼šåº”è¯¥è‡ªåŠ¨å›é€€åˆ°ä¼ ç»ŸåŠ è½½\n")

        bottom, trunk, top = ModelFactory.create_split_models(
            model_type='gpt2',
            model_name_or_path='gpt2',
            split_point_1=2,
            split_point_2=10,
            device='cpu',
            low_memory=True,   # å¢é‡æ¨¡å¼
            verbose=True,      # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        )

        logger.info("\nâœ… æ¨¡å‹åŠ è½½æˆåŠŸ (åº”è¯¥ä½¿ç”¨äº†ä¼ ç»ŸåŠ è½½)")

        # æµ‹è¯•æ¨ç†
        logger.info("\næµ‹è¯•æ¨ç†...")
        vocab_size = bottom.wte.weight.shape[0]
        input_ids = torch.randint(0, vocab_size, (1, 5))

        hidden = bottom(input_ids)
        hidden = trunk(hidden)
        output = top(hidden)

        # Top æ¨¡å‹è¿”å› CausalLMOutputWithPast å¯¹è±¡
        if hasattr(output, 'logits'):
            logger.info(f"  æ¨ç†æˆåŠŸ: {output.logits.shape}")
        else:
            logger.info(f"  æ¨ç†æˆåŠŸ: {output.shape}")
        logger.info("\nâœ… å¢é‡åŠ è½½éåˆ†ç‰‡æ¨¡å‹æµ‹è¯•é€šè¿‡")
        return True

    except Exception as e:
        logger.error(f"âŒ å¢é‡åŠ è½½éåˆ†ç‰‡æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_device_map():
    """æµ‹è¯•è®¾å¤‡æ˜ å°„"""
    logger.info("\n" + "=" * 70)
    logger.info("æµ‹è¯• 6: è®¾å¤‡æ˜ å°„")
    logger.info("=" * 70)

    try:
        from splitlearn import ModelFactory

        # æµ‹è¯•æ‰‹åŠ¨è®¾å¤‡æ˜ å°„
        logger.info("æµ‹è¯•æ‰‹åŠ¨è®¾å¤‡æ˜ å°„...")
        device_map = {
            'bottom': 'cpu',
            'trunk': 'cpu',
            'top': 'cpu',
        }

        bottom, trunk, top = ModelFactory.create_split_models(
            model_type='gpt2',
            model_name_or_path='gpt2',
            split_point_1=2,
            split_point_2=10,
            device='cpu',
            device_map=device_map,
            low_memory=True,
        )

        logger.info("âœ… æ‰‹åŠ¨è®¾å¤‡æ˜ å°„æˆåŠŸ")
        logger.info(f"  Bottom è®¾å¤‡: {next(bottom.parameters()).device}")
        logger.info(f"  Trunk è®¾å¤‡:  {next(trunk.parameters()).device}")
        logger.info(f"  Top è®¾å¤‡:    {next(top.parameters()).device}")

        # æµ‹è¯• auto è®¾å¤‡æ˜ å°„
        logger.info("\næµ‹è¯• auto è®¾å¤‡æ˜ å°„...")
        bottom2, trunk2, top2 = ModelFactory.create_split_models(
            model_type='gpt2',
            model_name_or_path='gpt2',
            split_point_1=2,
            split_point_2=10,
            device='cpu',
            device_map='auto',
            low_memory=True,
        )

        logger.info("âœ… Auto è®¾å¤‡æ˜ å°„æˆåŠŸ")
        logger.info(f"  Bottom è®¾å¤‡: {next(bottom2.parameters()).device}")
        logger.info(f"  Trunk è®¾å¤‡:  {next(trunk2.parameters()).device}")
        logger.info(f"  Top è®¾å¤‡:    {next(top2.parameters()).device}")

        logger.info("\nâœ… è®¾å¤‡æ˜ å°„æµ‹è¯•é€šè¿‡")
        return True

    except Exception as e:
        logger.error(f"âŒ è®¾å¤‡æ˜ å°„æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    logger.info("\n" + "=" * 70)
    logger.info("å¼€å§‹æµ‹è¯•å¢é‡åŠ è½½åŠŸèƒ½")
    logger.info("=" * 70)

    results = []

    # è¿è¡Œæµ‹è¯•
    results.append(("å¯¼å…¥æ¨¡å—", test_imports()))

    if results[-1][1]:  # å¦‚æœå¯¼å…¥æˆåŠŸï¼Œç»§ç»­å…¶ä»–æµ‹è¯•
        results.append(("åˆ†ç‰‡æ£€æµ‹", test_shard_detection()))
        results.append(("å†…å­˜è¿½è¸ªå™¨", test_memory_tracker()))
        results.append(("ä¼ ç»ŸåŠ è½½", test_traditional_loading()))
        results.append(("å¢é‡åŠ è½½éåˆ†ç‰‡", test_incremental_loading_non_sharded()))
        results.append(("è®¾å¤‡æ˜ å°„", test_device_map()))

    # æ˜¾ç¤ºç»“æœæ‘˜è¦
    logger.info("\n" + "=" * 70)
    logger.info("æµ‹è¯•ç»“æœæ‘˜è¦")
    logger.info("=" * 70)

    for test_name, passed in results:
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        logger.info(f"{test_name:20s}: {status}")

    total = len(results)
    passed = sum(1 for _, p in results if p)

    logger.info("\n" + "=" * 70)
    logger.info(f"æ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    logger.info("=" * 70)

    if passed == total:
        logger.info("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¢é‡åŠ è½½åŠŸèƒ½æ­£å¸¸å·¥ä½œã€‚")
        return 0
    else:
        logger.error(f"\nâš ï¸  æœ‰ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
        return 1

if __name__ == "__main__":
    exit(main())
