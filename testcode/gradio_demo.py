"""
Split Learning Gradio æ¼”ç¤º (ä½¿ç”¨ Gradio 3.x)
"""
import sys
import os
import torch
from transformers import AutoTokenizer
import gradio as gr

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.append(os.path.join(project_root, 'SplitLearning', 'src'))
sys.path.append(os.path.join(project_root, 'splitlearn-comm', 'src'))

from splitlearn_comm import GRPCComputeClient

# å…¨å±€å˜é‡
bottom_model = None
top_model = None
tokenizer = None
client = None

def load_models():
    """åŠ è½½æœ¬åœ°æ¨¡å‹å’Œè¿æ¥æœåŠ¡å™¨"""
    global bottom_model, top_model, tokenizer, client
    
    try:
        # åŠ è½½æ¨¡å‹
        bottom_path = os.path.join(current_dir, "gpt2_bottom_cached.pt")
        top_path = os.path.join(current_dir, "gpt2_top_cached.pt")
        
        if not os.path.exists(bottom_path) or not os.path.exists(top_path):
            return "âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼\nè¯·å…ˆè¿è¡Œ: python testcode/prepare_models.py"
        
        bottom_model = torch.load(bottom_path, map_location='cpu')
        top_model = torch.load(top_path, map_location='cpu')
        tokenizer = AutoTokenizer.from_pretrained('gpt2')
        
        # è¿æ¥æœåŠ¡å™¨
        client = GRPCComputeClient("127.0.0.1:50053", timeout=20.0)
        if not client.connect():
            return "âŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ï¼\nè¯·ç¡®ä¿æœåŠ¡å™¨æ­£åœ¨è¿è¡Œ:\npython testcode/start_server.py"
            
        return "âœ… åˆå§‹åŒ–æˆåŠŸï¼\n\n- Bottom æ¨¡å‹å·²åŠ è½½\n- Top æ¨¡å‹å·²åŠ è½½\n- æœåŠ¡å™¨å·²è¿æ¥ (127.0.0.1:50053)\n\nç°åœ¨å¯ä»¥å¼€å§‹ç”Ÿæˆæ–‡æœ¬äº†ï¼"
    except Exception as e:
        import traceback
        return f"âŒ åˆå§‹åŒ–å¤±è´¥:\n{str(e)}\n\n{traceback.format_exc()}"

def generate_text(prompt, max_length=20):
    """ç”Ÿæˆæ–‡æœ¬"""
    global bottom_model, top_model, tokenizer, client
    
    if client is None:
        return "è¯·å…ˆç‚¹å‡»'åˆå§‹åŒ–'æŒ‰é’®ï¼"
    
    try:
        input_ids = tokenizer.encode(prompt, return_tensors="pt")
        generated_text = prompt
        
        for _ in range(max_length):
            # Bottom (æœ¬åœ°)
            with torch.no_grad():
                hidden_bottom = bottom_model(input_ids)
            
            # Trunk (è¿œç¨‹)
            hidden_trunk = client.compute(hidden_bottom, model_id="gpt2-trunk")
            
            # Top (æœ¬åœ°)
            with torch.no_grad():
                output = top_model(hidden_trunk)
                logits = output.logits
            
            # é‡‡æ ·
            next_token_id = logits[:, -1, :].argmax(dim=-1).unsqueeze(-1)
            input_ids = torch.cat([input_ids, next_token_id], dim=-1)
            
            generated_text += tokenizer.decode(next_token_id[0])
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = client.get_statistics()
        stats_text = f"\n\nç»Ÿè®¡ä¿¡æ¯:\n- æ€»è¯·æ±‚: {stats['total_requests']}\n- å¹³å‡å»¶è¿Ÿ: {stats['avg_network_time_ms']:.2f}ms\n- å¹³å‡è®¡ç®—: {stats['avg_compute_time_ms']:.2f}ms"
        
        return generated_text + stats_text
        
    except Exception as e:
        return f"âŒ ç”Ÿæˆå¤±è´¥:\n{str(e)}"

def get_system_status():
    """è·å–ç³»ç»ŸçŠ¶æ€ï¼ˆæœåŠ¡å™¨èµ„æº + å®¢æˆ·ç«¯ç»Ÿè®¡ï¼‰"""
    global client
    
    if client is None:
        return "è¯·å…ˆåˆå§‹åŒ–æ¨¡å‹"
        
    try:
        # 1. è·å–æœåŠ¡å™¨ä¿¡æ¯
        server_info = client.get_service_info()
        if not server_info:
            server_status = "æ— æ³•è·å–æœåŠ¡å™¨ä¿¡æ¯"
        else:
            custom = server_info.get("custom_info", {})
            
            # å¤„ç†åµŒå¥—çš„å­—ç¬¦ä¸²å­—å…¸
            import ast
            if "custom_info" in custom and isinstance(custom["custom_info"], str):
                try:
                    nested_custom = ast.literal_eval(custom["custom_info"])
                    if isinstance(nested_custom, dict):
                        custom = nested_custom
                except:
                    pass
            
            cpu = custom.get("cpu_percent", "N/A")
            mem = custom.get("memory_mb", "N/A")
            mem_pct = custom.get("memory_percent", "N/A")
            reqs = server_info.get("total_requests", 0)
            uptime = server_info.get("uptime_seconds", 0)
            
            server_status = (
                f"ğŸŒ æœåŠ¡å™¨çŠ¶æ€ (Trunk)\n"
                f"-------------------\n"
                f"CPU ä½¿ç”¨ç‡: {cpu}%\n"
                f"å†…å­˜ä½¿ç”¨: {mem} MB ({mem_pct}%)\n"
                f"æ€»å¤„ç†è¯·æ±‚: {reqs}\n"
                f"è¿è¡Œæ—¶é—´: {int(uptime)}ç§’"
            )

        # 2. è·å–å®¢æˆ·ç«¯ç»Ÿè®¡
        stats = client.get_statistics()
        client_status = (
            f"ğŸš€ å®¢æˆ·ç«¯æ€§èƒ½\n"
            f"-------------------\n"
            f"æœ¬åœ°å·²å‘è¯·æ±‚: {stats.get('total_requests', 0)}\n"
            f"å¹³å‡ç½‘ç»œå»¶è¿Ÿ: {stats.get('avg_network_time_ms', 0):.2f} ms\n"
            f"å¹³å‡è®¡ç®—è€—æ—¶: {stats.get('avg_compute_time_ms', 0):.2f} ms"
        )
        
        return server_status + "\n\n" + client_status
        
    except Exception as e:
        return f"è·å–çŠ¶æ€å¤±è´¥: {str(e)}"

# åˆ›å»º Gradio ç•Œé¢
with gr.Blocks(title="Split Learning Demo") as demo:
    gr.Markdown("# ğŸš€ Split Learning åˆ†å¸ƒå¼æ¨ç†æ¼”ç¤º")
    
    with gr.Row():
        with gr.Column(scale=2):
            gr.Markdown("**æ¶æ„**: Bottom(æœ¬åœ°) â†’ Trunk(è¿œç¨‹æœåŠ¡å™¨) â†’ Top(æœ¬åœ°)")
            with gr.Row():
                init_btn = gr.Button("åˆå§‹åŒ–æ¨¡å‹å¹¶è¿æ¥æœåŠ¡å™¨", variant="primary")
            
            status_box = gr.Textbox(label="åˆå§‹åŒ–çŠ¶æ€", value="æœªåˆå§‹åŒ–", lines=3)
            
            gr.Markdown("---")
            
            with gr.Row():
                prompt_box = gr.Textbox(
                    label="è¾“å…¥ Prompt", 
                    placeholder="ä¾‹å¦‚: The future of AI is...",
                    value="The future of AI is"
                )
            
            with gr.Row():
                max_length = gr.Slider(
                    minimum=5, 
                    maximum=50, 
                    value=20, 
                    step=1, 
                    label="ç”Ÿæˆé•¿åº¦ (tokens)"
                )
            
            generate_btn = gr.Button("å¼€å§‹ç”Ÿæˆ", variant="primary")
            output_box = gr.Textbox(label="ç”Ÿæˆç»“æœ", lines=8)

        with gr.Column(scale=1):
            gr.Markdown("### ğŸ“Š å®æ—¶ç›‘æ§")
            monitor_box = gr.Textbox(label="ç³»ç»ŸçŠ¶æ€", lines=15, value="ç­‰å¾…è¿æ¥...")
            refresh_btn = gr.Button("åˆ·æ–°çŠ¶æ€")

    # äº‹ä»¶ç»‘å®š
    init_btn.click(fn=load_models, outputs=status_box)
    generate_btn.click(fn=generate_text, inputs=[prompt_box, max_length], outputs=output_box)
    
    # ç›‘æ§åˆ·æ–°
    refresh_btn.click(fn=get_system_status, outputs=monitor_box)
    # è‡ªåŠ¨åˆ·æ–° (æ¯ 2 ç§’) - æ³¨æ„ï¼šGradio 3.x ä½¿ç”¨ every å‚æ•°
    demo.load(fn=get_system_status, inputs=None, outputs=monitor_box, every=2.0)

if __name__ == "__main__":
    print("=" * 70)
    print("Split Learning Gradio å®¢æˆ·ç«¯")
    print("=" * 70)
    print("è¯·åœ¨æµè§ˆå™¨ä¸­è®¿é—®: http://127.0.0.1:7788")
    print("=" * 70)
    demo.queue()  # æ˜¾å¼å¯ç”¨é˜Ÿåˆ—
    demo.launch(
        server_name="127.0.0.1",
        server_port=7788,
        share=False,
        inbrowser=True
    )
