# 模型位置说明

## 模型在哪里？

### 1. HuggingFace 原始模型缓存

**位置**: `~/.cache/huggingface/hub/models--gpt2/`

**内容**:
```
~/.cache/huggingface/hub/models--gpt2/
├── blobs/
│   └── 248dfc...707  (523 MB)  ← 完整的 GPT-2 模型权重
└── snapshots/
    └── 607a30d.../
        ├── config.json
        ├── model.safetensors → ../../blobs/...
        ├── tokenizer.json
        └── vocab.json
```

**说明**:
- 这是 HuggingFace Transformers 下载的**完整**、**未拆分**的 GPT-2 模型
- 大小: 约 523 MB
- 用途: 作为拆分的源模型

---

### 2. 拆分后的模型（本地保存）

**位置**: `./models/` (项目根目录下)

**目录结构**:
```
./models/
├── bottom/
│   ├── gpt2_2-10_bottom.pt              (204 MB)  ← Bottom 模型权重
│   └── gpt2_2-10_bottom_metadata.json   (2.6 KB)  ← 配置信息
├── trunk/
│   ├── gpt2_2-10_trunk.pt               (216 MB)  ← Trunk 模型权重
│   └── gpt2_2-10_trunk_metadata.json    (2.6 KB)
└── top/
    ├── gpt2_2-10_top.pt                 (201 MB)  ← Top 模型权重
    └── gpt2_2-10_top_metadata.json      (2.6 KB)
```

**说明**:
- 这是拆分后的三个模型
- 总大小: 约 622 MB (比原始模型大，因为有重复的嵌入层和归一化层)
- 拆分配置: `[2, 10]` 表示:
  - **Bottom**: 层 0-1 (2层) + 嵌入层
  - **Trunk**: 层 2-9 (8层)
  - **Top**: 层 10-11 (2层) + LM Head

---

## 为什么保存的模型比原始模型大？

| 组件 | 参数量 | 原因 |
|------|--------|------|
| Bottom | 53.56M | 包含完整的嵌入层 (词嵌入 + 位置嵌入) |
| Trunk | 56.70M | 只包含中间的 Transformer 层 |
| Top | 52.77M | 包含完整的 LM Head (词汇表映射层) |
| **总计** | **163M** | 嵌入层和 LM Head 被重复保存 |
| 原始模型 | 124M | 共享嵌入层和 LM Head |

这是**正常现象**，因为：
- Bottom 需要嵌入层来处理输入 token
- Top 需要 LM Head 来生成输出概率
- 这种设计允许每个部分**独立运行**

---

## 如何使用这些模型？

### 方法 1: 从零开始加载和拆分 (首次使用)

```python
from splitlearn_core import ModelFactory

# 会从 HuggingFace 下载并拆分
bottom, trunk, top = ModelFactory.create_split_models(
    model_type="gpt2",
    model_name_or_path="gpt2",
    split_point_1=2,
    split_point_2=10,
    device="cpu"
)

# 保存到本地
bottom.save_split_model("./models/bottom/gpt2_2-10_bottom.pt")
trunk.save_split_model("./models/trunk/gpt2_2-10_trunk.pt")
top.save_split_model("./models/top/gpt2_2-10_top.pt")
```

**耗时**: 约 6-7 秒 (已缓存) 或 几分钟 (首次下载)

---

### 方法 2: 从本地加载已保存的模型 (快速)

```python
import torch
from splitlearn_core.models.gpt2 import GPT2BottomModel, GPT2TrunkModel, GPT2TopModel
from transformers import AutoConfig

# 加载配置
config = AutoConfig.from_pretrained("gpt2")

# 创建模型实例并加载权重
bottom = GPT2BottomModel(config, end_layer=2)
bottom.load_state_dict(torch.load("./models/bottom/gpt2_2-10_bottom.pt"))
bottom.eval()

trunk = GPT2TrunkModel(config, start_layer=2, end_layer=10)
trunk.load_state_dict(torch.load("./models/trunk/gpt2_2-10_trunk.pt"))
trunk.eval()

top = GPT2TopModel(config, start_layer=10)
top.load_state_dict(torch.load("./models/top/gpt2_2-10_top.pt"))
top.eval()
```

**耗时**: 约 4-5 秒 (比从零开始快 30-40%)

---

## 测试脚本

我们提供了三个测试脚本：

### 1. `test_splitlearn_core_only.py`
- **功能**: 测试模型加载和推理
- **用途**: 验证 SplitLearnCore 功能是否正常
- **运行**: `python test_splitlearn_core_only.py`

### 2. `save_split_models.py`
- **功能**: 保存拆分后的模型到磁盘
- **用途**: 首次拆分模型并保存
- **运行**: `python save_split_models.py`

### 3. `load_saved_models.py`
- **功能**: 从磁盘加载已保存的模型
- **用途**: 快速加载和测试保存的模型
- **运行**: `python load_saved_models.py`

---

## 文件清单

### 完整模型 (HuggingFace 缓存)
```
~/.cache/huggingface/hub/models--gpt2/  (536 MB)
```

### 拆分模型 (本地)
```
./models/bottom/gpt2_2-10_bottom.pt              204 MB
./models/bottom/gpt2_2-10_bottom_metadata.json   2.6 KB
./models/trunk/gpt2_2-10_trunk.pt                216 MB
./models/trunk/gpt2_2-10_trunk_metadata.json     2.6 KB
./models/top/gpt2_2-10_top.pt                    201 MB
./models/top/gpt2_2-10_top_metadata.json         2.6 KB
───────────────────────────────────────────────────────
总计                                             622 MB
```

---

## 常见问题

### Q1: 为什么我没有看到模型？
**A**: 如果你只运行了 `test_splitlearn_core_only.py`，模型只在内存中，程序结束后就消失了。需要运行 `save_split_models.py` 来保存到磁盘。

### Q2: 可以删除 HuggingFace 缓存中的模型吗？
**A**: 如果你已经保存了拆分后的模型到 `./models/`，理论上可以删除。但建议保留，因为：
- 可能需要用其他拆分点重新拆分
- 其他项目可能需要完整模型
- 占用空间不大 (523 MB)

### Q3: 如何部署到生产环境？
**A**:
1. 在开发环境运行 `save_split_models.py` 保存模型
2. 将 `./models/trunk/` 目录复制到服务器
3. 服务器只加载 Trunk 模型（无需下载完整模型）
4. 客户端加载 Bottom 和 Top 模型

### Q4: 可以修改拆分点吗？
**A**: 可以！修改 `split_point_1` 和 `split_point_2` 参数：
```python
# 例如: Bottom(0-4), Trunk(4-8), Top(8-12)
bottom, trunk, top = ModelFactory.create_split_models(
    model_type="gpt2",
    model_name_or_path="gpt2",
    split_point_1=4,   # ← 修改这里
    split_point_2=8,   # ← 修改这里
    device="cpu"
)
```

---

## 推荐的工作流程

### 开发阶段
1. 运行 `test_splitlearn_core_only.py` 验证功能
2. 运行 `save_split_models.py` 保存模型
3. 运行 `load_saved_models.py` 验证保存的模型

### 部署阶段
1. 将 `./models/` 目录打包
2. 部署到目标服务器
3. 使用 `load_saved_models.py` 中的方法加载模型
4. 集成到你的应用中

---

## 总结

- **原始模型**: HuggingFace 缓存 (`~/.cache/huggingface/`)
- **拆分模型**: 本地目录 (`./models/`)
- **快速开始**: 运行 `save_split_models.py` → `load_saved_models.py`
- **生产部署**: 只需要 `./models/trunk/` 目录 (服务器端)
