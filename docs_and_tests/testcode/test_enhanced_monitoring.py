"""
æµ‹è¯•å¢å¼ºçš„ç›‘æ§åŠŸèƒ½

è¿™ä¸ªè„šæœ¬æ¼”ç¤ºäº†æ–°å¢çš„ç›‘æ§åŠŸèƒ½ï¼š
1. è¯¦ç»†çš„å»¶è¿Ÿç»Ÿè®¡ï¼ˆP95/P99ç­‰ï¼‰
2. å®æ—¶æ—¥å¿—æ˜¾ç¤º
3. å»¶è¿Ÿåˆ†å¸ƒå’Œè¶‹åŠ¿å›¾
"""

import os
import sys
import torch
import time

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, os.path.join(project_root, 'SplitLearnComm', 'src'))

from splitlearn_comm.core import ModelComputeFunction
from splitlearn_comm.server import ComputeServicer
from splitlearn_comm.ui import ServerMonitoringUI


def main():
    print("=== æµ‹è¯•å¢å¼ºçš„ç›‘æ§åŠŸèƒ½ ===\n")

    # 1. åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•æ¨¡å‹
    print("1. åˆ›å»ºæµ‹è¯•æ¨¡å‹...")
    test_model = torch.nn.Sequential(
        torch.nn.Linear(10, 20),
        torch.nn.ReLU(),
        torch.nn.Linear(20, 10)
    )
    test_model.eval()
    print("   âœ“ æ¨¡å‹åˆ›å»ºå®Œæˆ\n")

    # 2. åˆ›å»º ComputeFunction
    print("2. åˆ›å»º ComputeFunction...")
    compute_fn = ModelComputeFunction(model=test_model, device="cpu")
    print("   âœ“ ComputeFunction åˆ›å»ºå®Œæˆ\n")

    # 3. åˆ›å»º Servicerï¼ˆä¼šè‡ªåŠ¨åˆå§‹åŒ–ç›‘æ§ç®¡ç†å™¨ï¼‰
    print("3. åˆ›å»º ComputeServicerï¼ˆé›†æˆç›‘æ§ï¼‰...")
    servicer = ComputeServicer(compute_fn, history_size=100)
    print("   âœ“ Servicer åˆ›å»ºå®Œæˆ")
    print("   âœ“ MetricsManager å·²åˆå§‹åŒ–")
    print("   âœ“ LogManager å·²åˆå§‹åŒ–\n")

    # 4. æ¨¡æ‹Ÿä¸€äº›è¯·æ±‚æ¥ç”Ÿæˆç›‘æ§æ•°æ®
    print("4. æ¨¡æ‹Ÿè¯·æ±‚ç”Ÿæˆç›‘æ§æ•°æ®...")
    print("   ç”Ÿæˆ 20 ä¸ªè¯·æ±‚...")

    for i in range(20):
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        test_input = torch.randn(1, 10)

        # ç¼–ç 
        encoded_data, shape = servicer.codec.encode(test_input)

        # æ¨¡æ‹Ÿ gRPC è¯·æ±‚å¯¹è±¡
        class MockRequest:
            def __init__(self, data, shape):
                self.data = data
                self.shape = shape

            def HasField(self, field):
                return False

        class MockContext:
            def set_code(self, code):
                pass

            def set_details(self, details):
                pass

        request = MockRequest(encoded_data, shape)
        context = MockContext()

        # è°ƒç”¨ Computeï¼ˆä¼šè‡ªåŠ¨è®°å½•å»¶è¿Ÿå’Œæ—¥å¿—ï¼‰
        response = servicer.Compute(request, context)

        # éšæœºå»¶è¿Ÿæ¨¡æ‹Ÿä¸åŒçš„è¯·æ±‚æ—¶é—´
        time.sleep(0.1)

        if (i + 1) % 5 == 0:
            print(f"   å·²å®Œæˆ {i + 1} ä¸ªè¯·æ±‚")

    print("   âœ“ 20 ä¸ªè¯·æ±‚å®Œæˆ\n")

    # 5. æµ‹è¯• get_metrics() æ–¹æ³•
    print("5. æµ‹è¯•å¢å¼ºçš„ get_metrics() æ–¹æ³•...")
    metrics = servicer.get_metrics()

    print(f"   æ€»è¯·æ±‚æ•°: {metrics['total_requests']}")
    print(f"   æˆåŠŸç‡: {metrics['success_rate']*100:.1f}%")

    latency_stats = metrics['latency_stats']
    print(f"\n   å»¶è¿Ÿç»Ÿè®¡:")
    print(f"   - å¹³å‡: {latency_stats['mean']:.2f} ms")
    print(f"   - P50 (ä¸­ä½æ•°): {latency_stats['p50']:.2f} ms")
    print(f"   - P95: {latency_stats['p95']:.2f} ms")
    print(f"   - P99: {latency_stats['p99']:.2f} ms")
    print(f"   - æœ€å°: {latency_stats['min']:.2f} ms")
    print(f"   - æœ€å¤§: {latency_stats['max']:.2f} ms")

    print(f"\n   ååé‡: {metrics['current_rps']:.2f} RPS")
    print("   âœ“ get_metrics() æµ‹è¯•å®Œæˆ\n")

    # 6. æµ‹è¯• get_logs() æ–¹æ³•
    print("6. æµ‹è¯• get_logs() æ–¹æ³•...")
    logs = servicer.get_logs(level_filter="INFO", limit=5)
    print(f"   è·å–åˆ° {len(logs)} æ¡ INFO çº§åˆ«æ—¥å¿—")
    if logs:
        print("   æœ€æ–°çš„ä¸€æ¡æ—¥å¿—:")
        latest = logs[0]
        print(f"   - æ—¶é—´: {latest['timestamp'].strftime('%H:%M:%S')}")
        print(f"   - çº§åˆ«: {latest['level']}")
        print(f"   - æ¶ˆæ¯: {latest['message']}")
    print("   âœ“ get_logs() æµ‹è¯•å®Œæˆ\n")

    # 7. å¯åŠ¨ç›‘æ§ UI
    print("7. å¯åŠ¨å¢å¼ºçš„ç›‘æ§ UI...")
    print("\n" + "="*60)
    print("ğŸ“Š ç›‘æ§ UI åŒ…å« 3 ä¸ªæ ‡ç­¾é¡µ:")
    print("   1. ğŸ“ˆ æ¦‚è§ˆ - æœåŠ¡å™¨ç»Ÿè®¡å’Œè¯·æ±‚å†å²")
    print("   2. ğŸ“ å®æ—¶æ—¥å¿— - æ”¯æŒçº§åˆ«è¿‡æ»¤çš„æ—¥å¿—æŸ¥çœ‹")
    print("   3. â±ï¸ å»¶è¿Ÿåˆ†æ - P95/P99ç»Ÿè®¡ã€åˆ†å¸ƒå›¾ã€è¶‹åŠ¿å›¾")
    print("="*60)
    print("\nğŸŒ UI å°†åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€: http://127.0.0.1:7862")
    print("\nâš¡ åŠŸèƒ½ç‰¹æ€§:")
    print("   â€¢ æ¯ 2 ç§’è‡ªåŠ¨åˆ·æ–°")
    print("   â€¢ æ”¯æŒæ—¥å¿—çº§åˆ«è¿‡æ»¤ (DEBUG/INFO/WARNING/ERROR)")
    print("   â€¢ äº¤äº’å¼ Plotly å›¾è¡¨")
    print("   â€¢ è¯¦ç»†çš„å»¶è¿Ÿç™¾åˆ†ä½ç»Ÿè®¡")
    print("\næŒ‰ Ctrl+C åœæ­¢\n")

    # åˆ›å»ºå¹¶å¯åŠ¨ UI
    ui = ServerMonitoringUI(
        servicer=servicer,
        theme="default",
        refresh_interval=2
    )

    try:
        ui.launch(
            share=False,
            server_port=7862,
            inbrowser=True,
            blocking=True
        )
    except KeyboardInterrupt:
        print("\n\nâœ“ æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main()
