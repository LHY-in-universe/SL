#!/usr/bin/env python3
"""
ä½¿ç”¨ SplitLearnCore åº“åŠ è½½æ¨¡å‹æ–‡ä»¶

æµ‹è¯•ä»ç¡¬ç›˜åŠ è½½æ¨¡å‹åˆ°å†…å­˜çš„åŠŸèƒ½ï¼Œä½¿ç”¨ core åº“çš„æ¨¡å‹ç±»
"""

import os
import sys
import time
import torch
from pathlib import Path

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆåœ¨å¯¼å…¥ torch ä¹‹å‰ï¼‰
os.environ.setdefault('OMP_NUM_THREADS', '1')
os.environ.setdefault('MKL_NUM_THREADS', '1')
os.environ.setdefault('NUMEXPR_NUM_THREADS', '1')

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, os.path.join(project_root, 'SplitLearnCore', 'src'))

# å¯¼å…¥ core åº“
from splitlearn_core.models.gpt2 import GPT2TrunkModel, GPT2BottomModel, GPT2TopModel
from transformers import GPT2Config

# æµ‹è¯•é…ç½®
MODEL_FILES = {
    "gpt2_trunk_full.pt": {
        "type": "trunk",
        "model_class": GPT2TrunkModel,
        "split_config": {"start_layer": 2, "end_layer": 10}
    },
    "gpt2_bottom_cached.pt": {
        "type": "bottom",
        "model_class": GPT2BottomModel,
        "split_config": {"end_layer": 2}
    },
    "gpt2_top_cached.pt": {
        "type": "top",
        "model_class": GPT2TopModel,
        "split_config": {"start_layer": 10}
    },
}


def format_size(size_bytes):
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if size_bytes < 1024.0:
            return f"{size_bytes:.2f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.2f} TB"


def get_model_info(model):
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    model_size_mb = total_params * 4 / (1024 * 1024)  # å‡è®¾ float32
    
    return {
        "type": type(model).__name__,
        "total_params": total_params,
        "trainable_params": trainable_params,
        "model_size_mb": model_size_mb,
    }


def test_load_with_torch_load(model_path):
    """æ–¹æ³• 1: ç›´æ¥ä½¿ç”¨ torch.load() åŠ è½½æ•´ä¸ªæ¨¡å‹å¯¹è±¡"""
    print("\n" + "=" * 70)
    print(f"ğŸ“¦ æ–¹æ³• 1: ä½¿ç”¨ torch.load() åŠ è½½æ¨¡å‹")
    print("=" * 70)
    print(f"æ–‡ä»¶: {os.path.basename(model_path)}")
    
    if not os.path.exists(model_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return None
    
    file_size = os.path.getsize(model_path)
    print(f"\nğŸ“ æ–‡ä»¶ä¿¡æ¯:")
    print(f"   å¤§å°: {format_size(file_size)}")
    
    print(f"\nâ³ å¼€å§‹åŠ è½½...")
    start_time = time.time()
    
    try:
        model = torch.load(model_path, map_location='cpu', weights_only=False)
        load_time = time.time() - start_time
        
        print(f"   âœ“ åŠ è½½æˆåŠŸï¼è€—æ—¶: {load_time:.2f} ç§’")
        
        model_info = get_model_info(model)
        print(f"\nğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"   ç±»å‹: {model_info['type']}")
        print(f"   å‚æ•°é‡: {model_info['total_params']:,}")
        print(f"   æ¨¡å‹å¤§å°: {model_info['model_size_mb']:.2f} MB")
        
        return model
        
    except Exception as e:
        print(f"   âŒ åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def test_load_with_core_class(model_path, model_info_config):
    """æ–¹æ³• 2: ä½¿ç”¨ core åº“çš„æ¨¡å‹ç±»åŠ è½½"""
    print("\n" + "=" * 70)
    print(f"ğŸ“¦ æ–¹æ³• 2: ä½¿ç”¨ SplitLearnCore æ¨¡å‹ç±»åŠ è½½")
    print("=" * 70)
    print(f"æ–‡ä»¶: {os.path.basename(model_path)}")
    print(f"æ¨¡å‹ç±»å‹: {model_info_config['type']}")
    
    if not os.path.exists(model_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return None
    
    file_size = os.path.getsize(model_path)
    print(f"\nğŸ“ æ–‡ä»¶ä¿¡æ¯:")
    print(f"   å¤§å°: {format_size(file_size)}")
    
    print(f"\nâ³ å¼€å§‹åŠ è½½...")
    start_time = time.time()
    
    try:
        # æ–¹æ³• 2a: å¦‚æœæ–‡ä»¶ä¿å­˜çš„æ˜¯æ•´ä¸ªæ¨¡å‹å¯¹è±¡ï¼Œç›´æ¥åŠ è½½
        loaded_data = torch.load(model_path, map_location='cpu', weights_only=False)
        
        # æ£€æŸ¥åŠ è½½çš„æ˜¯ä»€ä¹ˆ
        if isinstance(loaded_data, torch.nn.Module):
            print(f"   âœ“ æ–‡ä»¶åŒ…å«å®Œæ•´æ¨¡å‹å¯¹è±¡")
            model = loaded_data
            load_time = time.time() - start_time
            print(f"   âœ“ åŠ è½½æˆåŠŸï¼è€—æ—¶: {load_time:.2f} ç§’")
            
        elif isinstance(loaded_data, dict):
            print(f"   âœ“ æ–‡ä»¶åŒ…å« state_dict")
            print(f"   æ­£åœ¨ä½¿ç”¨ core åº“åˆ›å»ºæ¨¡å‹å®ä¾‹...")
            
            # ä½¿ç”¨ core åº“åˆ›å»ºæ¨¡å‹å®ä¾‹
            model_class = model_info_config['model_class']
            split_config = model_info_config['split_config']
            
            # åˆ›å»ºé…ç½®
            config = GPT2Config()
            
            # æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºå®ä¾‹
            if model_info_config['type'] == 'trunk':
                model = model_class(
                    config=config,
                    start_layer=split_config['start_layer'],
                    end_layer=split_config['end_layer']
                )
            elif model_info_config['type'] == 'bottom':
                model = model_class(
                    config=config,
                    end_layer=split_config['end_layer']
                )
            elif model_info_config['type'] == 'top':
                model = model_class(
                    config=config,
                    start_layer=split_config['start_layer']
                )
            
            # åŠ è½½ state_dict
            print(f"   æ­£åœ¨åŠ è½½ state_dict...")
            model.load_state_dict(loaded_data, strict=False)
            
            load_time = time.time() - start_time
            print(f"   âœ“ åŠ è½½æˆåŠŸï¼è€—æ—¶: {load_time:.2f} ç§’")
        else:
            print(f"   âš ï¸  æœªçŸ¥çš„æ•°æ®ç±»å‹: {type(loaded_data)}")
            return None
        
        model.eval()
        print(f"   âœ“ æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼")
        
        model_info = get_model_info(model)
        print(f"\nğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"   ç±»å‹: {model_info['type']}")
        print(f"   å‚æ•°é‡: {model_info['total_params']:,}")
        print(f"   æ¨¡å‹å¤§å°: {model_info['model_size_mb']:.2f} MB")
        
        # æµ‹è¯•æ¨ç†
        print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹æ¨ç†...")
        try:
            if model_info_config['type'] == 'trunk':
                test_input = torch.randn(1, 5, 768)  # [batch, seq_len, hidden_dim]
            elif model_info_config['type'] == 'bottom':
                test_input = torch.randint(0, 50257, (1, 5))  # [batch, seq_len] token ids
            else:  # top
                test_input = torch.randn(1, 5, 768)  # [batch, seq_len, hidden_dim]
            
            with torch.no_grad():
                output = model(test_input)
            
            print(f"   âœ“ æ¨ç†æµ‹è¯•æˆåŠŸ")
            print(f"   è¾“å…¥å½¢çŠ¶: {test_input.shape}")
            print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
            
        except Exception as e:
            print(f"   âš ï¸  æ¨ç†æµ‹è¯•å¤±è´¥: {e}")
            print(f"   ï¼ˆè¿™å¯èƒ½æ˜¯å› ä¸ºè¾“å…¥æ ¼å¼ä¸æ­£ç¡®ï¼‰")
        
        return model
        
    except Exception as e:
        print(f"   âŒ åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 70)
    print("ğŸ§ª ä½¿ç”¨ SplitLearnCore åº“åŠ è½½æ¨¡å‹æ–‡ä»¶æµ‹è¯•")
    print("=" * 70)
    print("\nğŸ“‹ æµ‹è¯•ç›®æ ‡:")
    print("   1. ä½¿ç”¨ torch.load() ç›´æ¥åŠ è½½æ¨¡å‹")
    print("   2. ä½¿ç”¨ SplitLearnCore æ¨¡å‹ç±»åŠ è½½æ¨¡å‹")
    print("   3. æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯å’Œæ€§èƒ½ç»Ÿè®¡")
    print("   4. æµ‹è¯•æ¨¡å‹æ¨ç†åŠŸèƒ½")
    
    # è·å–æ¨¡å‹æ–‡ä»¶è·¯å¾„
    current_dir = os.path.dirname(os.path.abspath(__file__))
    
    # åªæµ‹è¯•å­˜åœ¨çš„æ–‡ä»¶
    existing_files = {}
    for filename, config in MODEL_FILES.items():
        filepath = os.path.join(current_dir, filename)
        if os.path.exists(filepath):
            existing_files[filepath] = config
    
    if not existing_files:
        print(f"\nâŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡å‹æ–‡ä»¶")
        print(f"   æŸ¥æ‰¾è·¯å¾„: {current_dir}")
        return 1
    
    print(f"\nğŸ“ æ‰¾åˆ° {len(existing_files)} ä¸ªæ¨¡å‹æ–‡ä»¶:")
    for filepath in existing_files.keys():
        print(f"   - {os.path.basename(filepath)}")
    
    # æµ‹è¯•æ¯ä¸ªæ¨¡å‹
    results = []
    for model_path, config in existing_files.items():
        print("\n" + "=" * 70)
        print(f"æµ‹è¯•æ–‡ä»¶: {os.path.basename(model_path)}")
        print("=" * 70)
        
        # æ–¹æ³• 1: ç›´æ¥ä½¿ç”¨ torch.load()
        model1 = test_load_with_torch_load(model_path)
        
        # æ–¹æ³• 2: ä½¿ç”¨ core åº“
        model2 = test_load_with_core_class(model_path, config)
        
        success = model1 is not None or model2 is not None
        results.append((os.path.basename(model_path), success))
        
        time.sleep(1)  # çŸ­æš‚ä¼‘æ¯
    
    # æ€»ç»“
    print("\n" + "=" * 70)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("=" * 70)
    
    for model_name, success in results:
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
        print(f"   {status}: {model_name}")
    
    success_count = sum(1 for _, s in results if s)
    print(f"\næ€»è®¡: {success_count}/{len(results)} ä¸ªæ¨¡å‹åŠ è½½æˆåŠŸ")
    
    return 0 if success_count == len(results) else 1


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

