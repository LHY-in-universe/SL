#!/usr/bin/env python3
"""
gRPC æœåŠ¡å™¨æµ‹è¯•è„šæœ¬ - å¼‚æ­¥ç‰ˆæœ¬ï¼ˆä¸ä½¿ç”¨æ¨¡å‹ï¼‰

åªæµ‹è¯•é€šä¿¡åŠŸèƒ½ï¼Œä½¿ç”¨ç®€å•çš„æ•°å­¦è¿ç®—ä»£æ›¿æ¨¡å‹
- ä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬ï¼ˆAsyncGRPCComputeServerï¼‰
- ä¸éœ€è¦åŠ è½½æ¨¡å‹
- ä¸éœ€è¦ PyTorch æ¨¡å‹
- åªæµ‹è¯•æ•°æ®ä¼ è¾“
- æ— çº¿ç¨‹ç«äº‰é—®é¢˜
"""

import os
import sys
import asyncio
import time
import torch
import logging

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¿…é¡»åœ¨å¯¼å…¥ grpc ä¹‹å‰ï¼‰
os.environ['GRPC_VERBOSITY'] = 'ERROR'
os.environ['GLOG_minloglevel'] = '2'

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, os.path.join(project_root, 'SplitLearnComm', 'src'))

# é…ç½®è¯¦ç»†æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)

from splitlearn_comm import AsyncGRPCComputeServer
from splitlearn_comm.core import AsyncComputeFunction

# æµ‹è¯•é…ç½®
PORT = 50056
HOST = "0.0.0.0"


class SimpleAsyncComputeFunction(AsyncComputeFunction):
    """
    ç®€å•çš„å¼‚æ­¥è®¡ç®—å‡½æ•° - ä¸ä½¿ç”¨æ¨¡å‹
    
    åªåšç®€å•çš„æ•°å­¦è¿ç®—æ¥æµ‹è¯•é€šä¿¡åŠŸèƒ½
    ä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬ï¼Œé¿å…çº¿ç¨‹ç«äº‰
    """
    
    def __init__(self):
        self.request_count = 0
        print("âœ“ ç®€å•å¼‚æ­¥è®¡ç®—å‡½æ•°åˆ›å»ºï¼ˆä¸ä½¿ç”¨æ¨¡å‹ï¼‰")
    
    async def compute(self, input_tensor: torch.Tensor) -> torch.Tensor:
        """æ‰§è¡Œç®€å•è®¡ç®—ï¼šè¾“å…¥ * 2 + 1"""
        self.request_count += 1
        req_id = self.request_count
        
        print("\n" + "=" * 70)
        print(f"ğŸ“¥ æœåŠ¡å™¨æ”¶åˆ°è¯·æ±‚ #{req_id}")
        print("=" * 70)
        
        # æ˜¾ç¤ºè¾“å…¥æ•°æ®ä¿¡æ¯
        print(f"\nğŸ“Š è¾“å…¥æ•°æ®ä¿¡æ¯:")
        print(f"   å½¢çŠ¶: {input_tensor.shape}")
        print(f"   æ•°æ®ç±»å‹: {input_tensor.dtype}")
        print(f"   æ•°æ®å¤§å°: {input_tensor.numel() * 4 / 1024:.2f} KB")
        
        # æ˜¾ç¤ºè¾“å…¥æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“ˆ è¾“å…¥æ•°æ®ç»Ÿè®¡:")
        print(f"   æœ€å°å€¼: {input_tensor.min().item():.6f}")
        print(f"   æœ€å¤§å€¼: {input_tensor.max().item():.6f}")
        print(f"   å¹³å‡å€¼: {input_tensor.mean().item():.6f}")
        
        # æ‰§è¡Œç®€å•è®¡ç®—ï¼ˆæ¨¡æ‹Ÿæ¨¡å‹æ¨ç†ï¼‰
        print(f"\nâš™ï¸  æ‰§è¡Œè®¡ç®—: output = input * 2 + 1")
        start_time = time.time()
        
        with torch.no_grad():
            output = input_tensor * 2 + 1
        
        compute_time = (time.time() - start_time) * 1000
        
        # æ˜¾ç¤ºè¾“å‡ºæ•°æ®ä¿¡æ¯
        print(f"\nğŸ“¤ è¾“å‡ºæ•°æ®ä¿¡æ¯:")
        print(f"   å½¢çŠ¶: {output.shape}")
        print(f"   æ•°æ®ç±»å‹: {output.dtype}")
        print(f"   æ•°æ®å¤§å°: {output.numel() * 4 / 1024:.2f} KB")
        print(f"   è®¡ç®—è€—æ—¶: {compute_time:.2f} ms")
        
        # æ˜¾ç¤ºè¾“å‡ºæ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“ˆ è¾“å‡ºæ•°æ®ç»Ÿè®¡:")
        print(f"   æœ€å°å€¼: {output.min().item():.6f}")
        print(f"   æœ€å¤§å€¼: {output.max().item():.6f}")
        print(f"   å¹³å‡å€¼: {output.mean().item():.6f}")
        
        # æ•°æ®ä¼ è¾“ä¿¡æ¯
        input_size_kb = input_tensor.numel() * 4 / 1024
        output_size_kb = output.numel() * 4 / 1024
        total_size_kb = input_size_kb + output_size_kb
        
        print(f"\nğŸ“¡ æ•°æ®ä¼ è¾“ç»Ÿè®¡:")
        print(f"   æ¥æ”¶æ•°æ®: {input_size_kb:.2f} KB")
        print(f"   å‘é€æ•°æ®: {output_size_kb:.2f} KB")
        print(f"   æ€»ä¼ è¾“: {total_size_kb:.2f} KB")
        print(f"   æ€»è€—æ—¶: {compute_time:.2f} ms")
        if compute_time > 0:
            print(f"   ååé‡: {total_size_kb / (compute_time / 1000):.2f} KB/s")
        
        print("=" * 70)
        
        return output
    
    def get_info(self):
        return {
            "name": "SimpleAsyncComputeFunction",
            "description": "ç®€å•å¼‚æ­¥è®¡ç®—å‡½æ•°ï¼ˆä¸ä½¿ç”¨æ¨¡å‹ï¼‰",
            "operation": "output = input * 2 + 1",
            "total_requests": self.request_count
        }
    
    async def setup(self):
        """åˆå§‹åŒ–"""
        pass
    
    async def teardown(self):
        """æ¸…ç†"""
        pass


async def async_main():
    """å¼‚æ­¥ä¸»å‡½æ•°"""
    print("\n" + "=" * 70)
    print("ğŸš€ gRPC æœåŠ¡å™¨å¯åŠ¨ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ - ä¸ä½¿ç”¨æ¨¡å‹ï¼‰")
    print("=" * 70)
    
    # åˆ›å»ºè®¡ç®—å‡½æ•°ï¼ˆä¸ä½¿ç”¨æ¨¡å‹ï¼‰
    print(f"\nğŸ”§ åˆ›å»ºå¼‚æ­¥è®¡ç®—å‡½æ•°...")
    compute_fn = SimpleAsyncComputeFunction()
    
    # åˆ›å»ºå¼‚æ­¥æœåŠ¡å™¨
    print(f"\nğŸŒ åˆ›å»ºå¼‚æ­¥ gRPC æœåŠ¡å™¨...")
    print(f"   ç›‘å¬åœ°å€: {HOST}:{PORT}")
    print(f"   ä½¿ç”¨åç¨‹ï¼ˆä¸æ˜¯çº¿ç¨‹æ± ï¼‰")
    print(f"   âœ… æ— çº¿ç¨‹ç«äº‰é—®é¢˜")
    
    server = AsyncGRPCComputeServer(
        compute_fn=compute_fn,
        host=HOST,
        port=PORT
    )
    print("   âœ“ å¼‚æ­¥æœåŠ¡å™¨åˆ›å»ºæˆåŠŸ")
    
    # å¯åŠ¨æœåŠ¡å™¨
    print(f"\nâ–¶ï¸  å¯åŠ¨æœåŠ¡å™¨...")
    await server.start()
    print("   âœ“ æœåŠ¡å™¨å·²å¯åŠ¨")
    
    print("\n" + "=" * 70)
    print("âœ… æœåŠ¡å™¨è¿è¡Œä¸­ï¼Œç­‰å¾…å®¢æˆ·ç«¯è¿æ¥...")
    print("=" * 70)
    print(f"\nğŸ“¡ æœåŠ¡å™¨åœ°å€: localhost:{PORT}")
    print(f"ğŸ’¡ åœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œå®¢æˆ·ç«¯: python testcode/client_comm_simple.py")
    print(f"ğŸ’¡ æˆ–è€…ä½¿ç”¨å¼‚æ­¥å®¢æˆ·ç«¯: python testcode/client_comm_simple_async.py")
    print(f"â¹ï¸  æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨\n")
    
    try:
        await server.wait_for_termination()
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨å…³é—­æœåŠ¡å™¨...")
        await server.stop()
        print("   âœ“ æœåŠ¡å™¨å·²å…³é—­")
        print(f"\nğŸ“Š æ€»å…±å¤„ç†äº† {compute_fn.request_count} ä¸ªè¯·æ±‚")


def main():
    """ä¸»å‡½æ•°"""
    try:
        asyncio.run(async_main())
        return 0
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ æœåŠ¡å™¨è¢«ç”¨æˆ·ä¸­æ–­")
        return 0


if __name__ == "__main__":
    try:
        sys.exit(main())
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

