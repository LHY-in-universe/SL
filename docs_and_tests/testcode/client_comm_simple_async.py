#!/usr/bin/env python3
"""
gRPC å®¢æˆ·ç«¯æµ‹è¯•è„šæœ¬ - å¼‚æ­¥ç‰ˆæœ¬

è¿æ¥å¼‚æ­¥æœåŠ¡å™¨ï¼Œæµ‹è¯•é€šä¿¡åŠŸèƒ½
ä½¿ç”¨å¼‚æ­¥å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨ grpc.aioï¼‰
"""

import os
import sys
import asyncio
import time
import torch
import logging

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¿…é¡»åœ¨å¯¼å…¥ grpc ä¹‹å‰ï¼‰
os.environ['GRPC_VERBOSITY'] = 'ERROR'
os.environ['GLOG_minloglevel'] = '2'

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, os.path.join(project_root, 'SplitLearnComm', 'src'))

# é…ç½®è¯¦ç»†æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)

import grpc.aio
from splitlearn_comm.protocol import compute_service_pb2, compute_service_pb2_grpc
from splitlearn_comm.core import TensorCodec

# æµ‹è¯•é…ç½®
SERVER_ADDRESS = "localhost:50056"  # å¼‚æ­¥æœåŠ¡å™¨ç«¯å£
TIMEOUT = 30.0


def print_tensor_info(tensor, name, prefix="   "):
    """æ‰“å°å¼ é‡è¯¦ç»†ä¿¡æ¯"""
    print(f"{prefix}å½¢çŠ¶: {tensor.shape}")
    print(f"{prefix}æ•°æ®ç±»å‹: {tensor.dtype}")
    print(f"{prefix}æ•°æ®å¤§å°: {tensor.numel() * 4 / 1024:.2f} KB")
    print(f"{prefix}æœ€å°å€¼: {tensor.min().item():.6f}")
    print(f"{prefix}æœ€å¤§å€¼: {tensor.max().item():.6f}")
    print(f"{prefix}å¹³å‡å€¼: {tensor.mean().item():.6f}")


class AsyncGRPCClient:
    """ç®€å•çš„å¼‚æ­¥ gRPC å®¢æˆ·ç«¯"""
    
    def __init__(self, server_address: str, timeout: float = 30.0):
        self.server_address = server_address
        self.timeout = timeout
        self.channel = None
        self.stub = None
        self.codec = TensorCodec()
        self.request_count = 0
    
    async def connect(self):
        """å¼‚æ­¥è¿æ¥æœåŠ¡å™¨"""
        print(f"\nğŸ“¡ è¿æ¥æœåŠ¡å™¨: {self.server_address}")
        print("   æ­£åœ¨è¿æ¥...")
        
        try:
            self.channel = grpc.aio.insecure_channel(
                self.server_address,
                options=[
                    ("grpc.keepalive_time_ms", 30000),
                    ("grpc.keepalive_timeout_ms", 10000),
                ]
            )
            self.stub = compute_service_pb2_grpc.ComputeServiceStub(self.channel)
            
            # å¥åº·æ£€æŸ¥
            try:
                response = await asyncio.wait_for(
                    self.stub.HealthCheck(compute_service_pb2.HealthRequest()),
                    timeout=5.0
                )
                print("   âœ“ è¿æ¥æˆåŠŸï¼")
                print(f"   æœåŠ¡å™¨çŠ¶æ€: {response.status}")
                return True
            except asyncio.TimeoutError:
                print("   âš ï¸  è¿æ¥è¶…æ—¶")
                return False
            except Exception as e:
                print(f"   âš ï¸  å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
                return True  # è¿æ¥å¯èƒ½æˆåŠŸï¼Œåªæ˜¯å¥åº·æ£€æŸ¥å¤±è´¥
                
        except Exception as e:
            print(f"   âŒ è¿æ¥å¤±è´¥: {e}")
            return False
    
    async def compute(self, input_tensor: torch.Tensor) -> torch.Tensor:
        """å¼‚æ­¥å‘é€è®¡ç®—è¯·æ±‚"""
        self.request_count += 1
        
        # ç¼–ç è¾“å…¥
        data, shape = self.codec.encode(input_tensor)
        
        # åˆ›å»ºè¯·æ±‚
        request = compute_service_pb2.ComputeRequest(
            data=data,
            shape=list(shape)
        )
        
        # å‘é€è¯·æ±‚
        try:
            response = await asyncio.wait_for(
                self.stub.Compute(request),
                timeout=self.timeout
            )
            
            # è§£ç è¾“å‡º
            output = self.codec.decode(
                data=response.data,
                shape=tuple(response.shape)
            )
            
            return output
            
        except asyncio.TimeoutError:
            raise TimeoutError(f"è¯·æ±‚è¶…æ—¶ï¼ˆ{self.timeout} ç§’ï¼‰")
        except Exception as e:
            raise RuntimeError(f"è®¡ç®—è¯·æ±‚å¤±è´¥: {e}")
    
    async def get_service_info(self):
        """è·å–æœåŠ¡å™¨ä¿¡æ¯"""
        if self.stub is None:
            return None
        
        try:
            response = await asyncio.wait_for(
                self.stub.GetServiceInfo(compute_service_pb2.ServiceInfoRequest()),
                timeout=5.0
            )
            
            return {
                "service_name": response.service_name,
                "version": response.version,
                "device": response.device,
                "total_requests": response.total_requests,
                "uptime_seconds": response.uptime_seconds,
            }
        except Exception as e:
            print(f"   âš ï¸  è·å–æœåŠ¡å™¨ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    async def close(self):
        """å…³é—­è¿æ¥"""
        if self.channel:
            await self.channel.close()
            print("   âœ“ è¿æ¥å·²å…³é—­")


async def test_connection(client):
    """æµ‹è¯•è¿æ¥"""
    print("\n" + "=" * 70)
    print("ğŸ”Œ è¿æ¥æµ‹è¯•")
    print("=" * 70)
    
    if await client.connect():
        return True
    else:
        print("   âŒ è¿æ¥å¤±è´¥ï¼")
        print(f"\nğŸ’¡ è¯·ç¡®ä¿æœåŠ¡å™¨æ­£åœ¨è¿è¡Œ:")
        print(f"   python testcode/server_comm_simple.py")
        return False


async def test_compute(client, request_num=1):
    """æµ‹è¯•è®¡ç®—å¹¶æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯"""
    print("\n" + "=" * 70)
    print(f"ğŸ“¤ å‘é€è¯·æ±‚ #{request_num}")
    print("=" * 70)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    test_input = torch.randn(1, 10, 768)
    
    print(f"\nğŸ“Š å‡†å¤‡å‘é€çš„æ•°æ®:")
    print_tensor_info(test_input, "è¾“å…¥æ•°æ®")
    
    # è®¡ç®—æ•°æ®å¤§å°
    input_size_kb = test_input.numel() * 4 / 1024
    
    # å‘é€è¯·æ±‚
    print(f"\nğŸš€ å‘é€è®¡ç®—è¯·æ±‚...")
    print(f"   æ•°æ®å¤§å°: {input_size_kb:.2f} KB")
    print(f"   æ­£åœ¨ä¼ è¾“...")
    
    start_time = time.time()
    
    try:
        output = await client.compute(test_input)
        
        total_time = (time.time() - start_time) * 1000
        
        print(f"\nğŸ“¥ æ”¶åˆ°å“åº”")
        print("=" * 70)
        
        # æ˜¾ç¤ºè¾“å‡ºæ•°æ®ä¿¡æ¯
        print(f"\nğŸ“Š æ¥æ”¶åˆ°çš„æ•°æ®:")
        print_tensor_info(output, "è¾“å‡ºæ•°æ®")
        
        # éªŒè¯è®¡ç®—ç»“æœï¼ˆåº”è¯¥æ˜¯ input * 2 + 1ï¼‰
        expected = test_input * 2 + 1
        if torch.allclose(output, expected, atol=1e-5):
            print(f"\nâœ… è®¡ç®—ç»“æœæ­£ç¡®: output = input * 2 + 1")
        else:
            print(f"\nâš ï¸  è®¡ç®—ç»“æœä¸ç¬¦åˆé¢„æœŸ")
        
        # è®¡ç®—ä¼ è¾“ç»Ÿè®¡
        output_size_kb = output.numel() * 4 / 1024
        total_size_kb = input_size_kb + output_size_kb
        
        print(f"\nğŸ“¡ ä¼ è¾“ç»Ÿè®¡:")
        print(f"   å‘é€æ•°æ®: {input_size_kb:.2f} KB")
        print(f"   æ¥æ”¶æ•°æ®: {output_size_kb:.2f} KB")
        print(f"   æ€»ä¼ è¾“: {total_size_kb:.2f} KB")
        print(f"   æ€»è€—æ—¶: {total_time:.2f} ms")
        if total_time > 0:
            print(f"   ååé‡: {total_size_kb / (total_time / 1000):.2f} KB/s")
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        if output.shape == test_input.shape:
            print(f"\nâœ… è¾“å‡ºå½¢çŠ¶æ­£ç¡®: {output.shape}")
        else:
            print(f"\nâš ï¸  è¾“å‡ºå½¢çŠ¶ä¸ç¬¦åˆé¢„æœŸ: {output.shape} (æœŸæœ›: {test_input.shape})")
        
        print("=" * 70)
        
        return True
        
    except Exception as e:
        print(f"\nâŒ è¯·æ±‚å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_multiple_requests(client, num_requests=5):
    """æµ‹è¯•å¤šæ¬¡è¯·æ±‚"""
    print("\n" + "=" * 70)
    print(f"ğŸ”„ å¤šæ¬¡è¯·æ±‚æµ‹è¯• ({num_requests} æ¬¡)")
    print("=" * 70)
    
    successes = 0
    total_time = 0.0
    total_data = 0.0
    
    for i in range(num_requests):
        print(f"\n--- è¯·æ±‚ {i+1}/{num_requests} ---")
        
        test_input = torch.randn(1, 5, 768)
        input_size_kb = test_input.numel() * 4 / 1024
        
        start_time = time.time()
        try:
            output = await client.compute(test_input)
            elapsed = (time.time() - start_time) * 1000
            
            # éªŒè¯ç»“æœ
            expected = test_input * 2 + 1
            if torch.allclose(output, expected, atol=1e-5):
                output_size_kb = output.numel() * 4 / 1024
                request_data = input_size_kb + output_size_kb
                
                total_time += elapsed
                total_data += request_data
                successes += 1
                
                print(f"   âœ“ æˆåŠŸ (è€—æ—¶: {elapsed:.2f} ms, æ•°æ®: {request_data:.2f} KB)")
            else:
                print(f"   âš ï¸  ç»“æœä¸æ­£ç¡®")
            
        except Exception as e:
            print(f"   âŒ å¤±è´¥: {e}")
    
    print(f"\nğŸ“Š æ€»ç»“:")
    print(f"   æˆåŠŸ: {successes}/{num_requests}")
    if successes > 0:
        print(f"   æ€»è€—æ—¶: {total_time:.2f} ms")
        print(f"   å¹³å‡è€—æ—¶: {total_time/successes:.2f} ms")
        print(f"   æ€»ä¼ è¾“: {total_data:.2f} KB")
        if total_time > 0:
            print(f"   å¹³å‡ååé‡: {total_data / (total_time / 1000):.2f} KB/s")
    
    return successes == num_requests


async def test_service_info(client):
    """æµ‹è¯•æœåŠ¡ä¿¡æ¯"""
    print("\n" + "=" * 70)
    print("â„¹ï¸  æœåŠ¡ä¿¡æ¯æŸ¥è¯¢")
    print("=" * 70)
    
    info = await client.get_service_info()
    
    if info:
        print(f"\nğŸ“‹ æœåŠ¡å™¨ä¿¡æ¯:")
        print(f"   æœåŠ¡å: {info.get('service_name', 'N/A')}")
        print(f"   ç‰ˆæœ¬: {info.get('version', 'N/A')}")
        print(f"   è®¾å¤‡: {info.get('device', 'N/A')}")
        print(f"   æ€»è¯·æ±‚æ•°: {info.get('total_requests', 0)}")
        print(f"   è¿è¡Œæ—¶é—´: {info.get('uptime_seconds', 0):.1f} ç§’")
        return True
    else:
        print("   âŒ æ— æ³•è·å–æœåŠ¡å™¨ä¿¡æ¯")
        return False


async def async_main():
    """å¼‚æ­¥ä¸»å‡½æ•°"""
    print("\n" + "=" * 70)
    print("ğŸ’» gRPC å®¢æˆ·ç«¯æµ‹è¯•ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ - æµ‹è¯•é€šä¿¡åŠŸèƒ½ï¼‰")
    print("=" * 70)
    print(f"\nğŸ“¡ æœåŠ¡å™¨åœ°å€: {SERVER_ADDRESS}")
    print(f"â±ï¸  è¶…æ—¶æ—¶é—´: {TIMEOUT} ç§’")
    print(f"ğŸ’¡ æœåŠ¡å™¨æ‰§è¡Œ: output = input * 2 + 1")
    print(f"âœ… ä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬ï¼ˆæ— çº¿ç¨‹ç«äº‰ï¼‰")
    print()
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = AsyncGRPCClient(
        server_address=SERVER_ADDRESS,
        timeout=TIMEOUT
    )
    
    # è¿æ¥æœåŠ¡å™¨
    if not await test_connection(client):
        return 1
    
    try:
        # æµ‹è¯•æœåŠ¡ä¿¡æ¯
        await test_service_info(client)
        
        # æµ‹è¯•å•æ¬¡è®¡ç®—
        await test_compute(client, request_num=1)
        
        # æµ‹è¯•å¤šæ¬¡è¯·æ±‚
        await test_multiple_requests(client, num_requests=5)
        
        print("\n" + "=" * 70)
        print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ")
        print("=" * 70)
        
        print(f"\nğŸ“Š å®¢æˆ·ç«¯ç»Ÿè®¡:")
        print(f"   æ€»è¯·æ±‚æ•°: {client.request_count}")
        
    finally:
        print("\nğŸ”Œ å…³é—­è¿æ¥...")
        await client.close()
    
    return 0


def main():
    """ä¸»å‡½æ•°"""
    try:
        return asyncio.run(async_main())
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())

