#!/usr/bin/env python3
"""
gRPC æœåŠ¡å™¨æµ‹è¯•è„šæœ¬ - æ˜¾ç¤ºæ•°æ®ä¼ è¾“è¯¦æƒ…

åœ¨ç»ˆç«¯è¿è¡Œæ­¤è„šæœ¬å¯åŠ¨æœåŠ¡å™¨ï¼Œå¯ä»¥çœ‹åˆ°ï¼š
- æ¥æ”¶åˆ°çš„è¯·æ±‚æ•°æ®
- å‘é€çš„å“åº”æ•°æ®
- æ•°æ®ä¼ è¾“çš„è¯¦ç»†ä¿¡æ¯
"""

import os
import sys
import time
import torch
import logging

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¿…é¡»åœ¨å¯¼å…¥ grpc ä¹‹å‰ï¼‰
os.environ['GRPC_VERBOSITY'] = 'ERROR'
os.environ['GLOG_minloglevel'] = '2'
os.environ.setdefault('OMP_NUM_THREADS', '1')
os.environ.setdefault('MKL_NUM_THREADS', '1')
os.environ.setdefault('NUMEXPR_NUM_THREADS', '1')

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, os.path.join(project_root, 'SplitLearnComm', 'src'))
sys.path.insert(0, os.path.join(project_root, 'SplitLearnCore', 'src'))

# é…ç½®è¯¦ç»†æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)

from splitlearn_comm import GRPCComputeServer
from splitlearn_comm.core import ModelComputeFunction, ComputeFunction

# æµ‹è¯•é…ç½®
PORT = 50055
HOST = "0.0.0.0"
MODEL_PATH = os.path.join(current_dir, "gpt2_trunk_full.pt")


class VerboseComputeFunction(ComputeFunction):
    """å¸¦è¯¦ç»†è¾“å‡ºçš„è®¡ç®—å‡½æ•°"""
    
    def __init__(self, model, device="cpu", model_name="test-model"):
        self.model = model.to(device).eval()
        self.device = device
        self.model_name = model_name
        self.request_count = 0
    
    def compute(self, input_tensor: torch.Tensor) -> torch.Tensor:
        """æ‰§è¡Œè®¡ç®—å¹¶æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯"""
        self.request_count += 1
        req_id = self.request_count
        
        print("\n" + "=" * 70)
        print(f"ğŸ“¥ æœåŠ¡å™¨æ”¶åˆ°è¯·æ±‚ #{req_id}")
        print("=" * 70)
        
        # æ˜¾ç¤ºè¾“å…¥æ•°æ®ä¿¡æ¯
        print(f"\nğŸ“Š è¾“å…¥æ•°æ®ä¿¡æ¯:")
        print(f"   å½¢çŠ¶: {input_tensor.shape}")
        print(f"   æ•°æ®ç±»å‹: {input_tensor.dtype}")
        print(f"   æ•°æ®å¤§å°: {input_tensor.numel() * 4 / 1024:.2f} KB")
        print(f"   è®¾å¤‡: {input_tensor.device}")
        
        # æ˜¾ç¤ºè¾“å…¥æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“ˆ è¾“å…¥æ•°æ®ç»Ÿè®¡:")
        print(f"   æœ€å°å€¼: {input_tensor.min().item():.6f}")
        print(f"   æœ€å¤§å€¼: {input_tensor.max().item():.6f}")
        print(f"   å¹³å‡å€¼: {input_tensor.mean().item():.6f}")
        print(f"   æ ‡å‡†å·®: {input_tensor.std().item():.6f}")
        
        # æ˜¾ç¤ºè¾“å…¥æ•°æ®çš„éƒ¨åˆ†å€¼ï¼ˆå‰å‡ ä¸ªå…ƒç´ ï¼‰
        flat_input = input_tensor.flatten()
        print(f"\nğŸ”¢ è¾“å…¥æ•°æ®å‰10ä¸ªå€¼:")
        print(f"   {flat_input[:10].tolist()}")
        
        # æ‰§è¡Œè®¡ç®—
        print(f"\nâš™ï¸  å¼€å§‹è®¡ç®—...")
        start_time = time.time()
        
        input_on_device = input_tensor.to(self.device)
        with torch.no_grad():
            output = self.model(input_on_device)
        
        compute_time = (time.time() - start_time) * 1000
        
        # æ˜¾ç¤ºè¾“å‡ºæ•°æ®ä¿¡æ¯
        print(f"\nğŸ“¤ è¾“å‡ºæ•°æ®ä¿¡æ¯:")
        print(f"   å½¢çŠ¶: {output.shape}")
        print(f"   æ•°æ®ç±»å‹: {output.dtype}")
        print(f"   æ•°æ®å¤§å°: {output.numel() * 4 / 1024:.2f} KB")
        print(f"   è®¾å¤‡: {output.device}")
        print(f"   è®¡ç®—è€—æ—¶: {compute_time:.2f} ms")
        
        # æ˜¾ç¤ºè¾“å‡ºæ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“ˆ è¾“å‡ºæ•°æ®ç»Ÿè®¡:")
        print(f"   æœ€å°å€¼: {output.min().item():.6f}")
        print(f"   æœ€å¤§å€¼: {output.max().item():.6f}")
        print(f"   å¹³å‡å€¼: {output.mean().item():.6f}")
        print(f"   æ ‡å‡†å·®: {output.std().item():.6f}")
        
        # æ˜¾ç¤ºè¾“å‡ºæ•°æ®çš„éƒ¨åˆ†å€¼
        flat_output = output.flatten()
        print(f"\nğŸ”¢ è¾“å‡ºæ•°æ®å‰10ä¸ªå€¼:")
        print(f"   {flat_output[:10].tolist()}")
        
        # æ•°æ®ä¼ è¾“ä¿¡æ¯
        input_size_kb = input_tensor.numel() * 4 / 1024
        output_size_kb = output.numel() * 4 / 1024
        total_size_kb = input_size_kb + output_size_kb
        
        print(f"\nğŸ“¡ æ•°æ®ä¼ è¾“ç»Ÿè®¡:")
        print(f"   æ¥æ”¶æ•°æ®: {input_size_kb:.2f} KB")
        print(f"   å‘é€æ•°æ®: {output_size_kb:.2f} KB")
        print(f"   æ€»ä¼ è¾“: {total_size_kb:.2f} KB")
        print(f"   æ€»è€—æ—¶: {compute_time:.2f} ms")
        print(f"   ååé‡: {total_size_kb / (compute_time / 1000):.2f} KB/s")
        
        print("=" * 70)
        
        return output.cpu()
    
    def get_info(self):
        return {
            "name": self.model_name,
            "device": self.device,
            "total_requests": self.request_count
        }


def main():
    print("\n" + "=" * 70)
    print("ğŸš€ gRPC æœåŠ¡å™¨å¯åŠ¨")
    print("=" * 70)
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not os.path.exists(MODEL_PATH):
        print(f"\nâŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {MODEL_PATH}")
        print("   è¯·å…ˆè¿è¡Œ: python testcode/prepare_models.py")
        return 1
    
    # åŠ è½½æ¨¡å‹
    print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹: {MODEL_PATH}")
    try:
        from splitlearn_core.models.gpt2 import GPT2TrunkModel
        print("   âœ“ GPT2TrunkModel å¯¼å…¥æˆåŠŸ")
    except ImportError:
        print("   âš ï¸  æ— æ³•å¯¼å…¥ GPT2TrunkModelï¼Œå°è¯•ç›´æ¥åŠ è½½...")
    
    print("   æ­£åœ¨åŠ è½½æ¨¡å‹...")
    start_time = time.time()
    model = torch.load(MODEL_PATH, map_location='cpu', weights_only=False)
    model.eval()
    load_time = time.time() - start_time
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"   âœ“ æ¨¡å‹åŠ è½½å®Œæˆ (è€—æ—¶: {load_time:.2f} ç§’)")
    print(f"   âœ“ å‚æ•°é‡: {total_params:,}")
    
    # åˆ›å»ºè®¡ç®—å‡½æ•°
    print(f"\nğŸ”§ åˆ›å»ºè®¡ç®—å‡½æ•°...")
    compute_fn = VerboseComputeFunction(
        model=model,
        device="cpu",
        model_name="gpt2-trunk"
    )
    print("   âœ“ è®¡ç®—å‡½æ•°åˆ›å»ºæˆåŠŸ")
    
    # åˆ›å»ºæœåŠ¡å™¨
    print(f"\nğŸŒ åˆ›å»º gRPC æœåŠ¡å™¨...")
    print(f"   ç›‘å¬åœ°å€: {HOST}:{PORT}")
    print(f"   æœ€å¤§å·¥ä½œçº¿ç¨‹: 1 (å•çº¿ç¨‹æ¨¡å¼)")
    
    server = GRPCComputeServer(
        compute_fn=compute_fn,
        host=HOST,
        port=PORT,
        max_workers=1
    )
    print("   âœ“ æœåŠ¡å™¨åˆ›å»ºæˆåŠŸ")
    
    # å¯åŠ¨æœåŠ¡å™¨
    print(f"\nâ–¶ï¸  å¯åŠ¨æœåŠ¡å™¨...")
    server.start()
    print("   âœ“ æœåŠ¡å™¨å·²å¯åŠ¨")
    
    print("\n" + "=" * 70)
    print("âœ… æœåŠ¡å™¨è¿è¡Œä¸­ï¼Œç­‰å¾…å®¢æˆ·ç«¯è¿æ¥...")
    print("=" * 70)
    print(f"\nğŸ“¡ æœåŠ¡å™¨åœ°å€: localhost:{PORT}")
    print(f"ğŸ’¡ åœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œå®¢æˆ·ç«¯: python testcode/client_comm_test.py")
    print(f"â¹ï¸  æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨\n")
    
    try:
        server.wait_for_termination()
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨å…³é—­æœåŠ¡å™¨...")
        server.stop(grace=2)
        print("   âœ“ æœåŠ¡å™¨å·²å…³é—­")
        print(f"\nğŸ“Š æ€»å…±å¤„ç†äº† {compute_fn.request_count} ä¸ªè¯·æ±‚")
    
    return 0


if __name__ == "__main__":
    try:
        sys.exit(main())
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

