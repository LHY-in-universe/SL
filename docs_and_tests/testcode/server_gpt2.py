#!/usr/bin/env python3
"""
GPT-2 Split Learning Server
åŠ è½½ GPT-2 trunk æ¨¡å‹å¹¶æä¾› gRPC æœåŠ¡
"""

import os
import sys

# æŠ‘åˆ¶ gRPC å’Œ protobuf è­¦å‘Š (å¿…é¡»åœ¨å¯¼å…¥ grpc ä¹‹å‰è®¾ç½®)
os.environ['GRPC_VERBOSITY'] = 'ERROR'
os.environ['GRPC_TRACE'] = ''
os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'  # ä½¿ç”¨ Python å®ç°

import torch
import logging

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, os.path.join(project_root, 'SplitLearnComm', 'src'))
sys.path.insert(0, os.path.join(project_root, 'SplitLearnCore', 'src'))

from splitlearn_comm import GRPCComputeServer
from splitlearn_comm.core import ModelComputeFunction

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)


def main():
    print("=" * 70)
    print("GPT-2 Split Learning Server")
    print("=" * 70)

    # 1. å¯¼å…¥ splitlearn æ¨¡å—ï¼ˆtorch.load éœ€è¦è¿™äº›ç±»æ¥ååºåˆ—åŒ–ï¼‰
    print("\n[1] å¯¼å…¥ splitlearn æ¨¡å—...")
    try:
        from splitlearn_core.models.gpt2 import GPT2TrunkModel
        print("    âœ“ GPT2TrunkModel å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"    âŒ å¯¼å…¥å¤±è´¥: {e}")
        print("\nè¯·ç¡®ä¿ SplitLearning åŒ…å·²æ­£ç¡®å®‰è£…")
        return

    # 2. åŠ è½½ GPT-2 trunk æ¨¡å‹
    trunk_path = os.path.join(current_dir, "gpt2_trunk_full.pt")

    if not os.path.exists(trunk_path):
        print(f"\nâŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {trunk_path}")
        print("\nè¯·å…ˆè¿è¡Œ: python testcode/prepare_models.py")
        return

    print(f"\n[2] åŠ è½½ GPT-2 Trunk æ¨¡å‹...")
    print(f"    è·¯å¾„: {trunk_path}")
    trunk_model = torch.load(trunk_path, map_location='cpu', weights_only=False)
    trunk_model.eval()

    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in trunk_model.parameters())
    print(f"    âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"    âœ“ å‚æ•°é‡: {total_params:,}")

    # 3. åŒ…è£…ä¸º ComputeFunction
    print("\n[3] åˆ›å»º ComputeFunction...")
    compute_fn = ModelComputeFunction(
        model=trunk_model,
        device="cpu",  # ä½¿ç”¨ "cuda" å¦‚æœæœ‰ GPU
        model_name="gpt2-trunk"
    )
    print("    âœ“ ComputeFunction åˆ›å»ºå®Œæˆ")

    # 4. åˆ›å»ºå¹¶å¯åŠ¨æœåŠ¡å™¨
    print("\n[4] å¯åŠ¨ gRPC æœåŠ¡å™¨...")
    server = GRPCComputeServer(
        compute_fn=compute_fn,
        host="0.0.0.0",
        port=50051,
        max_workers=10
    )

    print("\n" + "=" * 70)
    print("âœ… æœåŠ¡å™¨å¯åŠ¨æˆåŠŸï¼")
    print("=" * 70)
    print(f"ğŸ“¡ ç›‘å¬åœ°å€: 0.0.0.0:50051")
    print(f"ğŸ”— æœ¬åœ°è¿æ¥: localhost:50051")
    print(f"ğŸ”— å±€åŸŸç½‘è¿æ¥: <ä½ çš„IP>:50051")
    print("\nğŸ’¡ æç¤º:")
    print("   â€¢ ä¿æŒæ­¤çª—å£è¿è¡Œ")
    print("   â€¢ åœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œå®¢æˆ·ç«¯")
    print("   â€¢ æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨")
    print("=" * 70)
    print()

    try:
        server.start()
        server.wait_for_termination()
    except KeyboardInterrupt:
        print("\n\nåœæ­¢æœåŠ¡å™¨...")
        server.stop()
        print("âœ“ æœåŠ¡å™¨å·²å…³é—­")


if __name__ == "__main__":
    main()
