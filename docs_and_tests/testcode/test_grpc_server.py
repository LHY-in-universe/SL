#!/usr/bin/env python3
"""
æµ‹è¯• gRPC æœåŠ¡å™¨åŠŸèƒ½

æµ‹è¯•å†…å®¹ï¼š
1. æœåŠ¡å™¨åˆ›å»ºå’Œåˆå§‹åŒ–
2. æœåŠ¡å™¨å¯åŠ¨
3. ç«¯å£ç›‘å¬æ£€æŸ¥
4. æœåŠ¡å™¨åœæ­¢
5. ä½¿ç”¨å®é™…æ¨¡å‹è¿›è¡Œæµ‹è¯•
"""

import os
import sys
import time
import socket
import torch
import logging

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¿…é¡»åœ¨å¯¼å…¥ grpc ä¹‹å‰ï¼‰
os.environ['GRPC_VERBOSITY'] = 'ERROR'
os.environ['GLOG_minloglevel'] = '2'
os.environ.setdefault('OMP_NUM_THREADS', '1')
os.environ.setdefault('MKL_NUM_THREADS', '1')
os.environ.setdefault('NUMEXPR_NUM_THREADS', '1')

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, os.path.join(project_root, 'SplitLearnComm', 'src'))
sys.path.insert(0, os.path.join(project_root, 'SplitLearnCore', 'src'))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

from splitlearn_comm import GRPCComputeServer
from splitlearn_comm.core import ModelComputeFunction

# æµ‹è¯•é…ç½®
TEST_PORT = 50053
TEST_HOST = "0.0.0.0"
MODEL_PATH = os.path.join(current_dir, "gpt2_trunk_full.pt")


def check_port_available(host, port):
    """æ£€æŸ¥ç«¯å£æ˜¯å¦å¯ç”¨"""
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        sock.bind((host, port))
        sock.close()
        return True
    except OSError:
        return False


def test_server_creation():
    """æµ‹è¯• 1: æœåŠ¡å™¨åˆ›å»º"""
    print("=" * 70)
    print("æµ‹è¯• 1: æœåŠ¡å™¨åˆ›å»º")
    print("=" * 70)
    
    try:
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        if not os.path.exists(MODEL_PATH):
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {MODEL_PATH}")
            print("   è¯·å…ˆè¿è¡Œ: python testcode/prepare_models.py")
            return False
        
        # åŠ è½½æ¨¡å‹
        print(f"\nåŠ è½½æ¨¡å‹: {MODEL_PATH}")
        model = torch.load(MODEL_PATH, map_location='cpu', weights_only=False)
        model.eval()
        print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # åˆ›å»ºè®¡ç®—å‡½æ•°
        print("\nåˆ›å»º ComputeFunction...")
        compute_fn = ModelComputeFunction(
            model=model,
            device="cpu",
            model_name="gpt2-trunk-test"
        )
        print("âœ“ ComputeFunction åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºæœåŠ¡å™¨
        print(f"\nåˆ›å»º gRPC æœåŠ¡å™¨ (ç«¯å£: {TEST_PORT})...")
        server = GRPCComputeServer(
            compute_fn=compute_fn,
            host=TEST_HOST,
            port=TEST_PORT,
            max_workers=1  # å•çº¿ç¨‹æ¨¡å¼
        )
        print("âœ“ æœåŠ¡å™¨åˆ›å»ºæˆåŠŸ")
        
        # æ£€æŸ¥æœåŠ¡å™¨å±æ€§
        print(f"\næœåŠ¡å™¨ä¿¡æ¯:")
        print(f"  ä¸»æœº: {server.host}")
        print(f"  ç«¯å£: {server.port}")
        print(f"  æœ€å¤§å·¥ä½œçº¿ç¨‹: {server.max_workers}")
        print(f"  æœ€å¤§æ¶ˆæ¯é•¿åº¦: {server.max_message_length / (1024*1024):.1f} MB")
        
        return server, model
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None


def test_server_start_stop(server):
    """æµ‹è¯• 2: æœåŠ¡å™¨å¯åŠ¨å’Œåœæ­¢"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 2: æœåŠ¡å™¨å¯åŠ¨å’Œåœæ­¢")
    print("=" * 70)
    
    try:
        # æ£€æŸ¥ç«¯å£æ˜¯å¦å¯ç”¨
        if not check_port_available(TEST_HOST, TEST_PORT):
            print(f"âš ï¸  ç«¯å£ {TEST_PORT} å·²è¢«å ç”¨")
        else:
            print(f"âœ“ ç«¯å£ {TEST_PORT} å¯ç”¨")
        
        # å¯åŠ¨æœåŠ¡å™¨
        print("\nå¯åŠ¨æœåŠ¡å™¨...")
        server.start()
        print("âœ“ æœåŠ¡å™¨å¯åŠ¨æˆåŠŸ")
        
        # ç­‰å¾…æœåŠ¡å™¨å®Œå…¨å¯åŠ¨
        time.sleep(2)
        
        # æ£€æŸ¥ç«¯å£æ˜¯å¦åœ¨ç›‘å¬
        print("\næ£€æŸ¥ç«¯å£ç›‘å¬çŠ¶æ€...")
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        result = sock.connect_ex(('localhost', TEST_PORT))
        sock.close()
        
        if result == 0:
            print(f"âœ“ ç«¯å£ {TEST_PORT} æ­£åœ¨ç›‘å¬")
        else:
            print(f"âš ï¸  ç«¯å£ {TEST_PORT} æœªç›‘å¬")
        
        # è·å–æœåŠ¡ä¿¡æ¯
        print("\næœåŠ¡å™¨çŠ¶æ€:")
        print(f"  è¿è¡Œä¸­: {server.server is not None}")
        
        # åœæ­¢æœåŠ¡å™¨
        print("\nåœæ­¢æœåŠ¡å™¨...")
        server.stop(grace=2)
        print("âœ“ æœåŠ¡å™¨åœæ­¢æˆåŠŸ")
        
        # å†æ¬¡æ£€æŸ¥ç«¯å£
        time.sleep(1)
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        result = sock.connect_ex(('localhost', TEST_PORT))
        sock.close()
        
        if result != 0:
            print(f"âœ“ ç«¯å£ {TEST_PORT} å·²é‡Šæ”¾")
        else:
            print(f"âš ï¸  ç«¯å£ {TEST_PORT} ä»åœ¨ç›‘å¬")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        try:
            server.stop(grace=1)
        except:
            pass
        return False


def test_server_with_model(server, model):
    """æµ‹è¯• 3: ä½¿ç”¨æ¨¡å‹è¿›è¡Œå®é™…è®¡ç®—"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 3: æœåŠ¡å™¨æ¨¡å‹è®¡ç®—åŠŸèƒ½")
    print("=" * 70)
    
    try:
        # å¯åŠ¨æœåŠ¡å™¨
        print("å¯åŠ¨æœåŠ¡å™¨...")
        server.start()
        time.sleep(2)
        print("âœ“ æœåŠ¡å™¨å¯åŠ¨æˆåŠŸ")
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        test_input = torch.randn(1, 5, 768)
        print(f"\næµ‹è¯•è¾“å…¥:")
        print(f"  å½¢çŠ¶: {test_input.shape}")
        print(f"  å¤§å°: {test_input.numel() * 4 / 1024:.2f} KB")
        
        # ç›´æ¥ä½¿ç”¨è®¡ç®—å‡½æ•°æµ‹è¯•ï¼ˆä¸é€šè¿‡ gRPCï¼‰
        print("\nç›´æ¥è°ƒç”¨ ComputeFunction...")
        start_time = time.time()
        output = server.compute_fn.compute(test_input)
        elapsed = time.time() - start_time
        
        print(f"âœ“ è®¡ç®—å®Œæˆ")
        print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"  è€—æ—¶: {elapsed*1000:.2f} ms")
        
        # éªŒè¯è¾“å‡º
        if output.shape == test_input.shape:
            print("âœ“ è¾“å‡ºå½¢çŠ¶æ­£ç¡®")
        else:
            print(f"âš ï¸  è¾“å‡ºå½¢çŠ¶ä¸ç¬¦åˆé¢„æœŸ: {output.shape} (æœŸæœ›: {test_input.shape})")
        
        # åœæ­¢æœåŠ¡å™¨
        print("\nåœæ­¢æœåŠ¡å™¨...")
        server.stop(grace=2)
        print("âœ“ æœåŠ¡å™¨åœæ­¢æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        try:
            server.stop(grace=1)
        except:
            pass
        return False


def test_server_context_manager():
    """æµ‹è¯• 4: ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 4: æœåŠ¡å™¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨")
    print("=" * 70)
    
    try:
        # åŠ è½½æ¨¡å‹
        model = torch.load(MODEL_PATH, map_location='cpu', weights_only=False)
        model.eval()
        
        compute_fn = ModelComputeFunction(
            model=model,
            device="cpu",
            model_name="gpt2-trunk-test"
        )
        
        # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        print("ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨å¯åŠ¨æœåŠ¡å™¨...")
        with GRPCComputeServer(
            compute_fn=compute_fn,
            host=TEST_HOST,
            port=TEST_PORT,
            max_workers=1
        ) as server:
            print("âœ“ æœåŠ¡å™¨åœ¨ä¸Šä¸‹æ–‡ä¸­è¿è¡Œ")
            time.sleep(1)
            
            # æ£€æŸ¥ç«¯å£
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            result = sock.connect_ex(('localhost', TEST_PORT))
            sock.close()
            
            if result == 0:
                print("âœ“ ç«¯å£æ­£åœ¨ç›‘å¬")
            else:
                print("âš ï¸  ç«¯å£æœªç›‘å¬")
        
        print("âœ“ ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡ºï¼ŒæœåŠ¡å™¨è‡ªåŠ¨åœæ­¢")
        
        # æ£€æŸ¥ç«¯å£æ˜¯å¦é‡Šæ”¾
        time.sleep(1)
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        result = sock.connect_ex(('localhost', TEST_PORT))
        sock.close()
        
        if result != 0:
            print("âœ“ ç«¯å£å·²é‡Šæ”¾")
        else:
            print("âš ï¸  ç«¯å£ä»åœ¨ç›‘å¬")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "=" * 70)
    print("gRPC æœåŠ¡å™¨åŠŸèƒ½æµ‹è¯•")
    print("=" * 70)
    print(f"\næµ‹è¯•é…ç½®:")
    print(f"  æ¨¡å‹æ–‡ä»¶: {MODEL_PATH}")
    print(f"  æœåŠ¡å™¨åœ°å€: {TEST_HOST}:{TEST_PORT}")
    print(f"  å•çº¿ç¨‹æ¨¡å¼: æ˜¯")
    print()
    
    results = {}
    
    # æµ‹è¯• 1: æœåŠ¡å™¨åˆ›å»º
    server, model = test_server_creation()
    if server is None:
        print("\nâŒ æœåŠ¡å™¨åˆ›å»ºå¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
        return 1
    
    results['creation'] = server is not None
    
    # æµ‹è¯• 2: æœåŠ¡å™¨å¯åŠ¨å’Œåœæ­¢
    results['start_stop'] = test_server_start_stop(server)
    
    # æµ‹è¯• 3: ä½¿ç”¨æ¨¡å‹è®¡ç®—
    results['model_compute'] = test_server_with_model(server, model)
    
    # æµ‹è¯• 4: ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    results['context_manager'] = test_server_context_manager()
    
    # æ€»ç»“
    print("\n" + "=" * 70)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 70)
    
    for test_name, result in results.items():
        status = "âœ“ é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"  {test_name:20s}: {status}")
    
    passed = sum(results.values())
    total = len(results)
    
    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼gRPC æœåŠ¡å™¨åŠŸèƒ½æ­£å¸¸ï¼")
        return 0
    else:
        print(f"\nâš ï¸  æœ‰ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥")
        return 1


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print("\n\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

