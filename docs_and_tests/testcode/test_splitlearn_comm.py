#!/usr/bin/env python3
"""
SplitLearnComm åº“åŠŸèƒ½æµ‹è¯•

ä½¿ç”¨ testcode ç›®å½•ä¸­çš„ç°æˆæ¨¡å‹æ–‡ä»¶æµ‹è¯• SplitLearnComm çš„åŠŸèƒ½ï¼š
- æœåŠ¡å™¨å¯åŠ¨å’Œåœæ­¢
- å®¢æˆ·ç«¯è¿æ¥
- è®¡ç®—åŠŸèƒ½
- å¤šæ¬¡è¯·æ±‚
- é”™è¯¯å¤„ç†
- æ€§èƒ½æµ‹è¯•
"""

import os
import sys
import time
import threading
import torch
import logging

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¿…é¡»åœ¨å¯¼å…¥ grpc ä¹‹å‰ï¼‰
os.environ['GRPC_VERBOSITY'] = 'ERROR'
os.environ['GLOG_minloglevel'] = '2'
os.environ.setdefault('OMP_NUM_THREADS', '1')
os.environ.setdefault('MKL_NUM_THREADS', '1')
os.environ.setdefault('NUMEXPR_NUM_THREADS', '1')

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, os.path.join(project_root, 'SplitLearnComm', 'src'))
sys.path.insert(0, os.path.join(project_root, 'SplitLearnCore', 'src'))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

from splitlearn_comm import GRPCComputeServer, GRPCComputeClient
from splitlearn_comm.core import ModelComputeFunction

# æµ‹è¯•é…ç½®
TEST_PORT = 50052  # ä½¿ç”¨ä¸åŒçš„ç«¯å£é¿å…å†²çª
TEST_HOST = "localhost"
MODEL_PATH = os.path.join(current_dir, "gpt2_trunk_full.pt")


def check_model_file():
    """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    print("=" * 70)
    print("æ£€æŸ¥æ¨¡å‹æ–‡ä»¶")
    print("=" * 70)
    
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {MODEL_PATH}")
        print("\nè¯·å…ˆè¿è¡Œ: python testcode/prepare_models.py")
        return False
    
    file_size_mb = os.path.getsize(MODEL_PATH) / (1024 * 1024)
    print(f"âœ“ æ¨¡å‹æ–‡ä»¶å­˜åœ¨: {MODEL_PATH}")
    print(f"  å¤§å°: {file_size_mb:.2f} MB")
    return True


def load_model():
    """åŠ è½½æ¨¡å‹"""
    print("\n" + "=" * 70)
    print("åŠ è½½æ¨¡å‹")
    print("=" * 70)
    
    try:
        # å¯¼å…¥å¿…è¦çš„ç±»ï¼ˆç”¨äºååºåˆ—åŒ–ï¼‰
        from splitlearn_core.models.gpt2 import GPT2TrunkModel
        print("âœ“ GPT2TrunkModel å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âš ï¸  æ— æ³•å¯¼å…¥ GPT2TrunkModel: {e}")
        print("   å°è¯•ç›´æ¥åŠ è½½...")
    
    print(f"\nåŠ è½½æ¨¡å‹: {MODEL_PATH}")
    start_time = time.time()
    
    model = torch.load(MODEL_PATH, map_location='cpu', weights_only=False)
    model.eval()
    
    elapsed = time.time() - start_time
    
    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ (è€—æ—¶: {elapsed:.2f} ç§’)")
    print(f"  æ€»å‚æ•°é‡: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"  æ¨¡å‹ç±»å‹: {type(model).__name__}")
    
    return model


def test_server_start_stop(model):
    """æµ‹è¯•æœåŠ¡å™¨å¯åŠ¨å’Œåœæ­¢"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 1: æœåŠ¡å™¨å¯åŠ¨å’Œåœæ­¢")
    print("=" * 70)
    
    try:
        # åˆ›å»º ComputeFunction
        compute_fn = ModelComputeFunction(
            model=model,
            device="cpu",
            model_name="gpt2-trunk-test"
        )
        print("âœ“ ComputeFunction åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºæœåŠ¡å™¨ï¼ˆå•çº¿ç¨‹æ¨¡å¼ï¼‰
        server = GRPCComputeServer(
            compute_fn=compute_fn,
            host="0.0.0.0",
            port=TEST_PORT,
            max_workers=1  # å•çº¿ç¨‹æ¨¡å¼
        )
        print(f"âœ“ æœåŠ¡å™¨åˆ›å»ºæˆåŠŸ (ç«¯å£: {TEST_PORT})")
        
        # å¯åŠ¨æœåŠ¡å™¨
        server.start()
        print("âœ“ æœåŠ¡å™¨å¯åŠ¨æˆåŠŸ")
        
        # ç­‰å¾…ä¸€ä¸‹ç¡®ä¿æœåŠ¡å™¨å®Œå…¨å¯åŠ¨
        time.sleep(2)
        
        # æ£€æŸ¥ç«¯å£æ˜¯å¦åœ¨ç›‘å¬
        import socket
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        result = sock.connect_ex((TEST_HOST, TEST_PORT))
        sock.close()
        
        if result == 0:
            print("âœ“ ç«¯å£æ­£åœ¨ç›‘å¬")
        else:
            print("âš ï¸  ç«¯å£æœªç›‘å¬")
        
        # åœæ­¢æœåŠ¡å™¨
        server.stop(grace=2)
        print("âœ“ æœåŠ¡å™¨åœæ­¢æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_client_connection(model):
    """æµ‹è¯•å®¢æˆ·ç«¯è¿æ¥"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 2: å®¢æˆ·ç«¯è¿æ¥")
    print("=" * 70)
    
    # åœ¨åå°å¯åŠ¨æœåŠ¡å™¨
    compute_fn = ModelComputeFunction(
        model=model,
        device="cpu",
        model_name="gpt2-trunk-test"
    )
    server = GRPCComputeServer(
        compute_fn=compute_fn,
        host="0.0.0.0",
        port=TEST_PORT,
        max_workers=1
    )
    
    def run_server():
        server.start()
        server.wait_for_termination()
    
    server_thread = threading.Thread(target=run_server, daemon=True)
    server_thread.start()
    
    # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
    print("ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨...")
    time.sleep(3)
    
    try:
        # åˆ›å»ºå®¢æˆ·ç«¯
        client = GRPCComputeClient(
            server_address=f"{TEST_HOST}:{TEST_PORT}",
            timeout=10.0
        )
        print("âœ“ å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
        
        # è¿æ¥æœåŠ¡å™¨
        if client.connect():
            print("âœ“ è¿æ¥æˆåŠŸ")
            
            # è·å–æœåŠ¡ä¿¡æ¯
            info = client.get_service_info()
            print(f"âœ“ æœåŠ¡ä¿¡æ¯:")
            print(f"  æœåŠ¡å: {info.get('service_name', 'N/A')}")
            print(f"  ç‰ˆæœ¬: {info.get('version', 'N/A')}")
            print(f"  è®¾å¤‡: {info.get('device', 'N/A')}")
            print(f"  æ€»è¯·æ±‚æ•°: {info.get('total_requests', 0)}")
            
            # å¥åº·æ£€æŸ¥
            is_healthy = client.health_check()
            print(f"âœ“ å¥åº·æ£€æŸ¥: {'å¥åº·' if is_healthy else 'ä¸å¥åº·'}")
            
            client.close()
            print("âœ“ å®¢æˆ·ç«¯å…³é—­æˆåŠŸ")
            
            # åœæ­¢æœåŠ¡å™¨
            server.stop(grace=2)
            print("âœ“ æœåŠ¡å™¨åœæ­¢æˆåŠŸ")
            
            return True
        else:
            print("âŒ è¿æ¥å¤±è´¥")
            server.stop(grace=2)
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        server.stop(grace=2)
        return False


def test_compute_functionality(model):
    """æµ‹è¯•è®¡ç®—åŠŸèƒ½"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 3: è®¡ç®—åŠŸèƒ½")
    print("=" * 70)
    
    # å¯åŠ¨æœåŠ¡å™¨
    compute_fn = ModelComputeFunction(
        model=model,
        device="cpu",
        model_name="gpt2-trunk-test"
    )
    server = GRPCComputeServer(
        compute_fn=compute_fn,
        host="0.0.0.0",
        port=TEST_PORT,
        max_workers=1
    )
    
    def run_server():
        server.start()
        server.wait_for_termination()
    
    server_thread = threading.Thread(target=run_server, daemon=True)
    server_thread.start()
    time.sleep(3)
    
    try:
        # åˆ›å»ºå®¢æˆ·ç«¯
        client = GRPCComputeClient(
            server_address=f"{TEST_HOST}:{TEST_PORT}",
            timeout=30.0
        )
        
        if not client.connect():
            print("âŒ è¿æ¥å¤±è´¥")
            server.stop(grace=2)
            return False
        
        print("âœ“ å®¢æˆ·ç«¯è¿æ¥æˆåŠŸ")
        
        # æµ‹è¯• 1: åŸºæœ¬è®¡ç®—
        print("\n[3.1] åŸºæœ¬è®¡ç®—æµ‹è¯•")
        test_input = torch.randn(1, 10, 768)  # [batch, seq_len, hidden_dim]
        print(f"  è¾“å…¥å½¢çŠ¶: {test_input.shape}")
        print(f"  è¾“å…¥å¤§å°: {test_input.numel() * 4 / 1024:.2f} KB")
        
        start_time = time.time()
        output = client.compute(test_input)
        elapsed = time.time() - start_time
        
        print(f"âœ“ è®¡ç®—å®Œæˆ (è€—æ—¶: {elapsed*1000:.2f} ms)")
        print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"  è¾“å‡ºå¤§å°: {output.numel() * 4 / 1024:.2f} KB")
        print(f"  è¾“å‡ºç±»å‹: {output.dtype}")
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        if output.shape == test_input.shape:
            print("âœ“ è¾“å‡ºå½¢çŠ¶æ­£ç¡®")
        else:
            print(f"âš ï¸  è¾“å‡ºå½¢çŠ¶ä¸ç¬¦åˆé¢„æœŸ: {output.shape} (æœŸæœ›: {test_input.shape})")
        
        # æµ‹è¯• 2: ä¸åŒå½¢çŠ¶çš„è¾“å…¥
        print("\n[3.2] ä¸åŒå½¢çŠ¶æµ‹è¯•")
        test_cases = [
            (1, 5, 768),   # çŸ­åºåˆ—
            (1, 20, 768),  # é•¿åºåˆ—
            (2, 10, 768),  # æ‰¹é‡å¤§å° 2
        ]
        
        for i, shape in enumerate(test_cases, 1):
            test_input = torch.randn(*shape)
            try:
                output = client.compute(test_input)
                if output.shape == shape:
                    print(f"  âœ“ æµ‹è¯• {i}: {shape} â†’ {output.shape}")
                else:
                    print(f"  âš ï¸  æµ‹è¯• {i}: {shape} â†’ {output.shape} (å½¢çŠ¶ä¸åŒ¹é…)")
            except Exception as e:
                print(f"  âŒ æµ‹è¯• {i} å¤±è´¥: {e}")
        
        # æµ‹è¯• 3: æ•°å€¼éªŒè¯ï¼ˆç®€å•æ£€æŸ¥ï¼‰
        print("\n[3.3] æ•°å€¼éªŒè¯")
        test_input = torch.randn(1, 5, 768)
        output1 = client.compute(test_input)
        output2 = client.compute(test_input)  # ç›¸åŒè¾“å…¥
        
        # æ£€æŸ¥è¾“å‡ºæ˜¯å¦ä¸€è‡´ï¼ˆåº”è¯¥ä¸€è‡´ï¼Œå› ä¸ºæ¨¡å‹æ˜¯ç¡®å®šæ€§çš„ï¼‰
        if torch.allclose(output1, output2, atol=1e-5):
            print("âœ“ ç›¸åŒè¾“å…¥äº§ç”Ÿç›¸åŒè¾“å‡ºï¼ˆç¡®å®šæ€§æµ‹è¯•é€šè¿‡ï¼‰")
        else:
            print("âš ï¸  ç›¸åŒè¾“å…¥äº§ç”Ÿä¸åŒè¾“å‡ºï¼ˆå¯èƒ½æ˜¯æ•°å€¼ç²¾åº¦é—®é¢˜ï¼‰")
        
        # æµ‹è¯• 4: è¾¹ç•Œæƒ…å†µ
        print("\n[3.4] è¾¹ç•Œæƒ…å†µæµ‹è¯•")
        # æœ€å°è¾“å…¥
        min_input = torch.randn(1, 1, 768)
        try:
            min_output = client.compute(min_input)
            print(f"  âœ“ æœ€å°è¾“å…¥ (1, 1, 768) â†’ {min_output.shape}")
        except Exception as e:
            print(f"  âŒ æœ€å°è¾“å…¥å¤±è´¥: {e}")
        
        client.close()
        server.stop(grace=2)
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        server.stop(grace=2)
        return False


def test_multiple_requests(model):
    """æµ‹è¯•å¤šæ¬¡è¯·æ±‚"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 4: å¤šæ¬¡è¯·æ±‚ï¼ˆå¹¶å‘æµ‹è¯•ï¼‰")
    print("=" * 70)
    
    # å¯åŠ¨æœåŠ¡å™¨
    compute_fn = ModelComputeFunction(
        model=model,
        device="cpu",
        model_name="gpt2-trunk-test"
    )
    server = GRPCComputeServer(
        compute_fn=compute_fn,
        host="0.0.0.0",
        port=TEST_PORT,
        max_workers=1  # å•çº¿ç¨‹æ¨¡å¼
    )
    
    def run_server():
        server.start()
        server.wait_for_termination()
    
    server_thread = threading.Thread(target=run_server, daemon=True)
    server_thread.start()
    time.sleep(3)
    
    try:
        # åˆ›å»ºå®¢æˆ·ç«¯
        client = GRPCComputeClient(
            server_address=f"{TEST_HOST}:{TEST_PORT}",
            timeout=30.0
        )
        
        if not client.connect():
            print("âŒ è¿æ¥å¤±è´¥")
            server.stop(grace=2)
            return False
        
        print("âœ“ å®¢æˆ·ç«¯è¿æ¥æˆåŠŸ")
        
        num_requests = 10
        successes = 0
        total_time = 0.0
        
        print(f"\nå‘é€ {num_requests} ä¸ªè¯·æ±‚...")
        
        for i in range(num_requests):
            test_input = torch.randn(1, 5, 768)
            try:
                start_time = time.time()
                output = client.compute(test_input)
                elapsed = time.time() - start_time
                total_time += elapsed
                successes += 1
                print(f"  è¯·æ±‚ {i+1}/{num_requests}: âœ“ (è€—æ—¶: {elapsed*1000:.2f} ms)")
            except Exception as e:
                print(f"  è¯·æ±‚ {i+1}/{num_requests}: âŒ ({e})")
        
        avg_time = total_time / successes if successes > 0 else 0
        print(f"\næ€»ç»“:")
        print(f"  æˆåŠŸ: {successes}/{num_requests}")
        print(f"  å¹³å‡è€—æ—¶: {avg_time*1000:.2f} ms")
        print(f"  æ€»è€—æ—¶: {total_time*1000:.2f} ms")
        
        client.close()
        server.stop(grace=2)
        
        return successes == num_requests
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        server.stop(grace=2)
        return False


def test_error_handling(model):
    """æµ‹è¯•é”™è¯¯å¤„ç†"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 5: é”™è¯¯å¤„ç†")
    print("=" * 70)
    
    # å¯åŠ¨æœåŠ¡å™¨
    compute_fn = ModelComputeFunction(
        model=model,
        device="cpu",
        model_name="gpt2-trunk-test"
    )
    server = GRPCComputeServer(
        compute_fn=compute_fn,
        host="0.0.0.0",
        port=TEST_PORT,
        max_workers=1
    )
    
    def run_server():
        server.start()
        server.wait_for_termination()
    
    server_thread = threading.Thread(target=run_server, daemon=True)
    server_thread.start()
    time.sleep(3)
    
    try:
        client = GRPCComputeClient(
            server_address=f"{TEST_HOST}:{TEST_PORT}",
            timeout=10.0
        )
        
        if not client.connect():
            print("âŒ è¿æ¥å¤±è´¥")
            server.stop(grace=2)
            return False
        
        # æµ‹è¯• 1: æ— æ•ˆå½¢çŠ¶ï¼ˆå¦‚æœæ¨¡å‹ä¸æ”¯æŒï¼‰
        print("\n[5.1] æµ‹è¯•æ— æ•ˆè¾“å…¥å½¢çŠ¶")
        try:
            invalid_input = torch.randn(10, 10, 10)  # é”™è¯¯çš„å½¢çŠ¶
            output = client.compute(invalid_input)
            print("  âš ï¸  æ— æ•ˆè¾“å…¥è¢«æ¥å—ï¼ˆå¯èƒ½æ¨¡å‹æœ‰å®¹é”™æœºåˆ¶ï¼‰")
        except Exception as e:
            print(f"  âœ“ æ­£ç¡®æ‹’ç»æ— æ•ˆè¾“å…¥: {type(e).__name__}")
        
        # æµ‹è¯• 2: è¿æ¥æ–­å¼€åçš„è¡Œä¸º
        print("\n[5.2] æµ‹è¯•è¿æ¥æ–­å¼€")
        client.close()
        try:
            output = client.compute(torch.randn(1, 5, 768))
            print("  âš ï¸  è¿æ¥æ–­å¼€åä»èƒ½è®¡ç®—ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰")
        except Exception as e:
            print(f"  âœ“ æ­£ç¡®æ£€æµ‹åˆ°è¿æ¥æ–­å¼€: {type(e).__name__}")
        
        server.stop(grace=2)
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        server.stop(grace=2)
        return False


def test_performance(model):
    """æµ‹è¯•æ€§èƒ½"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 6: æ€§èƒ½æµ‹è¯•")
    print("=" * 70)
    
    # å¯åŠ¨æœåŠ¡å™¨
    compute_fn = ModelComputeFunction(
        model=model,
        device="cpu",
        model_name="gpt2-trunk-test"
    )
    server = GRPCComputeServer(
        compute_fn=compute_fn,
        host="0.0.0.0",
        port=TEST_PORT,
        max_workers=1
    )
    
    def run_server():
        server.start()
        server.wait_for_termination()
    
    server_thread = threading.Thread(target=run_server, daemon=True)
    server_thread.start()
    time.sleep(3)
    
    try:
        client = GRPCComputeClient(
            server_address=f"{TEST_HOST}:{TEST_PORT}",
            timeout=30.0
        )
        
        if not client.connect():
            print("âŒ è¿æ¥å¤±è´¥")
            server.stop(grace=2)
            return False
        
        # é¢„çƒ­
        print("é¢„çƒ­ä¸­...")
        for _ in range(3):
            client.compute(torch.randn(1, 5, 768))
        
        # æ€§èƒ½æµ‹è¯•
        num_tests = 20
        test_input = torch.randn(1, 10, 768)
        
        print(f"\næ‰§è¡Œ {num_tests} æ¬¡è®¡ç®—...")
        times = []
        
        for i in range(num_tests):
            start_time = time.time()
            output = client.compute(test_input)
            elapsed = time.time() - start_time
            times.append(elapsed)
            if (i + 1) % 5 == 0:
                print(f"  å®Œæˆ {i+1}/{num_tests}")
        
        # ç»Ÿè®¡
        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)
        total_time = sum(times)
        
        print(f"\næ€§èƒ½ç»Ÿè®¡:")
        print(f"  æ€»è¯·æ±‚æ•°: {num_tests}")
        print(f"  æ€»è€—æ—¶: {total_time*1000:.2f} ms")
        print(f"  å¹³å‡è€—æ—¶: {avg_time*1000:.2f} ms")
        print(f"  æœ€å°è€—æ—¶: {min_time*1000:.2f} ms")
        print(f"  æœ€å¤§è€—æ—¶: {max_time*1000:.2f} ms")
        print(f"  ååé‡: {num_tests/total_time:.2f} è¯·æ±‚/ç§’")
        
        # è·å–å®¢æˆ·ç«¯ç»Ÿè®¡
        stats = client.get_statistics()
        print(f"\nå®¢æˆ·ç«¯ç»Ÿè®¡:")
        print(f"  æ€»è¯·æ±‚æ•°: {stats.get('total_requests', 0)}")
        print(f"  å¹³å‡ç½‘ç»œæ—¶é—´: {stats.get('avg_network_time_ms', 0):.2f} ms")
        print(f"  å¹³å‡è®¡ç®—æ—¶é—´: {stats.get('avg_compute_time_ms', 0):.2f} ms")
        
        client.close()
        server.stop(grace=2)
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        server.stop(grace=2)
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "=" * 70)
    print("SplitLearnComm åº“åŠŸèƒ½æµ‹è¯•")
    print("=" * 70)
    print(f"\næµ‹è¯•é…ç½®:")
    print(f"  æ¨¡å‹æ–‡ä»¶: {MODEL_PATH}")
    print(f"  æœåŠ¡å™¨åœ°å€: {TEST_HOST}:{TEST_PORT}")
    print(f"  å•çº¿ç¨‹æ¨¡å¼: æ˜¯")
    print()
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not check_model_file():
        return 1
    
    # åŠ è½½æ¨¡å‹
    try:
        model = load_model()
    except Exception as e:
        print(f"\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    # è¿è¡Œæµ‹è¯•
    results = {}
    
    results['server'] = test_server_start_stop(model)
    results['connection'] = test_client_connection(model)
    results['compute'] = test_compute_functionality(model)
    results['multiple'] = test_multiple_requests(model)
    results['error'] = test_error_handling(model)
    results['performance'] = test_performance(model)
    
    # æ€»ç»“
    print("\n" + "=" * 70)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 70)
    
    for test_name, result in results.items():
        status = "âœ“ é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"  {test_name:15s}: {status}")
    
    passed = sum(results.values())
    total = len(results)
    
    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼SplitLearnComm åŠŸèƒ½æ­£å¸¸ï¼")
        return 0
    else:
        print(f"\nâš ï¸  æœ‰ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥")
        return 1


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print("\n\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

