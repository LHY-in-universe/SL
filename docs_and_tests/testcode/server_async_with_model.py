#!/usr/bin/env python3
"""
å¼‚æ­¥ gRPC æœåŠ¡å™¨ - ä½¿ç”¨ testcode ä¸­çš„æ¨¡å‹æ–‡ä»¶

ä½¿ç”¨ Comm å’Œ Core åº“ï¼š
- ä½¿ç”¨ AsyncGRPCComputeServerï¼ˆå¼‚æ­¥æœåŠ¡å™¨ï¼‰
- ä½¿ç”¨ AsyncModelComputeFunctionï¼ˆå¼‚æ­¥è®¡ç®—å‡½æ•°ï¼‰
- åŠ è½½ testcode ä¸­çš„ .pt æ¨¡å‹æ–‡ä»¶
"""

import os
import sys
import asyncio
import time
import torch
import logging
from concurrent.futures import ThreadPoolExecutor

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¿…é¡»åœ¨å¯¼å…¥ grpc ä¹‹å‰ï¼‰
os.environ['GRPC_VERBOSITY'] = 'ERROR'
os.environ['GLOG_minloglevel'] = '2'
os.environ.setdefault('OMP_NUM_THREADS', '1')
os.environ.setdefault('MKL_NUM_THREADS', '1')
os.environ.setdefault('NUMEXPR_NUM_THREADS', '1')

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, os.path.join(project_root, 'SplitLearnComm', 'src'))
sys.path.insert(0, os.path.join(project_root, 'SplitLearnCore', 'src'))

# é…ç½®è¯¦ç»†æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)

from splitlearn_comm import AsyncGRPCComputeServer
from splitlearn_comm.core import AsyncModelComputeFunction

# æµ‹è¯•é…ç½®
PORT = 50061
HOST = "0.0.0.0"
MODEL_PATH = os.path.join(current_dir, "gpt2_trunk_full.pt")


async def async_main():
    """å¼‚æ­¥ä¸»å‡½æ•°"""
    print("\n" + "=" * 70)
    print("ğŸš€ å¼‚æ­¥ gRPC æœåŠ¡å™¨å¯åŠ¨ï¼ˆä½¿ç”¨æ¨¡å‹æ–‡ä»¶ï¼‰")
    print("=" * 70)
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not os.path.exists(MODEL_PATH):
        print(f"\nâŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {MODEL_PATH}")
        print("   è¯·å…ˆè¿è¡Œ: python testcode/prepare_models.py")
        return 1
    
    # åŠ è½½æ¨¡å‹
    print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹: {MODEL_PATH}")
    try:
        from splitlearn_core.models.gpt2 import GPT2TrunkModel
        print("   âœ“ GPT2TrunkModel å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"   âš ï¸  æ— æ³•å¯¼å…¥ GPT2TrunkModel: {e}")
        print("   å°è¯•ç›´æ¥åŠ è½½...")
    
    print("   æ­£åœ¨åŠ è½½æ¨¡å‹...")
    print("   ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...ï¼‰")
    start_time = time.time()
    
    try:
        # åœ¨ executor ä¸­å¼‚æ­¥åŠ è½½æ¨¡å‹ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
        def _load_model():
            """åœ¨åå°çº¿ç¨‹ä¸­åŠ è½½æ¨¡å‹"""
            print("   â³ å¼€å§‹ torch.load()ï¼ˆåœ¨åå°çº¿ç¨‹ä¸­ï¼‰...")
            import sys
            sys.stdout.flush()
            model = torch.load(MODEL_PATH, map_location='cpu', weights_only=False)
            model.eval()
            return model
        
        # ä½¿ç”¨ä¸´æ—¶ executor åŠ è½½æ¨¡å‹
        temp_executor = ThreadPoolExecutor(max_workers=1)
        loop = asyncio.get_event_loop()
        model = await loop.run_in_executor(temp_executor, _load_model)
        temp_executor.shutdown(wait=False)
        
        print("   âœ“ æ¨¡å‹æ–‡ä»¶åŠ è½½æˆåŠŸ")
        print("   âœ“ æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼")
        
        load_time = time.time() - start_time
        
        total_params = sum(p.numel() for p in model.parameters())
        print(f"   âœ“ æ¨¡å‹åŠ è½½å®Œæˆ (è€—æ—¶: {load_time:.2f} ç§’)")
        print(f"   âœ“ å‚æ•°é‡: {total_params:,}")
        print(f"   âœ“ æ¨¡å‹ç±»å‹: {type(model).__name__}")
    except Exception as e:
        print(f"   âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    # åˆ›å»ºå•çº¿ç¨‹ executorï¼ˆç”¨äº PyTorch æ¨ç†ï¼‰
    executor = ThreadPoolExecutor(max_workers=1)
    print(f"\nğŸ”§ åˆ›å»ºå¼‚æ­¥è®¡ç®—å‡½æ•°...")
    print(f"   ä½¿ç”¨å•çº¿ç¨‹ executorï¼ˆé¿å…çº¿ç¨‹ç«äº‰ï¼‰")
    
    # åˆ›å»ºå¼‚æ­¥è®¡ç®—å‡½æ•°
    compute_fn = AsyncModelComputeFunction(
        model=model,
        device="cpu",
        executor=executor
    )
    print("   âœ“ å¼‚æ­¥è®¡ç®—å‡½æ•°åˆ›å»ºæˆåŠŸ")
    
    # åˆ›å»ºå¼‚æ­¥æœåŠ¡å™¨
    print(f"\nğŸŒ åˆ›å»ºå¼‚æ­¥ gRPC æœåŠ¡å™¨...")
    print(f"   ç›‘å¬åœ°å€: {HOST}:{PORT}")
    print(f"   ä½¿ç”¨åç¨‹ï¼ˆä¸æ˜¯çº¿ç¨‹æ± ï¼‰")
    print(f"   âœ… æ— çº¿ç¨‹ç«äº‰é—®é¢˜")
    
    server = AsyncGRPCComputeServer(
        compute_fn=compute_fn,
        host=HOST,
        port=PORT
    )
    print("   âœ“ å¼‚æ­¥æœåŠ¡å™¨åˆ›å»ºæˆåŠŸ")
    
    # å¯åŠ¨æœåŠ¡å™¨
    print(f"\nâ–¶ï¸  å¯åŠ¨æœåŠ¡å™¨...")
    await server.start()
    print("   âœ“ æœåŠ¡å™¨å·²å¯åŠ¨")
    
    print("\n" + "=" * 70)
    print("âœ… æœåŠ¡å™¨è¿è¡Œä¸­ï¼Œç­‰å¾…å®¢æˆ·ç«¯è¿æ¥...")
    print("=" * 70)
    print(f"\nğŸ“¡ æœåŠ¡å™¨åœ°å€: localhost:{PORT}")
    print(f"ğŸ’¡ åœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œå®¢æˆ·ç«¯: python testcode/client_async_with_model.py")
    print(f"â¹ï¸  æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨\n")
    
    try:
        await server.wait_for_termination()
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨å…³é—­æœåŠ¡å™¨...")
        await server.stop()
        executor.shutdown(wait=True)
        print("   âœ“ æœåŠ¡å™¨å·²å…³é—­")
    
    return 0


def main():
    """ä¸»å‡½æ•°"""
    try:
        return asyncio.run(async_main())
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ æœåŠ¡å™¨è¢«ç”¨æˆ·ä¸­æ–­")
        return 0
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())

