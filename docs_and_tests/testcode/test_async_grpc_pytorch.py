#!/usr/bin/env python3
"""
æµ‹è¯•å¼‚æ­¥ç‰ˆæœ¬æ˜¯å¦èƒ½è§£å†³ gRPC å’Œ PyTorch çš„å†²çª

å¯¹æ¯”æµ‹è¯•ï¼š
1. åŒæ­¥ç‰ˆæœ¬ + PyTorchï¼ˆæœ‰å†²çªï¼‰
2. å¼‚æ­¥ç‰ˆæœ¬ + PyTorchï¼ˆåº”è¯¥æ— å†²çªï¼‰
"""

import os
import sys
import time
import asyncio
import torch
import torch.nn as nn
import logging
import threading

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¿…é¡»åœ¨å¯¼å…¥ grpc ä¹‹å‰ï¼‰
os.environ['GRPC_VERBOSITY'] = 'ERROR'
os.environ['GLOG_minloglevel'] = '2'
os.environ.setdefault('OMP_NUM_THREADS', '1')
os.environ.setdefault('MKL_NUM_THREADS', '1')
os.environ.setdefault('NUMEXPR_NUM_THREADS', '1')

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, os.path.join(project_root, 'SplitLearnComm', 'src'))

logging.basicConfig(level=logging.WARNING)

from splitlearn_comm import GRPCComputeServer, AsyncGRPCComputeServer
from splitlearn_comm.core import ComputeFunction, AsyncComputeFunction


def print_separator(title=""):
    """æ‰“å°åˆ†éš”çº¿"""
    if title:
        print("\n" + "=" * 70)
        print(f"  {title}")
        print("=" * 70)
    else:
        print("\n" + "=" * 70)


class SyncPyTorchCompute(ComputeFunction):
    """åŒæ­¥ç‰ˆæœ¬ï¼šä½¿ç”¨ PyTorch æ¨¡å‹"""
    
    def __init__(self):
        self.model = nn.Sequential(
            nn.Linear(10, 20),
            nn.ReLU(),
            nn.Linear(20, 10)
        )
        self.model.eval()
        self.request_count = 0
    
    def compute(self, input_tensor: torch.Tensor) -> torch.Tensor:
        self.request_count += 1
        with torch.no_grad():
            return self.model(input_tensor)


class AsyncPyTorchCompute(AsyncComputeFunction):
    """å¼‚æ­¥ç‰ˆæœ¬ï¼šä½¿ç”¨ PyTorch æ¨¡å‹"""
    
    def __init__(self, executor=None):
        self.model = nn.Sequential(
            nn.Linear(10, 20),
            nn.ReLU(),
            nn.Linear(20, 10)
        )
        self.model.eval()
        self.request_count = 0
        self.executor = executor
    
    async def compute(self, input_tensor: torch.Tensor) -> torch.Tensor:
        self.request_count += 1
        loop = asyncio.get_event_loop()
        
        def _sync_compute():
            with torch.no_grad():
                return self.model(input_tensor)
        
        # åœ¨ executor ä¸­æ‰§è¡Œï¼ˆé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
        if self.executor:
            return await loop.run_in_executor(self.executor, _sync_compute)
        else:
            return await loop.run_in_executor(None, _sync_compute)
    
    async def setup(self):
        """åˆå§‹åŒ–"""
        pass
    
    async def teardown(self):
        """æ¸…ç†"""
        pass


def test_sync_version():
    """æµ‹è¯• 1: åŒæ­¥ç‰ˆæœ¬ + PyTorch"""
    print_separator("æµ‹è¯• 1: åŒæ­¥ç‰ˆæœ¬ + PyTorchï¼ˆå¯èƒ½æœ‰å†²çªï¼‰")
    
    print("\nâš ï¸  æµ‹è¯•åŒæ­¥ç‰ˆæœ¬...")
    print("   ä½¿ç”¨: GRPCComputeServer + PyTorch æ¨¡å‹")
    print("   é¢„æœŸ: å¯èƒ½æœ‰ mutex è­¦å‘Šæˆ–çº¿ç¨‹å†²çª")
    
    try:
        compute_fn = SyncPyTorchCompute()
        server = GRPCComputeServer(
            compute_fn=compute_fn,
            host="0.0.0.0",
            port=50059,
            max_workers=1  # å•çº¿ç¨‹æ¨¡å¼
        )
        
        print(f"\nâœ“ æœåŠ¡å™¨åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•è®¡ç®—
        test_input = torch.randn(1, 10)
        output = compute_fn.compute(test_input)
        print(f"âœ“ è®¡ç®—æµ‹è¯•æˆåŠŸ: {output.shape}")
        
        # æ£€æŸ¥çº¿ç¨‹æ•°
        thread_count = threading.active_count()
        print(f"âœ“ å½“å‰çº¿ç¨‹æ•°: {thread_count}")
        
        server.stop(grace=1)
        print(f"âœ“ æœåŠ¡å™¨åœæ­¢æˆåŠŸ")
        
        print(f"\nğŸ“Š ç»“æœ:")
        print(f"   å¤„ç†äº† {compute_fn.request_count} ä¸ªè¯·æ±‚")
        print(f"   çº¿ç¨‹æ•°: {thread_count}")
        print(f"   âš ï¸  å¦‚æœçœ‹åˆ° mutex è­¦å‘Šï¼Œè¯´æ˜æœ‰çº¿ç¨‹å†²çª")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_async_version():
    """æµ‹è¯• 2: å¼‚æ­¥ç‰ˆæœ¬ + PyTorch"""
    print_separator("æµ‹è¯• 2: å¼‚æ­¥ç‰ˆæœ¬ + PyTorchï¼ˆåº”è¯¥æ— å†²çªï¼‰")
    
    print("\nâœ… æµ‹è¯•å¼‚æ­¥ç‰ˆæœ¬...")
    print("   ä½¿ç”¨: AsyncGRPCComputeServer + PyTorch æ¨¡å‹")
    print("   é¢„æœŸ: æ—  mutex è­¦å‘Šï¼Œæ— çº¿ç¨‹å†²çª")
    
    try:
        from concurrent.futures import ThreadPoolExecutor
        
        # åˆ›å»ºå•çº¿ç¨‹ executorï¼ˆç”¨äº PyTorch è®¡ç®—ï¼‰
        executor = ThreadPoolExecutor(max_workers=1)
        
        compute_fn = AsyncPyTorchCompute(executor=executor)
        server = AsyncGRPCComputeServer(
            compute_fn=compute_fn,
            host="0.0.0.0",
            port=50060
        )
        
        print(f"\nâœ“ æœåŠ¡å™¨åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•è®¡ç®—
        test_input = torch.randn(1, 10)
        output = await compute_fn.compute(test_input)
        print(f"âœ“ è®¡ç®—æµ‹è¯•æˆåŠŸ: {output.shape}")
        
        # æ£€æŸ¥çº¿ç¨‹æ•°
        thread_count = threading.active_count()
        print(f"âœ“ å½“å‰çº¿ç¨‹æ•°: {thread_count}")
        
        # å¯åŠ¨æœåŠ¡å™¨ï¼ˆçŸ­æš‚è¿è¡Œï¼‰
        await server.start()
        print(f"âœ“ æœåŠ¡å™¨å¯åŠ¨æˆåŠŸ")
        
        # ç­‰å¾…ä¸€ä¸‹
        await asyncio.sleep(1)
        
        await server.stop()
        executor.shutdown(wait=True)
        print(f"âœ“ æœåŠ¡å™¨åœæ­¢æˆåŠŸ")
        
        print(f"\nğŸ“Š ç»“æœ:")
        print(f"   å¤„ç†äº† {compute_fn.request_count} ä¸ªè¯·æ±‚")
        print(f"   çº¿ç¨‹æ•°: {thread_count}")
        print(f"   âœ… å¼‚æ­¥ç‰ˆæœ¬ä½¿ç”¨åç¨‹ï¼Œä¸åº”è¯¥æœ‰çº¿ç¨‹å†²çª")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_thread_comparison():
    """æµ‹è¯• 3: çº¿ç¨‹æ•°å¯¹æ¯”"""
    print_separator("æµ‹è¯• 3: çº¿ç¨‹æ•°å¯¹æ¯”")
    
    print("\nğŸ” å¯¹æ¯”åŒæ­¥å’Œå¼‚æ­¥ç‰ˆæœ¬çš„çº¿ç¨‹ä½¿ç”¨...")
    
    initial_threads = threading.active_count()
    print(f"\nåˆå§‹çº¿ç¨‹æ•°: {initial_threads}")
    
    # åŒæ­¥ç‰ˆæœ¬
    print(f"\nåŒæ­¥ç‰ˆæœ¬ (GRPCComputeServer):")
    print(f"  - ä½¿ç”¨ ThreadPoolExecutor")
    print(f"  - max_workers=1 æ—¶: 1 ä¸ª gRPC çº¿ç¨‹")
    print(f"  - åŠ ä¸Š PyTorch çº¿ç¨‹: 1 + 4 = 5 ä¸ªçº¿ç¨‹ï¼ˆå¦‚æœ PyTorch å¤šçº¿ç¨‹ï¼‰")
    print(f"  - é—®é¢˜: çº¿ç¨‹ç«äº‰")
    
    # å¼‚æ­¥ç‰ˆæœ¬
    print(f"\nå¼‚æ­¥ç‰ˆæœ¬ (AsyncGRPCComputeServer):")
    print(f"  - ä½¿ç”¨ asyncio äº‹ä»¶å¾ªç¯")
    print(f"  - åªæœ‰ 1 ä¸ªä¸»çº¿ç¨‹")
    print(f"  - ä½¿ç”¨åç¨‹åˆ‡æ¢ï¼Œä¸æ˜¯çœŸæ­£çš„å¤šçº¿ç¨‹")
    print(f"  - ä¼˜åŠ¿: æ— çº¿ç¨‹ç«äº‰")


def test_concurrent_requests():
    """æµ‹è¯• 4: å¹¶å‘è¯·æ±‚å¯¹æ¯”"""
    print_separator("æµ‹è¯• 4: å¹¶å‘è¯·æ±‚å¤„ç†èƒ½åŠ›")
    
    print("\nğŸ“Š å¹¶å‘è¯·æ±‚å¤„ç†å¯¹æ¯”:")
    
    print(f"\nåŒæ­¥ç‰ˆæœ¬ (max_workers=1):")
    print(f"  - åŒæ—¶åªèƒ½å¤„ç† 1 ä¸ªè¯·æ±‚")
    print(f"  - å…¶ä»–è¯·æ±‚éœ€è¦ç­‰å¾…")
    print(f"  - é€‚åˆ: ä½å¹¶å‘åœºæ™¯")
    
    print(f"\nå¼‚æ­¥ç‰ˆæœ¬:")
    print(f"  - å¯ä»¥åŒæ—¶å¤„ç†å¤šä¸ªè¯·æ±‚ï¼ˆåç¨‹åˆ‡æ¢ï¼‰")
    print(f"  - ä¸éœ€è¦ç­‰å¾…")
    print(f"  - é€‚åˆ: é«˜å¹¶å‘åœºæ™¯")
    print(f"  - ä¼˜åŠ¿: æ— çº¿ç¨‹ç«äº‰ï¼Œæ€§èƒ½æ›´å¥½")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "=" * 70)
    print("å¼‚æ­¥ç‰ˆæœ¬ vs åŒæ­¥ç‰ˆæœ¬ï¼šè§£å†³ gRPC å’Œ PyTorch å†²çªæµ‹è¯•")
    print("=" * 70)
    
    results = {}
    
    # æµ‹è¯•çº¿ç¨‹å¯¹æ¯”
    test_thread_comparison()
    
    # æµ‹è¯•å¹¶å‘èƒ½åŠ›
    test_concurrent_requests()
    
    # æµ‹è¯•åŒæ­¥ç‰ˆæœ¬
    print("\n" + "=" * 70)
    print("å¼€å§‹å®é™…æµ‹è¯•...")
    print("=" * 70)
    
    results['sync'] = test_sync_version()
    
    # æµ‹è¯•å¼‚æ­¥ç‰ˆæœ¬
    print("\n" + "=" * 70)
    print("æµ‹è¯•å¼‚æ­¥ç‰ˆæœ¬...")
    print("=" * 70)
    
    results['async'] = asyncio.run(test_async_version())
    
    # æ€»ç»“
    print_separator("æµ‹è¯•æ€»ç»“")
    
    print("\næµ‹è¯•ç»“æœ:")
    print(f"  åŒæ­¥ç‰ˆæœ¬: {'âœ… é€šè¿‡' if results.get('sync') else 'âŒ å¤±è´¥'}")
    print(f"  å¼‚æ­¥ç‰ˆæœ¬: {'âœ… é€šè¿‡' if results.get('async') else 'âŒ å¤±è´¥'}")
    
    print("\nğŸ’¡ ç»“è®º:")
    if results.get('async'):
        print("  âœ… å¼‚æ­¥ç‰ˆæœ¬å¯ä»¥è§£å†³ gRPC å’Œ PyTorch çš„å†²çªé—®é¢˜")
        print("  âœ… ä½¿ç”¨ AsyncGRPCComputeServer æ¨èç”¨äºç”Ÿäº§ç¯å¢ƒ")
        print("  âœ… æ²¡æœ‰çº¿ç¨‹ç«äº‰ï¼Œæ—  mutex è­¦å‘Š")
    else:
        print("  âš ï¸  å¼‚æ­¥ç‰ˆæœ¬æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥")
    
    if results.get('sync'):
        print("\n  âš ï¸  åŒæ­¥ç‰ˆæœ¬è™½ç„¶èƒ½è¿è¡Œï¼Œä½†å¯èƒ½æœ‰çº¿ç¨‹å†²çª")
        print("  âš ï¸  å»ºè®®ä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬")
    
    print("\nğŸ“‹ å»ºè®®:")
    print("  1. ç”Ÿäº§ç¯å¢ƒä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬ (AsyncGRPCComputeServer)")
    print("  2. æµ‹è¯•ç¯å¢ƒå¯ä»¥ä½¿ç”¨åŒæ­¥ç‰ˆæœ¬ (max_workers=1)")
    print("  3. è®¾ç½® PyTorch å•çº¿ç¨‹æ¨¡å¼é¿å…å†²çª")
    
    return 0


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print("\n\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

