#!/usr/bin/env python3
"""
ç®€å•çš„å®¢æˆ·ç«¯-æœåŠ¡å™¨é›†æˆæµ‹è¯•
æµ‹è¯• gRPC é€šä¿¡æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""
import os
import sys

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¿…é¡»åœ¨å¯¼å…¥ grpc ä¹‹å‰ï¼‰
os.environ['GRPC_VERBOSITY'] = 'ERROR'
os.environ['GRPC_TRACE'] = ''
os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'

import torch

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, os.path.join(project_root, 'SplitLearnComm', 'src'))

from splitlearn_comm import GRPCComputeClient

def test_connection():
    """æµ‹è¯•æœåŠ¡å™¨è¿æ¥"""
    print("=" * 70)
    print("æµ‹è¯• 1: æœåŠ¡å™¨è¿æ¥")
    print("=" * 70)

    client = GRPCComputeClient("localhost:50051", timeout=5.0)

    if client.connect():
        print("âœ… è¿æ¥æˆåŠŸï¼")
        return client
    else:
        print("âŒ è¿æ¥å¤±è´¥ï¼")
        sys.exit(1)

def test_compute(client):
    """æµ‹è¯•è®¡ç®—åŠŸèƒ½"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 2: è®¡ç®—åŠŸèƒ½")
    print("=" * 70)

    # åˆ›å»ºä¸€ä¸ªæµ‹è¯•å¼ é‡ï¼ˆæ¨¡æ‹Ÿ GPT-2 hidden stateï¼‰
    test_tensor = torch.randn(1, 10, 768)  # [batch, seq_len, hidden_dim]
    print(f"è¾“å…¥å¼ é‡å½¢çŠ¶: {test_tensor.shape}")

    try:
        # å‘é€è®¡ç®—è¯·æ±‚
        result = client.compute(test_tensor, model_id="gpt2-trunk")
        print(f"âœ… è®¡ç®—æˆåŠŸï¼")
        print(f"è¾“å‡ºå¼ é‡å½¢çŠ¶: {result.shape}")
        print(f"è¾“å‡ºå¼ é‡ç±»å‹: {result.dtype}")

        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        expected_shape = (1, 10, 768)  # GPT-2 ä¿æŒç›¸åŒå½¢çŠ¶
        if result.shape == expected_shape:
            print(f"âœ… è¾“å‡ºå½¢çŠ¶æ­£ç¡®: {result.shape}")
        else:
            print(f"âš ï¸  è¾“å‡ºå½¢çŠ¶ä¸ç¬¦åˆé¢„æœŸ: {result.shape} (æœŸæœ›: {expected_shape})")

        return True
    except Exception as e:
        print(f"âŒ è®¡ç®—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_multiple_requests(client):
    """æµ‹è¯•å¤šæ¬¡è¯·æ±‚"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 3: å¤šæ¬¡è¯·æ±‚")
    print("=" * 70)

    num_requests = 5
    successes = 0

    for i in range(num_requests):
        test_tensor = torch.randn(1, 5, 768)
        try:
            result = client.compute(test_tensor, model_id="gpt2-trunk")
            successes += 1
            print(f"  è¯·æ±‚ {i+1}/{num_requests}: âœ… æˆåŠŸ")
        except Exception as e:
            print(f"  è¯·æ±‚ {i+1}/{num_requests}: âŒ å¤±è´¥ ({e})")

    print(f"\næ€»ç»“: {successes}/{num_requests} è¯·æ±‚æˆåŠŸ")
    return successes == num_requests

def main():
    print("\nğŸ§ª Split Learning å®¢æˆ·ç«¯-æœåŠ¡å™¨é›†æˆæµ‹è¯•\n")

    # æµ‹è¯• 1: è¿æ¥
    client = test_connection()

    # æµ‹è¯• 2: å•æ¬¡è®¡ç®—
    compute_ok = test_compute(client)

    # æµ‹è¯• 3: å¤šæ¬¡è¯·æ±‚
    multiple_ok = test_multiple_requests(client)

    # æ€»ç»“
    print("\n" + "=" * 70)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("=" * 70)
    print(f"âœ… è¿æ¥æµ‹è¯•: é€šè¿‡")
    print(f"{'âœ…' if compute_ok else 'âŒ'} è®¡ç®—æµ‹è¯•: {'é€šè¿‡' if compute_ok else 'å¤±è´¥'}")
    print(f"{'âœ…' if multiple_ok else 'âŒ'} å¤šæ¬¡è¯·æ±‚æµ‹è¯•: {'é€šè¿‡' if multiple_ok else 'å¤±è´¥'}")

    if compute_ok and multiple_ok:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼")
        return 0
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        return 1

if __name__ == "__main__":
    sys.exit(main())
