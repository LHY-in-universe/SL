#!/usr/bin/env python3
"""
éªŒè¯ PEFT åº“æ˜¯å¦å¯ä»¥ç›´æ¥åº”ç”¨åˆ°æ‹†åˆ†çš„æ¨¡å‹

è¿™ä¸ªè„šæœ¬è¯æ˜ï¼šæ ‡å‡†çš„å¾®è°ƒåº“ï¼ˆPEFTï¼‰å¯ä»¥ç›´æ¥ä½¿ç”¨ï¼
"""

import os
import sys
import torch

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.insert(0, project_root)

# æ£€æŸ¥ PEFT æ˜¯å¦å·²å®‰è£…
try:
    from peft import LoraConfig, get_peft_model, TaskType
    PEFT_AVAILABLE = True
    print("âœ… PEFT åº“å·²å®‰è£…")
except ImportError:
    PEFT_AVAILABLE = False
    print("âŒ PEFT åº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install peft")
    sys.exit(1)

from transformers import GPT2Config, AutoConfig
from transformers.models.gpt2.modeling_gpt2 import GPT2Block
from splitlearn_core.models.gpt2 import GPT2BottomModel, GPT2TopModel


def test_single_block():
    """æµ‹è¯• 1: PEFT æ˜¯å¦å¯ä»¥åº”ç”¨åˆ°å•ä¸ª GPT2Block"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 1: å•ä¸ª GPT2Block")
    print("=" * 70)
    
    config = GPT2Config()
    block = GPT2Block(config, layer_idx=0)
    
    print(f"åŸå§‹ Block å‚æ•°æ•°é‡: {sum(p.numel() for p in block.parameters()):,}")
    
    # åº”ç”¨ PEFT
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=8,
        lora_alpha=16,
        target_modules=["c_attn", "c_fc", "c_proj"]
    )
    
    try:
        peft_block = get_peft_model(block, lora_config)
        print("âœ… PEFT æˆåŠŸåº”ç”¨åˆ°å•ä¸ª Block")
        peft_block.print_trainable_parameters()
        return True
    except Exception as e:
        print(f"âŒ å¤±è´¥: {e}")
        return False


def test_bottom_model():
    """æµ‹è¯• 2: PEFT æ˜¯å¦å¯ä»¥åº”ç”¨åˆ° Bottom æ¨¡å‹"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 2: Bottom æ¨¡å‹")
    print("=" * 70)
    
    config = GPT2Config()
    bottom = GPT2BottomModel(config, end_layer=2)
    
    total_params = sum(p.numel() for p in bottom.parameters())
    print(f"åŸå§‹ Bottom æ¨¡å‹å‚æ•°æ•°é‡: {total_params:,}")
    
    # åº”ç”¨ PEFT
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=8,
        lora_alpha=16,
        target_modules=["c_attn", "c_fc", "c_proj"]
    )
    
    try:
        bottom_peft = get_peft_model(bottom, lora_config)
        print("âœ… PEFT æˆåŠŸåº”ç”¨åˆ° Bottom æ¨¡å‹")
        print("\nå‚æ•°ç»Ÿè®¡:")
        bottom_peft.print_trainable_parameters()
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        input_ids = torch.randint(0, config.vocab_size, (1, 10))
        with torch.no_grad():
            output = bottom_peft(input_ids)
        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        return True
    except Exception as e:
        print(f"âŒ å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_top_model():
    """æµ‹è¯• 3: PEFT æ˜¯å¦å¯ä»¥åº”ç”¨åˆ° Top æ¨¡å‹"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 3: Top æ¨¡å‹")
    print("=" * 70)
    
    config = GPT2Config()
    top = GPT2TopModel(config, start_layer=10)
    
    total_params = sum(p.numel() for p in top.parameters())
    print(f"åŸå§‹ Top æ¨¡å‹å‚æ•°æ•°é‡: {total_params:,}")
    
    # åº”ç”¨ PEFT
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=8,
        lora_alpha=16,
        target_modules=["c_attn", "c_fc", "c_proj"]
    )
    
    try:
        top_peft = get_peft_model(top, lora_config)
        print("âœ… PEFT æˆåŠŸåº”ç”¨åˆ° Top æ¨¡å‹")
        print("\nå‚æ•°ç»Ÿè®¡:")
        top_peft.print_trainable_parameters()
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        hidden_states = torch.randn(1, 10, config.n_embd)
        with torch.no_grad():
            output = top_peft(hidden_states)
            logits = output.logits if hasattr(output, 'logits') else output
        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {logits.shape}")
        
        return True
    except Exception as e:
        print(f"âŒ å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_training_mode():
    """æµ‹è¯• 4: PEFT æ¨¡å‹æ˜¯å¦å¯ä»¥æ­£å¸¸è®­ç»ƒ"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 4: è®­ç»ƒæ¨¡å¼æµ‹è¯•")
    print("=" * 70)
    
    config = GPT2Config()
    bottom = GPT2BottomModel(config, end_layer=2)
    
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=8,
        target_modules=["c_attn", "c_fc", "c_proj"]
    )
    
    bottom_peft = get_peft_model(bottom, lora_config)
    bottom_peft.train()
    
    # è·å–å¯è®­ç»ƒå‚æ•°
    trainable_params = [p for p in bottom_peft.parameters() if p.requires_grad]
    frozen_params = [p for p in bottom_peft.parameters() if not p.requires_grad]
    
    print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {sum(p.numel() for p in trainable_params):,}")
    print(f"å†»ç»“å‚æ•°æ•°é‡: {sum(p.numel() for p in frozen_params):,}")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(trainable_params, lr=1e-4)
    
    # æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤
    input_ids = torch.randint(0, config.vocab_size, (2, 10))
    output = bottom_peft(input_ids)
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æŸå¤±
    target = torch.randn_like(output)
    loss = torch.nn.MSELoss()(output, target)
    
    # åå‘ä¼ æ’­
    loss.backward()
    
    # æ£€æŸ¥æ¢¯åº¦
    has_grad = any(p.grad is not None for p in trainable_params if p.requires_grad)
    
    if has_grad:
        print("âœ… æ¢¯åº¦è®¡ç®—æˆåŠŸ")
        
        # æ›´æ–°å‚æ•°
        optimizer.step()
        optimizer.zero_grad()
        print("âœ… å‚æ•°æ›´æ–°æˆåŠŸ")
        
        return True
    else:
        print("âŒ æœªæ£€æµ‹åˆ°æ¢¯åº¦")
        return False


def test_save_and_load():
    """æµ‹è¯• 5: PEFT æ¨¡å‹æ˜¯å¦å¯ä»¥ä¿å­˜å’ŒåŠ è½½"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 5: ä¿å­˜å’ŒåŠ è½½")
    print("=" * 70)
    
    from pathlib import Path
    import tempfile
    
    config = GPT2Config()
    bottom = GPT2BottomModel(config, end_layer=2)
    
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=8,
        target_modules=["c_attn", "c_fc", "c_proj"]
    )
    
    bottom_peft = get_peft_model(bottom, lora_config)
    
    # ä¿å­˜
    with tempfile.TemporaryDirectory() as tmpdir:
        save_path = Path(tmpdir) / "lora_weights"
        bottom_peft.save_pretrained(str(save_path))
        print(f"âœ… LoRA æƒé‡å·²ä¿å­˜åˆ°: {save_path}")
        
        # æ£€æŸ¥æ–‡ä»¶
        adapter_config_file = save_path / "adapter_config.json"
        adapter_model_file = save_path / "adapter_model.bin"
        
        if adapter_config_file.exists() and adapter_model_file.exists():
            print(f"âœ… é…ç½®æ–‡ä»¶å­˜åœ¨: {adapter_config_file.exists()}")
            print(f"âœ… æ¨¡å‹æ–‡ä»¶å­˜åœ¨: {adapter_model_file.exists()}")
            
            # åŠ è½½
            from peft import PeftModel
            bottom_loaded = PeftModel.from_pretrained(bottom, str(save_path))
            print("âœ… LoRA æƒé‡åŠ è½½æˆåŠŸ")
            
            return True
        else:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨")
            return False


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("=" * 70)
    print("PEFT å…¼å®¹æ€§éªŒè¯")
    print("=" * 70)
    print("\nè¿™ä¸ªè„šæœ¬éªŒè¯ï¼šæ ‡å‡†çš„ PEFT åº“å¯ä»¥ç›´æ¥åº”ç”¨åˆ°ä½ çš„æ‹†åˆ†æ¨¡å‹ï¼")
    
    results = []
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    results.append(("å•ä¸ª Block", test_single_block()))
    results.append(("Bottom æ¨¡å‹", test_bottom_model()))
    results.append(("Top æ¨¡å‹", test_top_model()))
    results.append(("è®­ç»ƒæ¨¡å¼", test_training_mode()))
    results.append(("ä¿å­˜/åŠ è½½", test_save_and_load()))
    
    # æ€»ç»“
    print("\n" + "=" * 70)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 70)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{name}: {status}")
    
    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("âœ… ç»“è®º: PEFT åº“å¯ä»¥å®Œå…¨å…¼å®¹ä½ çš„æ‹†åˆ†æ¨¡å‹ï¼")
        print("âœ… å»ºè®®: ç›´æ¥ä½¿ç”¨ PEFT åº“ï¼Œæ— éœ€è‡ªå®ç° LoRAï¼")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
    
    return passed == total


if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)
