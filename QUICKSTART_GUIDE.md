# Split Learning Quick Start Guide

æœ¬æŒ‡å—å¸®åŠ©æ‚¨åœ¨ 5 åˆ†é’Ÿå†…å¿«é€Ÿä¸Šæ‰‹ Split Learning æ¡†æ¶ã€‚

---

## ç›®å½•

- [5åˆ†é’Ÿå¿«é€Ÿå¼€å§‹](#5åˆ†é’Ÿå¿«é€Ÿå¼€å§‹)
- [å®¢æˆ·ç«¯ç¤ºä¾‹](#å®¢æˆ·ç«¯ç¤ºä¾‹)
- [æœåŠ¡ç«¯ç¤ºä¾‹](#æœåŠ¡ç«¯ç¤ºä¾‹)
- [å®Œæ•´çš„ Split Learning ç¤ºä¾‹](#å®Œæ•´çš„-split-learning-ç¤ºä¾‹)
- [å¸¸è§ä½¿ç”¨åœºæ™¯](#å¸¸è§ä½¿ç”¨åœºæ™¯)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## 5åˆ†é’Ÿå¿«é€Ÿå¼€å§‹

### æ­¥éª¤ 1: å®‰è£…

```bash
# å®¢æˆ·ç«¯ï¼ˆè½»é‡çº§ï¼‰
pip install splitlearn-core[client] splitlearn-comm[client]

# æœåŠ¡ç«¯ï¼ˆå®Œæ•´åŠŸèƒ½ï¼‰
pip install splitlearn-core[server] splitlearn-comm[server] splitlearn-manager[server]
```

### æ­¥éª¤ 2: è¿è¡ŒæœåŠ¡ç«¯

```python
# server.py
from splitlearn_manager.quickstart import ManagedServer

# ä¸€è¡Œä»£ç å¯åŠ¨æœåŠ¡å™¨ï¼
server = ManagedServer("gpt2", port=50051)
server.start()  # é˜»å¡è¿è¡Œ
```

```bash
# è¿è¡ŒæœåŠ¡ç«¯
python server.py
```

### æ­¥éª¤ 3: è¿è¡Œå®¢æˆ·ç«¯

```python
# client.py
from splitlearn_comm.quickstart import Client
import torch

# è¿æ¥åˆ°æœåŠ¡å™¨
client = Client("localhost:50051")

# å‘é€æ¨ç†è¯·æ±‚
input_tensor = torch.randn(1, 10, 768)  # (batch, seq_len, hidden_size)
output = client.compute(input_tensor)

print(f"Input shape: {input_tensor.shape}")
print(f"Output shape: {output.shape}")
```

```bash
# è¿è¡Œå®¢æˆ·ç«¯
python client.py
```

### å®Œæˆï¼

æ‚¨å·²ç»æˆåŠŸè¿è¡Œäº†ç¬¬ä¸€ä¸ª Split Learning åº”ç”¨ï¼

---

## å®¢æˆ·ç«¯ç¤ºä¾‹

### ç¤ºä¾‹ 1: åŸºç¡€ä½¿ç”¨

```python
from splitlearn_comm.quickstart import Client
import torch

# åˆ›å»ºå®¢æˆ·ç«¯ï¼ˆè‡ªåŠ¨è¿æ¥ï¼‰
client = Client("localhost:50051")

# å‡†å¤‡è¾“å…¥
input_tensor = torch.randn(1, 10, 768)

# å‘é€è®¡ç®—è¯·æ±‚
output = client.compute(input_tensor)

print(f"è®¡ç®—å®Œæˆï¼è¾“å‡ºå½¢çŠ¶: {output.shape}")

# å…³é—­è¿æ¥
client.close()
```

### ç¤ºä¾‹ 2: ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨

```python
from splitlearn_comm.quickstart import Client
import torch

# ä½¿ç”¨ with è¯­å¥è‡ªåŠ¨ç®¡ç†è¿æ¥
with Client("localhost:50051") as client:
    input_tensor = torch.randn(1, 10, 768)
    output = client.compute(input_tensor)
    print(f"è¾“å‡º: {output.shape}")

# è¿æ¥è‡ªåŠ¨å…³é—­
```

### ç¤ºä¾‹ 3: é…ç½®é‡è¯•å’Œè¶…æ—¶

```python
from splitlearn_comm.quickstart import Client

# è‡ªå®šä¹‰é…ç½®
client = Client(
    server_address="remote-server:50051",
    max_retries=10,       # æœ€å¤šé‡è¯•10æ¬¡
    timeout=60.0,         # è¶…æ—¶60ç§’
    auto_connect=True     # è‡ªåŠ¨è¿æ¥
)

# ä½¿ç”¨å®¢æˆ·ç«¯...
```

---

## æœåŠ¡ç«¯ç¤ºä¾‹

### ç¤ºä¾‹ 1: åŸºç¡€æœåŠ¡ç«¯

```python
from splitlearn_manager.quickstart import ManagedServer

# åˆ›å»ºå¹¶å¯åŠ¨æœåŠ¡å™¨ï¼ˆé˜»å¡ï¼‰
server = ManagedServer(
    model_type="gpt2",
    component="trunk",  # æœåŠ¡ç«¯é€šå¸¸è¿è¡Œ trunk
    port=50051
)

server.start()  # é˜»å¡ï¼Œç›´åˆ° Ctrl+C
```

### ç¤ºä¾‹ 2: è‡ªå®šä¹‰é…ç½®

```python
from splitlearn_manager.quickstart import ManagedServer

# è‡ªå®šä¹‰æœåŠ¡å™¨é…ç½®
server = ManagedServer(
    model_type="qwen2",
    model_path="Qwen/Qwen2.5-0.5B-Instruct",
    component="trunk",
    port=50051,
    host="0.0.0.0",
    device="cuda",  # ä½¿ç”¨ GPU
    max_models=10,   # æœ€å¤šç®¡ç†10ä¸ªæ¨¡å‹
    # ä¼ é€’ç»™æ¨¡å‹çš„é¢å¤–å‚æ•°
    start_layer=4,
    end_layer=20
)

server.start()
```

### ç¤ºä¾‹ 3: ä½¿ç”¨çº¯æ¨¡å‹æœåŠ¡

```python
import torch.nn as nn
from splitlearn_comm.quickstart import serve

# å®šä¹‰æ‚¨è‡ªå·±çš„æ¨¡å‹
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(768, 768)

    def forward(self, x):
        return self.linear(x)

# ä¸€è¡Œä»£ç å¯åŠ¨æœåŠ¡ï¼
serve(MyModel(), port=50051)  # é˜»å¡è¿è¡Œ
```

---

## å®Œæ•´çš„ Split Learning ç¤ºä¾‹

### åœºæ™¯ï¼šGPT-2 åˆ†å¸ƒå¼æ¨ç†

**æ¶æ„**:
- **å®¢æˆ·ç«¯**: è¿è¡Œ Bottom (0-2å±‚) + Top (10-12å±‚)
- **æœåŠ¡ç«¯**: è¿è¡Œ Trunk (2-10å±‚)

#### 1. å‡†å¤‡æ¨¡å‹ï¼ˆä»…éœ€ä¸€æ¬¡ï¼‰

```python
# prepare_models.py
from splitlearn_core.quickstart import load_split_model

# ä¸‹è½½å¹¶åˆ†å‰²æ¨¡å‹
bottom, trunk, top = load_split_model(
    model_type="gpt2",
    split_points=[2, 10],  # Bottom: 0-2, Trunk: 2-10, Top: 10-end
    cache_dir="./models"   # ä¿å­˜åˆ°æœ¬åœ°
)

print("âœ“ æ¨¡å‹å·²å‡†å¤‡å¥½")
print(f"  Bottom: {sum(p.numel() for p in bottom.parameters())/1e6:.2f}M å‚æ•°")
print(f"  Trunk:  {sum(p.numel() for p in trunk.parameters())/1e6:.2f}M å‚æ•°")
print(f"  Top:    {sum(p.numel() for p in top.parameters())/1e6:.2f}M å‚æ•°")
```

#### 2. å¯åŠ¨æœåŠ¡ç«¯ï¼ˆTrunkï¼‰

```python
# server.py
from splitlearn_core.quickstart import load_split_model
from splitlearn_comm.quickstart import Server

# åŠ è½½ Trunk æ¨¡å‹
_, trunk, _ = load_split_model(
    "gpt2",
    split_points=[2, 10],
    cache_dir="./models"
)

# å¯åŠ¨æœåŠ¡å™¨
server = Server(
    model=trunk,
    port=50051,
    device="cuda"  # æˆ– "cpu"
)

print("æœåŠ¡ç«¯å¯åŠ¨ï¼Œç›‘å¬ç«¯å£ 50051...")
server.start()
server.wait_for_termination()
```

#### 3. è¿è¡Œå®¢æˆ·ç«¯ï¼ˆBottom + Topï¼‰

```python
# client.py
from splitlearn_core.quickstart import load_split_model
from splitlearn_comm.quickstart import Client
import torch

# åŠ è½½ Bottom å’Œ Top æ¨¡å‹
bottom, _, top = load_split_model(
    "gpt2",
    split_points=[2, 10],
    cache_dir="./models"
)

# è¿æ¥åˆ°æœåŠ¡ç«¯
client = Client("localhost:50051")

# å‡†å¤‡è¾“å…¥ï¼ˆç¤ºä¾‹ï¼štokenized textï¼‰
input_ids = torch.randint(0, 50257, (1, 10))  # (batch=1, seq_len=10)

# === Split Learning æ¨ç†æµç¨‹ ===

# æ­¥éª¤ 1: å®¢æˆ·ç«¯ - Bottom æ¨¡å‹å‰å‘ä¼ æ’­
bottom_output = bottom(input_ids)
print(f"Bottom è¾“å‡ºå½¢çŠ¶: {bottom_output.shape}")

# æ­¥éª¤ 2: å‘é€åˆ°æœåŠ¡ç«¯ - Trunk æ¨¡å‹è®¡ç®—
trunk_output = client.compute(bottom_output)
print(f"Trunk è¾“å‡ºå½¢çŠ¶: {trunk_output.shape}")

# æ­¥éª¤ 3: å®¢æˆ·ç«¯ - Top æ¨¡å‹å‰å‘ä¼ æ’­
final_output = top(trunk_output)
print(f"Final è¾“å‡ºå½¢çŠ¶: {final_output.shape}")

# è·å–é¢„æµ‹ç»“æœ
logits = final_output
predicted_ids = torch.argmax(logits, dim=-1)
print(f"é¢„æµ‹çš„ token IDs: {predicted_ids}")

# å…³é—­è¿æ¥
client.close()
```

#### 4. è¿è¡Œ

```bash
# ç»ˆç«¯ 1: å¯åŠ¨æœåŠ¡ç«¯
python server.py

# ç»ˆç«¯ 2: è¿è¡Œå®¢æˆ·ç«¯
python client.py
```

---

## å¸¸è§ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1: ä½å»¶è¿Ÿæ¨ç†ï¼ˆæœ¬åœ° Bottom + Topï¼‰

**é€‚ç”¨äº**: éœ€è¦å¿«é€Ÿå“åº”çš„åº”ç”¨ï¼ˆèŠå¤©æœºå™¨äººã€å®æ—¶ç¿»è¯‘ï¼‰

```python
# å®¢æˆ·ç«¯ä¿ç•™ embedding å’Œ headï¼Œå»¶è¿Ÿæœ€ä½
bottom, _, top = load_split_model("gpt2", split_points=[2, 10])

# å¿«é€Ÿå‰å‘ä¼ æ’­
def fast_infer(input_ids):
    hidden = bottom(input_ids)
    hidden = client.compute(hidden)  # ä»…æ­¤æ­¥éª¤éœ€è¦ç½‘ç»œ
    return top(hidden)
```

### åœºæ™¯ 2: éšç§ä¿æŠ¤ï¼ˆæ•æ„Ÿæ•°æ®ä¸ç¦»å¼€å®¢æˆ·ç«¯ï¼‰

**é€‚ç”¨äº**: åŒ»ç–—ã€é‡‘èç­‰éšç§æ•æ„Ÿåœºæ™¯

```python
# åŸå§‹è¾“å…¥ï¼ˆæ•æ„Ÿï¼‰ä»…åœ¨å®¢æˆ·ç«¯å¤„ç†
sensitive_input = load_medical_data()

# Bottom æ¨¡å‹åœ¨æœ¬åœ°å¤„ç†ï¼Œæå–ç‰¹å¾
features = bottom(sensitive_input)  # ç‰¹å¾å·²è„±æ•

# ä»…å‘é€ç‰¹å¾åˆ°æœåŠ¡ç«¯ï¼ˆä¸å«åŸå§‹æ•°æ®ï¼‰
result = client.compute(features)
```

### åœºæ™¯ 3: èµ„æºå—é™è®¾å¤‡ï¼ˆç§»åŠ¨ç«¯ã€è¾¹ç¼˜è®¾å¤‡ï¼‰

**é€‚ç”¨äº**: æ‰‹æœºã€IoT è®¾å¤‡ç­‰ç®—åŠ›æœ‰é™çš„ç¯å¢ƒ

```python
# å®¢æˆ·ç«¯ä»…è¿è¡Œè½»é‡çº§ Bottom æ¨¡å‹
bottom = load_bottom_model("gpt2", end_layer=2)
bottom.eval()  # æ¨ç†æ¨¡å¼

# æœåŠ¡ç«¯è¿è¡Œé‡é‡çº§ Trunk + Top
# å®¢æˆ·ç«¯è®¾å¤‡åªéœ€è¦å°‘é‡å†…å­˜å’Œè®¡ç®—
```

### åœºæ™¯ 4: æ‰¹å¤„ç†ä¼˜åŒ–ï¼ˆæœåŠ¡ç«¯æ‰¹å¤„ç†å¤šä¸ªè¯·æ±‚ï¼‰

**é€‚ç”¨äº**: é«˜ååé‡åœºæ™¯

```python
# æœåŠ¡ç«¯å¯ä»¥æ‰¹å¤„ç†æ¥è‡ªå¤šä¸ªå®¢æˆ·ç«¯çš„è¯·æ±‚
# å®¢æˆ·ç«¯ä»£ç ä¸å˜ï¼ŒæœåŠ¡ç«¯è‡ªåŠ¨ä¼˜åŒ–æ‰¹å¤„ç†
```

---

## å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•é€‰æ‹© split_pointsï¼Ÿ

**A**: æ ¹æ®æ‚¨çš„éœ€æ±‚æƒè¡¡ï¼š

- **ä½å»¶è¿Ÿ**: æ›´å¤šå±‚æ”¾åœ¨å®¢æˆ·ç«¯ï¼ˆå¦‚ [4, 8]ï¼‰
- **ä½å¸¦å®½**: æ›´å°‘å±‚åœ¨å®¢æˆ·ç«¯ï¼ˆå¦‚ [1, 11]ï¼‰
- **éšç§ä¿æŠ¤**: ç¡®ä¿æ•æ„Ÿå¤„ç†åœ¨å®¢æˆ·ç«¯ï¼ˆBottom åŒ…å« embeddingï¼‰
- **å¹³è¡¡**: æ¨è GPT-2 ä½¿ç”¨ [2, 10]ï¼ŒQwen2 ä½¿ç”¨ [4, 20]

### Q2: æ¨¡å‹åŠ è½½å¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ

**A**: ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼š

```python
# ç¬¬ä¸€æ¬¡ä¸‹è½½åï¼Œæ¨¡å‹ä¿å­˜åœ¨æœ¬åœ°
bottom, trunk, top = load_split_model(
    "gpt2",
    split_points=[2, 10],
    cache_dir="./models"  # ä¿å­˜åˆ°æœ¬åœ°
)

# åç»­åŠ è½½ç›´æ¥ä»æœ¬åœ°è¯»å–ï¼Œéå¸¸å¿«
```

### Q3: å¦‚ä½•å¤„ç†å¤šä¸ªå®¢æˆ·ç«¯è¿æ¥ï¼Ÿ

**A**: æœåŠ¡ç«¯è‡ªåŠ¨å¤„ç†å¹¶å‘ï¼š

```python
# æœåŠ¡ç«¯ä»£ç ï¼ˆæ”¯æŒå¤šå®¢æˆ·ç«¯ï¼‰
server = Server(
    model=trunk,
    port=50051,
    max_workers=10  # æœ€å¤š10ä¸ªå¹¶å‘è¯·æ±‚
)
server.start()

# å¤šä¸ªå®¢æˆ·ç«¯å¯ä»¥åŒæ—¶è¿æ¥å’Œå‘é€è¯·æ±‚
```

### Q4: å¦‚ä½•ç›‘æ§æœåŠ¡ç«¯æ€§èƒ½ï¼Ÿ

**A**: ä½¿ç”¨ ManagedServerï¼š

```python
from splitlearn_manager.quickstart import ManagedServer

# è‡ªåŠ¨åŒ…å«æ€§èƒ½ç›‘æ§
server = ManagedServer("gpt2", port=50051)
server.start()

# æŸ¥çœ‹ Prometheus æŒ‡æ ‡
# è®¿é—® http://localhost:9090/metrics
```

### Q5: å®¢æˆ·ç«¯å’ŒæœåŠ¡ç«¯å¿…é¡»åœ¨åŒä¸€å°æœºå™¨å—ï¼Ÿ

**A**: ä¸éœ€è¦ï¼

```python
# å®¢æˆ·ç«¯è¿æ¥åˆ°è¿œç¨‹æœåŠ¡å™¨
client = Client("remote-server.example.com:50051")

# æˆ–ä½¿ç”¨ IP åœ°å€
client = Client("192.168.1.100:50051")
```

### Q6: å¦‚ä½•å®ç°è´Ÿè½½å‡è¡¡ï¼Ÿ

**A**: éƒ¨ç½²å¤šä¸ªæœåŠ¡ç«¯å®ä¾‹ï¼š

```python
# å®¢æˆ·ç«¯è½®è¯¢å¤šä¸ªæœåŠ¡å™¨
servers = [
    "server1:50051",
    "server2:50051",
    "server3:50051"
]

import random
client = Client(random.choice(servers))
```

---

## ä¸‹ä¸€æ­¥

- **æŸ¥çœ‹å®Œæ•´ç¤ºä¾‹**: `examples/` ç›®å½•åŒ…å«æ›´å¤šç¤ºä¾‹
- **æŸ¥çœ‹ API æ–‡æ¡£**: è¯¦ç»†çš„ API å‚è€ƒæ–‡æ¡£
- **æ€§èƒ½è°ƒä¼˜**: å­¦ä¹ å¦‚ä½•ä¼˜åŒ–æ€§èƒ½
- **éƒ¨ç½²æŒ‡å—**: ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æœ€ä½³å®è·µ

## è·å–å¸®åŠ©

- **æ–‡æ¡£**: [å®Œæ•´æ–‡æ¡£](https://splitlearn.readthedocs.io)
- **Issues**: [GitHub Issues](https://github.com/yourusername/SL/issues)
- **ç¤¾åŒº**: [Discord](https://discord.gg/splitlearn)

---

**Happy Split Learning!** ğŸ‰
