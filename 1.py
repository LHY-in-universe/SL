#!/usr/bin/env python3
"""
Qwen2-VL-2Bè¯¦ç»†æ¶æ„åˆ†æ
"""

from huggingface_hub import hf_hub_download
import json

def main():
    # è·å–é…ç½®æ–‡ä»¶
    config_path = hf_hub_download(
        repo_id="Qwen/Qwen2-VL-2B-Instruct",
        filename="config.json"
    )
    
    with open(config_path, 'r') as f:
        config = json.load(f)
    
    vision_config = config.get('vision_config', {})
    
    # è®¡ç®—å‚æ•°
    hidden_size = config['hidden_size']
    num_layers = config['num_hidden_layers']
    num_heads = config['num_attention_heads']
    kv_heads = config['num_key_value_heads']
    vocab_size = config['vocab_size']
    
    # å•å±‚å‚æ•°ä¼°ç®—
    layer_params = (
        # æ³¨æ„åŠ›
        4 * hidden_size * hidden_size +  # QKV+OæŠ•å½±
        # MLP (SwiGLU)
        3 * hidden_size * config['intermediate_size'] +  # gate/up/down
        # å½’ä¸€åŒ– (å¿½ç•¥)
        0
    )
    
    # æ€»å‚æ•°
    total_params = (
        vocab_size * hidden_size +  # è¯åµŒå…¥
        layer_params * num_layers +  # Transformerå±‚
        hidden_size * vocab_size     # è¾“å‡ºå±‚
    )
    
    print("="*70)
    print("Qwen2-VL-2B è¯¦ç»†æ¶æ„åˆ†æ")
    print("="*70)
    
    print(f"\nğŸ“Š æ€»å‚æ•°: ~{total_params/1e9:.2f}B")
    print(f"  è§†è§‰ç¼–ç å™¨: 675M (å®˜æ–¹)")
    print(f"  è¯­è¨€æ¨¡å‹: ~{total_params/1e9 - 0.675:.2f}B")
    print(f"  æ€»è®¡: ~2.175B")
    
    print("\nğŸ—ï¸  è¯­è¨€æ¨¡å‹æ¶æ„:")
    print(f"  â€¢ ç±»å‹: Decoder-only Transformer")
    print(f"  â€¢ éšè—ç»´åº¦: {hidden_size}")
    print(f"  â€¢ å±‚æ•°: {num_layers}")
    print(f"  â€¢ æ³¨æ„åŠ›: GQA {num_heads}:{kv_heads}")
    print(f"  â€¢ MLPæ‰©å±•: {hidden_size} â†’ {config['intermediate_size']} (Ã—{config['intermediate_size']/hidden_size:.1f})")
    print(f"  â€¢ æ¿€æ´»å‡½æ•°: {config['hidden_act']}")
    print(f"  â€¢ å½’ä¸€åŒ–: RMSNorm (Îµ={config['rms_norm_eps']})")
    print(f"  â€¢ ä½ç½®ç¼–ç : RoPE (Î¸={config.get('rope_theta', 1000000)})")
    
    print("\nğŸ‘ï¸  è§†è§‰ç¼–ç å™¨:")
    print(f"  â€¢ ç±»å‹: Vision Transformer")
    print(f"  â€¢ éšè—ç»´åº¦: {vision_config.get('hidden_size', hidden_size)}")
    print(f"  â€¢ Patchå¤§å°: {vision_config.get('patch_size', 14)}")
    print(f"  â€¢ åŠ¨æ€åˆ†è¾¨ç‡: æ˜¯")
    print(f"  â€¢ ä½ç½®ç¼–ç : M-ROPE (å¤šæ¨¡æ€)")
    
    print("\nğŸ”— å¤šæ¨¡æ€èåˆ:")
    print(f"  â€¢ æ–¹å¼: å…±äº«ç»´åº¦ ({vision_config.get('hidden_size', hidden_size)} = {hidden_size})")
    print(f"  â€¢ èåˆ: è§†è§‰ç‰¹å¾ä½œä¸ºè¯­è¨€æ¨¡å‹è¾“å…¥å‰ç¼€")
    print(f"  â€¢ Tokenæ•°: åŠ¨æ€ (æ ¹æ®å›¾åƒåˆ†è¾¨ç‡)")
    
    print("\nâš¡ ä¼˜åŒ–ç‰¹æ€§:")
    print(f"  â€¢ GQAä¼˜åŒ–: KVç¼“å­˜å‡å°‘ {(1 - kv_heads/num_heads)*100:.0f}%")
    print(f"  â€¢ æ»‘åŠ¨çª—å£: {config.get('sliding_window', 32768)} tokens")
    print(f"  â€¢ é•¿ä¸Šä¸‹æ–‡: {config['max_position_embeddings']:,} tokens")
    
    print("\nğŸ’¾ å†…å­˜éœ€æ±‚ (ä¼°ç®—):")
    dtypes = {
        'FP16': 2,
        'INT8': 1,
        'INT4': 0.5
    }
    
    for name, bytes_per_param in dtypes.items():
        param_memory = 2_175_000_000 * bytes_per_param / (1024**3)
        total_memory = param_memory * 2.5  # åŒ…å«æ¿€æ´»å€¼ç­‰
        print(f"  â€¢ {name}: {param_memory:.1f}GB / {total_memory:.1f}GB")
    
    print("="*70)

if __name__ == "__main__":
    main()